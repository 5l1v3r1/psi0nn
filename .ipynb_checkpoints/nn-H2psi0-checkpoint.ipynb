{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN for estimating $H$ ground state, $\\psi_0$\n",
    "Sam Greydanus. 16 May 2017. MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "import os, copy, glob\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import kron, identity\n",
    "\n",
    "np.random.seed(seed=123) # for reproducibility\n",
    "ms = torch.manual_seed(123) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "global_step = 0\n",
    "print_every = 100\n",
    "save_dir = 'H2psi0-models'\n",
    "checkpoint_steps = [1, 10,30,100,300,1000,3000,10000,30000,100000,300000]\n",
    "checkpoint_steps += [250, 500, 1000, 2000, 4000, 16000, 64000, 128000]\n",
    "total_steps = max(checkpoint_steps)\n",
    "d = 2\n",
    "chi = 4\n",
    "D_side = d**chi\n",
    "D_img = D_side**2\n",
    "D_hidden = 512\n",
    "batch_size = 16\n",
    "cost_func = nn.PairwiseDistance() # square and sum to get L2 loss (see train loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Use the same free site as we've been using for DMRG runs. Change the coupling constants `J` and `Jz` randomly, try to estimate change in energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym = lambda M: M + M.T\n",
    "local_op = lambda d: sym(np.random.randn(d,d))\n",
    "    \n",
    "class FreeSite():\n",
    "    def __init__(self, J=None, Jz=None, rand=False):\n",
    "        self.length = 1 # length\n",
    "        self.ops = ops = {}\n",
    "        self.J = J ; self.Jz = Jz\n",
    "        \n",
    "        # build operator dictionary\n",
    "        ops[\"H\"] = np.zeros((2,2)) # local Hamiltonian np.random.randn(2,2)\n",
    "        ops[\"Sz\"] = np.array([[0.5, 0], [0, -0.5]]) # z spin (S^z) operator\n",
    "        ops[\"Sp\"] = np.array([[0.0, 1.0], [0.0, 0.0]]) # raising (S^+) operator\n",
    "        \n",
    "        if rand:\n",
    "            ops[\"Sz\"] = np.random.randn()*np.array([[0, 0.5], [0.5, 0.]]) + np.random.randn()*ops[\"Sz\"]\n",
    "            ops[\"Sp\"] *= np.random.randn()\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return list(self.ops.values())[0].shape[0] # all ops should have same dimensionality\n",
    "        \n",
    "    def enlarge(self, site):\n",
    "        '''Enlarge block by a single site'''\n",
    "        \n",
    "        D1, H1, Sz1, Sp1 = self.get_dim(), self.ops['H'], self.ops['Sz'], self.ops['Sp'] # this block\n",
    "        D2, H2, Sz2, Sp2 = site.get_dim(), site.ops['H'], site.ops['Sz'], site.ops['Sp'] # another block (ie free site)\n",
    "\n",
    "        enlarged = copy.deepcopy(self)\n",
    "        enlarged.length += site.length\n",
    "        ops = enlarged.ops\n",
    "\n",
    "        ops['H'] = kron(H1, identity(D2)) + kron(identity(D1), H2) + self.interaction_H(site)\n",
    "        ops['Sz'] = kron(identity(D1), Sz2)\n",
    "        ops['Sp'] = kron(identity(D1), Sp2)\n",
    "\n",
    "        return enlarged\n",
    "    \n",
    "    def interaction_H(self, site):\n",
    "        '''Given another block, returns two-site term in the \n",
    "        Hamiltonain that joins the two sites.'''\n",
    "        Sz1, Sp1 = self.ops[\"Sz\"], self.ops[\"Sp\"] # this block\n",
    "        Sz2, Sp2 = site.ops[\"Sz\"], site.ops[\"Sp\"] # another block\n",
    "        \n",
    "        J = 1.*np.random.randn() if self.J is None else self.J\n",
    "        Jz = 1.*np.random.randn() if self.Jz is None else self.Jz\n",
    "        \n",
    "        join_Sp = (J/2)*(kron(Sp1, Sp2.conjugate().transpose()) + kron(Sp1.conjugate().transpose(), Sp2))\n",
    "        join_Sz = Jz*kron(Sz1, Sz2)\n",
    "        return (join_Sp + join_Sz)\n",
    "    \n",
    "    def rotate_ops(self, transformation_matrix):\n",
    "        # rotate and truncate each operator.\n",
    "        new_ops = {}\n",
    "        for name, op in self.ops.items():\n",
    "            new_ops[name] = self.rotate_and_truncate(op, transformation_matrix)\n",
    "        self.ops = new_ops\n",
    "    \n",
    "    @staticmethod\n",
    "    def rotate_and_truncate(S, O):\n",
    "        '''Transforms the operator to a new (possibly truncated) basis'''\n",
    "        return O.conjugate().transpose().dot(S.dot(O)) # eqn 7 in arXiv:cond-mat/0603842v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ham(J, Jz, rand=False):\n",
    "    sys = FreeSite(J=J, Jz=Jz, rand=rand)\n",
    "    for _ in range(chi-1):\n",
    "        fs = FreeSite(J=J, Jz=Jz, rand=rand)\n",
    "        sys = sys.enlarge(fs)\n",
    "    return sys.ops['H'].todense()\n",
    "\n",
    "def next_batch(D, batch_size, just_ground=True):\n",
    "    H_list = [] ; e0_list = [] ; psi0_list = [] ; e1_list = [] ; psi1_list = [] ; e2_list = [] ; psi2_list = []\n",
    "    for _ in range(batch_size):\n",
    "        H = ham(None, None, rand=True)\n",
    "        e0, psi0 = eigsh(H,k=3, which=\"SA\")\n",
    "        H_list.append(np.asarray(H).ravel()) ; e0_list.append(e0[0].ravel()) ; psi0_list.append(psi0[:,0].ravel())\n",
    "        if not just_ground:\n",
    "            e1_list.append(e0[1].ravel()) ; psi1_list.append(psi0[:,1].ravel())\n",
    "            e2_list.append(e0[2].ravel()) ; psi2_list.append(psi0[:,2].ravel())\n",
    "    out = (np.vstack(H_list), np.vstack(e0_list), np.vstack(psi0_list))\n",
    "    if not just_ground:\n",
    "        extras = (np.vstack(e1_list), np.vstack(psi1_list), np.vstack(e2_list), np.vstack(psi2_list))\n",
    "    out = out if just_ground else out + extras\n",
    "    return out\n",
    "\n",
    "def rand_psi(D_side):\n",
    "    psi = np.random.rand(1, D_side)\n",
    "    psi /= np.sqrt(np.dot(psi, psi.T))\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv8AAAFwCAYAAAAv9RSyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAmCJp/AAAAYIKg+QcAAAAmiP8P/qOfmebxiSIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109e1e8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot data\n",
    "f1 = plt.figure(figsize=[12,6])\n",
    "\n",
    "f1.text(0.5, .92, 'Example training Hamiltonians', ha='center', va='center', fontsize=18)\n",
    "v = 2\n",
    "h = 4\n",
    "H, e0, psi0 = next_batch(D_side, batch_size=batch_size)\n",
    "sd, mn = np.std(H), np.mean(H)\n",
    "cl = [mn-3*sd,mn+3*sd]\n",
    "\n",
    "for i in range(v):\n",
    "    for j in range(h):\n",
    "        plot_ix = i*h + j\n",
    "        curr_plot = v*100 + h*10 + plot_ix+1\n",
    "\n",
    "        plt.subplot(curr_plot)\n",
    "        plt.gca().axes.get_xaxis().set_visible(False)\n",
    "        plt.gca().axes.get_yaxis().set_visible(False)\n",
    "        H, e0, psi0 = next_batch(D_side, batch_size=1)\n",
    "        im = plt.imshow(np.reshape(H,[D_side,D_side]))\n",
    "        plt.clim(*cl)\n",
    "        \n",
    "cax = f1.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "cb = f1.colorbar(im, cax=cax, orientation='vertical')\n",
    "cb.set_label('color scale')\n",
    "    \n",
    "plt.show() #; f1.savefig('./figures/tph-training-psi0.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a neural network with three hidden layers\n",
    "def dot_norm(u):\n",
    "    m, n = u.size()\n",
    "    d = u.mm(u.t())\n",
    "    div_by = torch.sqrt(torch.diag(d))\n",
    "    v = u / div_by.resize(m, 1).repeat(1,n)\n",
    "    return v\n",
    "\n",
    "class SimpleNN3(torch.nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, h_dim, output_dim):\n",
    "        super(SimpleNN3, self).__init__()\n",
    "        self.W1 = nn.Parameter(torch.randn(input_dim, h_dim)*0.075)\n",
    "        self.b1 = nn.Parameter(torch.randn(h_dim)*0.075)\n",
    "        self.W2 = nn.Parameter(torch.randn(h_dim, h_dim)*0.075)\n",
    "        self.b2 = nn.Parameter(torch.randn(h_dim)*0.075)\n",
    "        self.W3 = nn.Parameter(torch.randn(h_dim, output_dim)*0.075)\n",
    "        self.b3 = nn.Parameter(torch.randn(output_dim)*0.075)\n",
    "        self.arch = 'SimpleNN3'\n",
    "\n",
    "    def forward(self, X, batch_size):\n",
    "        X = X.resize(batch_size, D_img)\n",
    "        h1 = F.relu(X.mm(self.W1) + self.b1.repeat(X.size(0), 1))\n",
    "        h2 = F.relu(h1.mm(self.W2) + self.b2.repeat(X.size(0), 1))\n",
    "        h3 = h2.mm(self.W3) + self.b3.repeat(X.size(0), 1)\n",
    "        h3 = dot_norm(h3)\n",
    "        return h3\n",
    "    \n",
    "model = SimpleNN3(batch_size=1, input_dim=D_img, h_dim=D_hidden, output_dim=D_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global_step = 0\n",
    "# load_was_success = True # yes, I'm being optimistic\n",
    "# model_zoo = []\n",
    "# paths = glob.glob(save_dir + '/*.tar')\n",
    "# try:\n",
    "#     for s in paths:\n",
    "#         checkpoint = torch.load(s)\n",
    "#         model.load_state_dict(checkpoint['state_dict'])\n",
    "#         global_step = checkpoint['global_step']\n",
    "#         model_zoo.append((global_step, copy.deepcopy(model)))\n",
    "#         print(\"loaded model: {}\".format(s))\n",
    "#     model_zoo = sorted(model_zoo, key=lambda tup: tup[0])\n",
    "#     global_step, model = model_zoo[-1]\n",
    "# except:\n",
    "#     print(\"no saved model to load.\") ; model_zoo = []\n",
    "#     load_was_success = False\n",
    "# else:\n",
    "#     if len(paths) is 0: print(\"no saved model to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model to estimate ground state energies\n",
    "Takes ~30 min to train on my MacBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def energy_func(psi, H, batch_size):\n",
    "    psi = psi.resize(batch_size,1,D_side)\n",
    "    H = H.resize(batch_size,D_side,D_side)\n",
    "    e0_hat = psi.bmm(H).bmm(torch.transpose(psi, 2,1))\n",
    "    return e0_hat\n",
    "\n",
    "def cost_func(e, psi, H, batch_size):\n",
    "    e = e.resize(batch_size,1,1)\n",
    "    e0_hat = energy_func(psi, H, batch_size)\n",
    "    return .5*torch.sum((e0_hat-e)**2) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss: 1.2297\n",
      "\tstep 1: saved model\n",
      "\tstep 10: saved model\n",
      "\tstep 30: saved model\n",
      "step 100: loss: 0.6167\n",
      "\tstep 100: saved model\n",
      "step 200: loss: 0.2908\n",
      "\tstep 250: saved model\n",
      "step 300: loss: 0.1640\n",
      "\tstep 300: saved model\n",
      "step 400: loss: 0.1020\n",
      "step 500: loss: 0.0752\n",
      "\tstep 500: saved model\n",
      "step 600: loss: 0.0621\n",
      "step 700: loss: 0.0524\n",
      "step 800: loss: 0.0506\n",
      "step 900: loss: 0.0441\n",
      "step 1000: loss: 0.0414\n",
      "\tstep 1000: saved model\n",
      "step 1100: loss: 0.0383\n",
      "step 1200: loss: 0.0343\n",
      "step 1300: loss: 0.0324\n",
      "step 1400: loss: 0.0317\n",
      "step 1500: loss: 0.0301\n",
      "step 1600: loss: 0.0285\n",
      "step 1700: loss: 0.0285\n",
      "step 1800: loss: 0.0265\n",
      "step 1900: loss: 0.0253\n",
      "step 2000: loss: 0.0247\n",
      "\tstep 2000: saved model\n",
      "step 2100: loss: 0.0231\n",
      "step 2200: loss: 0.0245\n",
      "step 2300: loss: 0.0284\n",
      "step 2400: loss: 0.0267\n",
      "step 2500: loss: 0.0233\n",
      "step 2600: loss: 0.0208\n",
      "step 2700: loss: 0.0206\n",
      "step 2800: loss: 0.0224\n",
      "step 2900: loss: 0.0200\n",
      "step 3000: loss: 0.0203\n",
      "\tstep 3000: saved model\n",
      "step 3100: loss: 0.0193\n",
      "step 3200: loss: 0.0199\n",
      "step 3300: loss: 0.0192\n",
      "step 3400: loss: 0.0175\n",
      "step 3500: loss: 0.0178\n",
      "step 3600: loss: 0.0168\n",
      "step 3700: loss: 0.0170\n",
      "step 3800: loss: 0.0153\n",
      "step 3900: loss: 0.0174\n",
      "step 4000: loss: 0.0169\n",
      "\tstep 4000: saved model\n",
      "step 4100: loss: 0.0155\n",
      "step 4200: loss: 0.0142\n",
      "step 4300: loss: 0.0150\n",
      "step 4400: loss: 0.0142\n",
      "step 4500: loss: 0.0134\n",
      "step 4600: loss: 0.0138\n",
      "step 4700: loss: 0.0182\n",
      "step 4800: loss: 0.0159\n",
      "step 4900: loss: 0.0145\n",
      "step 5000: loss: 0.0173\n",
      "step 5100: loss: 0.0170\n",
      "step 5200: loss: 0.0147\n",
      "step 5300: loss: 0.0149\n",
      "step 5400: loss: 0.0133\n",
      "step 5500: loss: 0.0118\n",
      "step 5600: loss: 0.0127\n",
      "step 5700: loss: 0.0117\n",
      "step 5800: loss: 0.0134\n",
      "step 5900: loss: 0.0113\n",
      "step 6000: loss: 0.0115\n",
      "step 6100: loss: 0.0113\n",
      "step 6200: loss: 0.0126\n",
      "step 6300: loss: 0.0145\n",
      "step 6400: loss: 0.0147\n",
      "step 6500: loss: 0.0118\n",
      "step 6600: loss: 0.0116\n",
      "step 6700: loss: 0.0122\n",
      "step 6800: loss: 0.0117\n",
      "step 6900: loss: 0.0113\n",
      "step 7000: loss: 0.0096\n",
      "step 7100: loss: 0.0103\n",
      "step 7200: loss: 0.0103\n",
      "step 7300: loss: 0.0104\n",
      "step 7400: loss: 0.0110\n",
      "step 7500: loss: 0.0110\n",
      "step 7600: loss: 0.0103\n",
      "step 7700: loss: 0.0116\n",
      "step 7800: loss: 0.0109\n",
      "step 7900: loss: 0.0093\n",
      "step 8000: loss: 0.0097\n",
      "step 8100: loss: 0.0094\n",
      "step 8200: loss: 0.0097\n",
      "step 8300: loss: 0.0101\n",
      "step 8400: loss: 0.0143\n",
      "step 8500: loss: 0.0132\n",
      "step 8600: loss: 0.0108\n",
      "step 8700: loss: 0.0095\n",
      "step 8800: loss: 0.0081\n",
      "step 8900: loss: 0.0077\n",
      "step 9000: loss: 0.0089\n",
      "step 9100: loss: 0.0092\n",
      "step 9200: loss: 0.0092\n",
      "step 9300: loss: 0.0101\n",
      "step 9400: loss: 0.0089\n",
      "step 9500: loss: 0.0109\n",
      "step 9600: loss: 0.0088\n",
      "step 9700: loss: 0.0091\n",
      "step 9800: loss: 0.0083\n",
      "step 9900: loss: 0.0081\n",
      "step 10000: loss: 0.0078\n",
      "\tstep 10000: saved model\n",
      "step 10100: loss: 0.0088\n",
      "step 10200: loss: 0.0078\n",
      "step 10300: loss: 0.0069\n",
      "step 10400: loss: 0.0079\n",
      "step 10500: loss: 0.0077\n",
      "step 10600: loss: 0.0070\n",
      "step 10700: loss: 0.0094\n",
      "step 10800: loss: 0.0088\n",
      "step 10900: loss: 0.0074\n",
      "step 11000: loss: 0.0064\n",
      "step 11100: loss: 0.0071\n",
      "step 11200: loss: 0.0095\n",
      "step 11300: loss: 0.0087\n",
      "step 11400: loss: 0.0091\n",
      "step 11500: loss: 0.0078\n",
      "step 11600: loss: 0.0089\n",
      "step 11700: loss: 0.0088\n",
      "step 11800: loss: 0.0084\n",
      "step 11900: loss: 0.0082\n",
      "step 12000: loss: 0.0073\n",
      "step 12100: loss: 0.0069\n",
      "step 12200: loss: 0.0064\n",
      "step 12300: loss: 0.0068\n",
      "step 12400: loss: 0.0063\n",
      "step 12500: loss: 0.0064\n",
      "step 12600: loss: 0.0064\n",
      "step 12700: loss: 0.0074\n",
      "step 12800: loss: 0.0079\n",
      "step 12900: loss: 0.0077\n",
      "step 13000: loss: 0.0066\n",
      "step 13100: loss: 0.0070\n",
      "step 13200: loss: 0.0065\n",
      "step 13300: loss: 0.0067\n",
      "step 13400: loss: 0.0074\n",
      "step 13500: loss: 0.0072\n",
      "step 13600: loss: 0.0064\n",
      "step 13700: loss: 0.0058\n",
      "step 13800: loss: 0.0070\n",
      "step 13900: loss: 0.0065\n",
      "step 14000: loss: 0.0067\n",
      "step 14100: loss: 0.0061\n",
      "step 14200: loss: 0.0063\n",
      "step 14300: loss: 0.0056\n",
      "step 14400: loss: 0.0051\n",
      "step 14500: loss: 0.0076\n",
      "step 14600: loss: 0.0065\n",
      "step 14700: loss: 0.0053\n",
      "step 14800: loss: 0.0054\n",
      "step 14900: loss: 0.0051\n",
      "step 15000: loss: 0.0056\n",
      "step 15100: loss: 0.0066\n",
      "step 15200: loss: 0.0058\n",
      "step 15300: loss: 0.0069\n",
      "step 15400: loss: 0.0066\n",
      "step 15500: loss: 0.0056\n",
      "step 15600: loss: 0.0054\n",
      "step 15700: loss: 0.0052\n",
      "step 15800: loss: 0.0048\n",
      "step 15900: loss: 0.0052\n",
      "step 16000: loss: 0.0055\n",
      "\tstep 16000: saved model\n",
      "step 16100: loss: 0.0051\n",
      "step 16200: loss: 0.0049\n",
      "step 16300: loss: 0.0052\n",
      "step 16400: loss: 0.0052\n",
      "step 16500: loss: 0.0062\n",
      "step 16600: loss: 0.0057\n",
      "step 16700: loss: 0.0051\n",
      "step 16800: loss: 0.0051\n",
      "step 16900: loss: 0.0047\n",
      "step 17000: loss: 0.0045\n",
      "step 17100: loss: 0.0053\n",
      "step 17200: loss: 0.0057\n",
      "step 17300: loss: 0.0072\n",
      "step 17400: loss: 0.0069\n",
      "step 17500: loss: 0.0059\n",
      "step 17600: loss: 0.0048\n",
      "step 17700: loss: 0.0041\n",
      "step 17800: loss: 0.0043\n",
      "step 17900: loss: 0.0044\n",
      "step 18000: loss: 0.0041\n",
      "step 18100: loss: 0.0046\n",
      "step 18200: loss: 0.0049\n",
      "step 18300: loss: 0.0043\n",
      "step 18400: loss: 0.0041\n",
      "step 18500: loss: 0.0050\n",
      "step 18600: loss: 0.0051\n",
      "step 18700: loss: 0.0051\n",
      "step 18800: loss: 0.0046\n",
      "step 18900: loss: 0.0044\n",
      "step 19000: loss: 0.0043\n",
      "step 19100: loss: 0.0043\n",
      "step 19200: loss: 0.0045\n",
      "step 19300: loss: 0.0048\n",
      "step 19400: loss: 0.0046\n",
      "step 19500: loss: 0.0044\n",
      "step 19600: loss: 0.0044\n",
      "step 19700: loss: 0.0048\n",
      "step 19800: loss: 0.0046\n",
      "step 19900: loss: 0.0041\n",
      "step 20000: loss: 0.0043\n",
      "step 20100: loss: 0.0044\n",
      "step 20200: loss: 0.0048\n",
      "step 20300: loss: 0.0047\n",
      "step 20400: loss: 0.0057\n",
      "step 20500: loss: 0.0055\n",
      "step 20600: loss: 0.0050\n",
      "step 20700: loss: 0.0042\n",
      "step 20800: loss: 0.0039\n",
      "step 20900: loss: 0.0044\n",
      "step 21000: loss: 0.0040\n",
      "step 21100: loss: 0.0037\n",
      "step 21200: loss: 0.0038\n",
      "step 21300: loss: 0.0042\n",
      "step 21400: loss: 0.0045\n",
      "step 21500: loss: 0.0043\n",
      "step 21600: loss: 0.0056\n",
      "step 21700: loss: 0.0059\n",
      "step 21800: loss: 0.0058\n",
      "step 21900: loss: 0.0063\n",
      "step 22000: loss: 0.0060\n",
      "step 22100: loss: 0.0051\n",
      "step 22200: loss: 0.0046\n",
      "step 22300: loss: 0.0046\n",
      "step 22400: loss: 0.0042\n",
      "step 22500: loss: 0.0042\n",
      "step 22600: loss: 0.0040\n",
      "step 22700: loss: 0.0042\n",
      "step 22800: loss: 0.0056\n",
      "step 22900: loss: 0.0053\n",
      "step 23000: loss: 0.0048\n",
      "step 23100: loss: 0.0042\n",
      "step 23200: loss: 0.0038\n",
      "step 23300: loss: 0.0038\n",
      "step 23400: loss: 0.0034\n",
      "step 23500: loss: 0.0037\n",
      "step 23600: loss: 0.0038\n",
      "step 23700: loss: 0.0041\n",
      "step 23800: loss: 0.0039\n",
      "step 23900: loss: 0.0044\n",
      "step 24000: loss: 0.0045\n",
      "step 24100: loss: 0.0040\n",
      "step 24200: loss: 0.0062\n",
      "step 24300: loss: 0.0059\n",
      "step 24400: loss: 0.0053\n",
      "step 24500: loss: 0.0056\n",
      "step 24600: loss: 0.0048\n",
      "step 24700: loss: 0.0039\n",
      "step 24800: loss: 0.0045\n",
      "step 24900: loss: 0.0051\n",
      "step 25000: loss: 0.0048\n",
      "step 25100: loss: 0.0042\n",
      "step 25200: loss: 0.0036\n",
      "step 25300: loss: 0.0034\n",
      "step 25400: loss: 0.0040\n",
      "step 25500: loss: 0.0038\n",
      "step 25600: loss: 0.0035\n",
      "step 25700: loss: 0.0038\n",
      "step 25800: loss: 0.0034\n",
      "step 25900: loss: 0.0036\n",
      "step 26000: loss: 0.0037\n",
      "step 26100: loss: 0.0034\n",
      "step 26200: loss: 0.0032\n",
      "step 26300: loss: 0.0032\n",
      "step 26400: loss: 0.0037\n",
      "step 26500: loss: 0.0034\n",
      "step 26600: loss: 0.0034\n",
      "step 26700: loss: 0.0038\n",
      "step 26800: loss: 0.0037\n",
      "step 26900: loss: 0.0038\n",
      "step 27000: loss: 0.0034\n",
      "step 27100: loss: 0.0036\n",
      "step 27200: loss: 0.0036\n",
      "step 27300: loss: 0.0030\n",
      "step 27400: loss: 0.0030\n",
      "step 27500: loss: 0.0028\n",
      "step 27600: loss: 0.0041\n",
      "step 27700: loss: 0.0037\n",
      "step 27800: loss: 0.0034\n",
      "step 27900: loss: 0.0036\n",
      "step 28000: loss: 0.0033\n",
      "step 28100: loss: 0.0037\n",
      "step 28200: loss: 0.0034\n",
      "step 28300: loss: 0.0034\n",
      "step 28400: loss: 0.0037\n",
      "step 28500: loss: 0.0032\n",
      "step 28600: loss: 0.0032\n",
      "step 28700: loss: 0.0032\n",
      "step 28800: loss: 0.0037\n",
      "step 28900: loss: 0.0036\n",
      "step 29000: loss: 0.0037\n",
      "step 29100: loss: 0.0036\n",
      "step 29200: loss: 0.0031\n",
      "step 29300: loss: 0.0030\n",
      "step 29400: loss: 0.0028\n",
      "step 29500: loss: 0.0028\n",
      "step 29600: loss: 0.0032\n",
      "step 29700: loss: 0.0035\n",
      "step 29800: loss: 0.0040\n",
      "step 29900: loss: 0.0036\n",
      "step 30000: loss: 0.0034\n",
      "\tstep 30000: saved model\n",
      "step 30100: loss: 0.0031\n",
      "step 30200: loss: 0.0049\n",
      "step 30300: loss: 0.0051\n",
      "step 30400: loss: 0.0039\n",
      "step 30500: loss: 0.0036\n",
      "step 30600: loss: 0.0034\n",
      "step 30700: loss: 0.0035\n",
      "step 30800: loss: 0.0031\n",
      "step 30900: loss: 0.0029\n",
      "step 31000: loss: 0.0029\n",
      "step 31100: loss: 0.0047\n",
      "step 31200: loss: 0.0036\n",
      "step 31300: loss: 0.0032\n",
      "step 31400: loss: 0.0029\n",
      "step 31500: loss: 0.0027\n",
      "step 31600: loss: 0.0030\n",
      "step 31700: loss: 0.0028\n",
      "step 31800: loss: 0.0027\n",
      "step 31900: loss: 0.0026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 32000: loss: 0.0030\n",
      "step 32100: loss: 0.0036\n",
      "step 32200: loss: 0.0038\n",
      "step 32300: loss: 0.0037\n",
      "step 32400: loss: 0.0034\n",
      "step 32500: loss: 0.0044\n",
      "step 32600: loss: 0.0035\n",
      "step 32700: loss: 0.0030\n",
      "step 32800: loss: 0.0033\n",
      "step 32900: loss: 0.0036\n",
      "step 33000: loss: 0.0034\n",
      "step 33100: loss: 0.0034\n",
      "step 33200: loss: 0.0037\n",
      "step 33300: loss: 0.0035\n",
      "step 33400: loss: 0.0029\n",
      "step 33500: loss: 0.0048\n",
      "step 33600: loss: 0.0040\n",
      "step 33700: loss: 0.0033\n",
      "step 33800: loss: 0.0031\n",
      "step 33900: loss: 0.0027\n",
      "step 34000: loss: 0.0030\n",
      "step 34100: loss: 0.0034\n",
      "step 34200: loss: 0.0033\n",
      "step 34300: loss: 0.0030\n",
      "step 34400: loss: 0.0028\n",
      "step 34500: loss: 0.0032\n",
      "step 34600: loss: 0.0030\n",
      "step 34700: loss: 0.0031\n",
      "step 34800: loss: 0.0029\n",
      "step 34900: loss: 0.0026\n",
      "step 35000: loss: 0.0031\n",
      "step 35100: loss: 0.0035\n",
      "step 35200: loss: 0.0031\n",
      "step 35300: loss: 0.0028\n",
      "step 35400: loss: 0.0026\n",
      "step 35500: loss: 0.0046\n",
      "step 35600: loss: 0.0037\n",
      "step 35700: loss: 0.0034\n",
      "step 35800: loss: 0.0030\n",
      "step 35900: loss: 0.0031\n",
      "step 36000: loss: 0.0029\n",
      "step 36100: loss: 0.0026\n",
      "step 36200: loss: 0.0028\n",
      "step 36300: loss: 0.0030\n",
      "step 36400: loss: 0.0028\n",
      "step 36500: loss: 0.0025\n",
      "step 36600: loss: 0.0022\n",
      "step 36700: loss: 0.0027\n",
      "step 36800: loss: 0.0029\n",
      "step 36900: loss: 0.0027\n",
      "step 37000: loss: 0.0027\n",
      "step 37100: loss: 0.0027\n",
      "step 37200: loss: 0.0036\n",
      "step 37300: loss: 0.0038\n",
      "step 37400: loss: 0.0036\n",
      "step 37500: loss: 0.0030\n",
      "step 37600: loss: 0.0031\n",
      "step 37700: loss: 0.0032\n",
      "step 37800: loss: 0.0027\n",
      "step 37900: loss: 0.0030\n",
      "step 38000: loss: 0.0027\n",
      "step 38100: loss: 0.0028\n",
      "step 38200: loss: 0.0025\n",
      "step 38300: loss: 0.0023\n",
      "step 38400: loss: 0.0025\n",
      "step 38500: loss: 0.0026\n",
      "step 38600: loss: 0.0025\n",
      "step 38700: loss: 0.0026\n",
      "step 38800: loss: 0.0030\n",
      "step 38900: loss: 0.0028\n",
      "step 39000: loss: 0.0026\n",
      "step 39100: loss: 0.0026\n",
      "step 39200: loss: 0.0022\n",
      "step 39300: loss: 0.0021\n",
      "step 39400: loss: 0.0023\n",
      "step 39500: loss: 0.0023\n",
      "step 39600: loss: 0.0025\n",
      "step 39700: loss: 0.0027\n",
      "step 39800: loss: 0.0026\n",
      "step 39900: loss: 0.0027\n",
      "step 40000: loss: 0.0027\n",
      "step 40100: loss: 0.0026\n",
      "step 40200: loss: 0.0026\n",
      "step 40300: loss: 0.0026\n",
      "step 40400: loss: 0.0028\n",
      "step 40500: loss: 0.0028\n",
      "step 40600: loss: 0.0025\n",
      "step 40700: loss: 0.0021\n",
      "step 40800: loss: 0.0024\n",
      "step 40900: loss: 0.0024\n",
      "step 41000: loss: 0.0025\n",
      "step 41100: loss: 0.0025\n",
      "step 41200: loss: 0.0028\n",
      "step 41300: loss: 0.0027\n",
      "step 41400: loss: 0.0025\n",
      "step 41500: loss: 0.0023\n",
      "step 41600: loss: 0.0027\n",
      "step 41700: loss: 0.0026\n",
      "step 41800: loss: 0.0024\n",
      "step 41900: loss: 0.0023\n",
      "step 42000: loss: 0.0025\n",
      "step 42100: loss: 0.0025\n",
      "step 42200: loss: 0.0028\n",
      "step 42300: loss: 0.0030\n",
      "step 42400: loss: 0.0031\n",
      "step 42500: loss: 0.0027\n",
      "step 42600: loss: 0.0034\n",
      "step 42700: loss: 0.0031\n",
      "step 42800: loss: 0.0027\n",
      "step 42900: loss: 0.0027\n",
      "step 43000: loss: 0.0028\n",
      "step 43100: loss: 0.0028\n",
      "step 43200: loss: 0.0033\n",
      "step 43300: loss: 0.0029\n",
      "step 43400: loss: 0.0031\n",
      "step 43500: loss: 0.0030\n",
      "step 43600: loss: 0.0028\n",
      "step 43700: loss: 0.0026\n",
      "step 43800: loss: 0.0025\n",
      "step 43900: loss: 0.0028\n",
      "step 44000: loss: 0.0026\n",
      "step 44100: loss: 0.0024\n",
      "step 44200: loss: 0.0023\n",
      "step 44300: loss: 0.0024\n",
      "step 44400: loss: 0.0022\n",
      "step 44500: loss: 0.0021\n",
      "step 44600: loss: 0.0023\n",
      "step 44700: loss: 0.0021\n",
      "step 44800: loss: 0.0021\n",
      "step 44900: loss: 0.0028\n",
      "step 45000: loss: 0.0029\n",
      "step 45100: loss: 0.0026\n",
      "step 45200: loss: 0.0026\n",
      "step 45300: loss: 0.0022\n",
      "step 45400: loss: 0.0022\n",
      "step 45500: loss: 0.0025\n",
      "step 45600: loss: 0.0026\n",
      "step 45700: loss: 0.0024\n",
      "step 45800: loss: 0.0026\n",
      "step 45900: loss: 0.0024\n",
      "step 46000: loss: 0.0024\n",
      "step 46100: loss: 0.0024\n",
      "step 46200: loss: 0.0026\n",
      "step 46300: loss: 0.0027\n",
      "step 46400: loss: 0.0028\n",
      "step 46500: loss: 0.0026\n",
      "step 46600: loss: 0.0023\n",
      "step 46700: loss: 0.0026\n",
      "step 46800: loss: 0.0024\n",
      "step 46900: loss: 0.0021\n",
      "step 47000: loss: 0.0023\n",
      "step 47100: loss: 0.0023\n",
      "step 47200: loss: 0.0021\n",
      "step 47300: loss: 0.0023\n",
      "step 47400: loss: 0.0021\n",
      "step 47500: loss: 0.0024\n",
      "step 47600: loss: 0.0023\n",
      "step 47700: loss: 0.0021\n",
      "step 47800: loss: 0.0021\n",
      "step 47900: loss: 0.0022\n",
      "step 48000: loss: 0.0019\n",
      "step 48100: loss: 0.0021\n",
      "step 48200: loss: 0.0022\n",
      "step 48300: loss: 0.0021\n",
      "step 48400: loss: 0.0021\n",
      "step 48500: loss: 0.0019\n",
      "step 48600: loss: 0.0019\n",
      "step 48700: loss: 0.0021\n",
      "step 48800: loss: 0.0022\n",
      "step 48900: loss: 0.0024\n",
      "step 49000: loss: 0.0032\n",
      "step 49100: loss: 0.0028\n",
      "step 49200: loss: 0.0024\n",
      "step 49300: loss: 0.0024\n",
      "step 49400: loss: 0.0024\n",
      "step 49500: loss: 0.0020\n",
      "step 49600: loss: 0.0020\n",
      "step 49700: loss: 0.0024\n",
      "step 49800: loss: 0.0029\n",
      "step 49900: loss: 0.0027\n",
      "step 50000: loss: 0.0024\n",
      "step 50100: loss: 0.0021\n",
      "step 50200: loss: 0.0021\n",
      "step 50300: loss: 0.0026\n",
      "step 50400: loss: 0.0023\n",
      "step 50500: loss: 0.0023\n",
      "step 50600: loss: 0.0021\n",
      "step 50700: loss: 0.0022\n",
      "step 50800: loss: 0.0024\n",
      "step 50900: loss: 0.0022\n",
      "step 51000: loss: 0.0028\n",
      "step 51100: loss: 0.0024\n",
      "step 51200: loss: 0.0022\n",
      "step 51300: loss: 0.0022\n",
      "step 51400: loss: 0.0019\n",
      "step 51500: loss: 0.0024\n",
      "step 51600: loss: 0.0020\n",
      "step 51700: loss: 0.0019\n",
      "step 51800: loss: 0.0020\n",
      "step 51900: loss: 0.0025\n",
      "step 52000: loss: 0.0029\n",
      "step 52100: loss: 0.0029\n",
      "step 52200: loss: 0.0025\n",
      "step 52300: loss: 0.0024\n",
      "step 52400: loss: 0.0026\n",
      "step 52500: loss: 0.0024\n",
      "step 52600: loss: 0.0025\n",
      "step 52700: loss: 0.0023\n",
      "step 52800: loss: 0.0022\n",
      "step 52900: loss: 0.0028\n",
      "step 53000: loss: 0.0026\n",
      "step 53100: loss: 0.0028\n",
      "step 53200: loss: 0.0023\n",
      "step 53300: loss: 0.0020\n",
      "step 53400: loss: 0.0018\n",
      "step 53500: loss: 0.0019\n",
      "step 53600: loss: 0.0021\n",
      "step 53700: loss: 0.0024\n",
      "step 53800: loss: 0.0025\n",
      "step 53900: loss: 0.0023\n",
      "step 54000: loss: 0.0022\n",
      "step 54100: loss: 0.0023\n",
      "step 54200: loss: 0.0022\n",
      "step 54300: loss: 0.0022\n",
      "step 54400: loss: 0.0021\n",
      "step 54500: loss: 0.0020\n",
      "step 54600: loss: 0.0022\n",
      "step 54700: loss: 0.0025\n",
      "step 54800: loss: 0.0024\n",
      "step 54900: loss: 0.0022\n",
      "step 55000: loss: 0.0024\n",
      "step 55100: loss: 0.0024\n",
      "step 55200: loss: 0.0022\n",
      "step 55300: loss: 0.0026\n",
      "step 55400: loss: 0.0026\n",
      "step 55500: loss: 0.0023\n",
      "step 55600: loss: 0.0025\n",
      "step 55700: loss: 0.0026\n",
      "step 55800: loss: 0.0026\n",
      "step 55900: loss: 0.0026\n",
      "step 56000: loss: 0.0025\n",
      "step 56100: loss: 0.0022\n",
      "step 56200: loss: 0.0023\n",
      "step 56300: loss: 0.0025\n",
      "step 56400: loss: 0.0024\n",
      "step 56500: loss: 0.0023\n",
      "step 56600: loss: 0.0026\n",
      "step 56700: loss: 0.0033\n",
      "step 56800: loss: 0.0029\n",
      "step 56900: loss: 0.0028\n",
      "step 57000: loss: 0.0026\n",
      "step 57100: loss: 0.0024\n",
      "step 57200: loss: 0.0028\n",
      "step 57300: loss: 0.0024\n",
      "step 57400: loss: 0.0025\n",
      "step 57500: loss: 0.0022\n",
      "step 57600: loss: 0.0019\n",
      "step 57700: loss: 0.0020\n",
      "step 57800: loss: 0.0019\n",
      "step 57900: loss: 0.0023\n",
      "step 58000: loss: 0.0022\n",
      "step 58100: loss: 0.0022\n",
      "step 58200: loss: 0.0021\n",
      "step 58300: loss: 0.0019\n",
      "step 58400: loss: 0.0018\n",
      "step 58500: loss: 0.0018\n",
      "step 58600: loss: 0.0022\n",
      "step 58700: loss: 0.0021\n",
      "step 58800: loss: 0.0020\n",
      "step 58900: loss: 0.0025\n",
      "step 59000: loss: 0.0023\n",
      "step 59100: loss: 0.0024\n",
      "step 59200: loss: 0.0023\n",
      "step 59300: loss: 0.0022\n",
      "step 59400: loss: 0.0023\n",
      "step 59500: loss: 0.0021\n",
      "step 59600: loss: 0.0027\n",
      "step 59700: loss: 0.0024\n",
      "step 59800: loss: 0.0023\n",
      "step 59900: loss: 0.0021\n",
      "step 60000: loss: 0.0022\n",
      "step 60100: loss: 0.0023\n",
      "step 60200: loss: 0.0023\n",
      "step 60300: loss: 0.0021\n",
      "step 60400: loss: 0.0018\n",
      "step 60500: loss: 0.0020\n",
      "step 60600: loss: 0.0023\n",
      "step 60700: loss: 0.0021\n",
      "step 60800: loss: 0.0024\n",
      "step 60900: loss: 0.0025\n",
      "step 61000: loss: 0.0023\n",
      "step 61100: loss: 0.0021\n",
      "step 61200: loss: 0.0019\n",
      "step 61300: loss: 0.0019\n",
      "step 61400: loss: 0.0018\n",
      "step 61500: loss: 0.0017\n",
      "step 61600: loss: 0.0017\n",
      "step 61700: loss: 0.0018\n",
      "step 61800: loss: 0.0017\n",
      "step 61900: loss: 0.0017\n",
      "step 62000: loss: 0.0019\n",
      "step 62100: loss: 0.0022\n",
      "step 62200: loss: 0.0020\n",
      "step 62300: loss: 0.0019\n",
      "step 62400: loss: 0.0017\n",
      "step 62500: loss: 0.0017\n",
      "step 62600: loss: 0.0016\n",
      "step 62700: loss: 0.0020\n",
      "step 62800: loss: 0.0017\n",
      "step 62900: loss: 0.0019\n",
      "step 63000: loss: 0.0018\n",
      "step 63100: loss: 0.0021\n",
      "step 63200: loss: 0.0019\n",
      "step 63300: loss: 0.0018\n",
      "step 63400: loss: 0.0017\n",
      "step 63500: loss: 0.0020\n",
      "step 63600: loss: 0.0020\n",
      "step 63700: loss: 0.0021\n",
      "step 63800: loss: 0.0020\n",
      "step 63900: loss: 0.0021\n",
      "step 64000: loss: 0.0021\n",
      "\tstep 64000: saved model\n",
      "step 64100: loss: 0.0025\n",
      "step 64200: loss: 0.0025\n",
      "step 64300: loss: 0.0021\n",
      "step 64400: loss: 0.0019\n",
      "step 64500: loss: 0.0021\n",
      "step 64600: loss: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 64700: loss: 0.0020\n",
      "step 64800: loss: 0.0021\n",
      "step 64900: loss: 0.0020\n",
      "step 65000: loss: 0.0020\n",
      "step 65100: loss: 0.0019\n",
      "step 65200: loss: 0.0017\n",
      "step 65300: loss: 0.0017\n",
      "step 65400: loss: 0.0017\n",
      "step 65500: loss: 0.0016\n",
      "step 65600: loss: 0.0018\n",
      "step 65700: loss: 0.0019\n",
      "step 65800: loss: 0.0020\n",
      "step 65900: loss: 0.0020\n",
      "step 66000: loss: 0.0021\n",
      "step 66100: loss: 0.0020\n",
      "step 66200: loss: 0.0019\n",
      "step 66300: loss: 0.0017\n",
      "step 66400: loss: 0.0017\n",
      "step 66500: loss: 0.0019\n",
      "step 66600: loss: 0.0017\n",
      "step 66700: loss: 0.0019\n",
      "step 66800: loss: 0.0018\n",
      "step 66900: loss: 0.0019\n",
      "step 67000: loss: 0.0019\n",
      "step 67100: loss: 0.0017\n",
      "step 67200: loss: 0.0017\n",
      "step 67300: loss: 0.0019\n",
      "step 67400: loss: 0.0017\n",
      "step 67500: loss: 0.0020\n",
      "step 67600: loss: 0.0019\n",
      "step 67700: loss: 0.0017\n",
      "step 67800: loss: 0.0016\n",
      "step 67900: loss: 0.0016\n",
      "step 68000: loss: 0.0019\n",
      "step 68100: loss: 0.0020\n",
      "step 68200: loss: 0.0023\n",
      "step 68300: loss: 0.0022\n",
      "step 68400: loss: 0.0021\n",
      "step 68500: loss: 0.0020\n",
      "step 68600: loss: 0.0019\n",
      "step 68700: loss: 0.0017\n",
      "step 68800: loss: 0.0022\n",
      "step 68900: loss: 0.0023\n",
      "step 69000: loss: 0.0022\n",
      "step 69100: loss: 0.0027\n",
      "step 69200: loss: 0.0031\n",
      "step 69300: loss: 0.0024\n",
      "step 69400: loss: 0.0028\n",
      "step 69500: loss: 0.0027\n",
      "step 69600: loss: 0.0024\n",
      "step 69700: loss: 0.0025\n",
      "step 69800: loss: 0.0023\n",
      "step 69900: loss: 0.0024\n",
      "step 70000: loss: 0.0028\n",
      "step 70100: loss: 0.0022\n",
      "step 70200: loss: 0.0021\n",
      "step 70300: loss: 0.0022\n",
      "step 70400: loss: 0.0020\n",
      "step 70500: loss: 0.0022\n",
      "step 70600: loss: 0.0018\n",
      "step 70700: loss: 0.0020\n",
      "step 70800: loss: 0.0020\n",
      "step 70900: loss: 0.0020\n",
      "step 71000: loss: 0.0018\n",
      "step 71100: loss: 0.0020\n",
      "step 71200: loss: 0.0023\n",
      "step 71300: loss: 0.0018\n",
      "step 71400: loss: 0.0018\n",
      "step 71500: loss: 0.0018\n",
      "step 71600: loss: 0.0017\n",
      "step 71700: loss: 0.0019\n",
      "step 71800: loss: 0.0019\n",
      "step 71900: loss: 0.0018\n",
      "step 72000: loss: 0.0017\n",
      "step 72100: loss: 0.0026\n",
      "step 72200: loss: 0.0026\n",
      "step 72300: loss: 0.0026\n",
      "step 72400: loss: 0.0024\n",
      "step 72500: loss: 0.0023\n",
      "step 72600: loss: 0.0022\n",
      "step 72700: loss: 0.0022\n",
      "step 72800: loss: 0.0025\n",
      "step 72900: loss: 0.0021\n",
      "step 73000: loss: 0.0022\n",
      "step 73100: loss: 0.0021\n",
      "step 73200: loss: 0.0019\n",
      "step 73300: loss: 0.0018\n",
      "step 73400: loss: 0.0019\n",
      "step 73500: loss: 0.0019\n",
      "step 73600: loss: 0.0020\n",
      "step 73700: loss: 0.0019\n",
      "step 73800: loss: 0.0018\n",
      "step 73900: loss: 0.0018\n",
      "step 74000: loss: 0.0020\n",
      "step 74100: loss: 0.0019\n",
      "step 74200: loss: 0.0018\n",
      "step 74300: loss: 0.0017\n",
      "step 74400: loss: 0.0018\n",
      "step 74500: loss: 0.0015\n",
      "step 74600: loss: 0.0016\n",
      "step 74700: loss: 0.0022\n",
      "step 74800: loss: 0.0018\n",
      "step 74900: loss: 0.0018\n",
      "step 75000: loss: 0.0022\n",
      "step 75100: loss: 0.0017\n",
      "step 75200: loss: 0.0018\n",
      "step 75300: loss: 0.0018\n",
      "step 75400: loss: 0.0018\n",
      "step 75500: loss: 0.0020\n",
      "step 75600: loss: 0.0019\n",
      "step 75700: loss: 0.0015\n",
      "step 75800: loss: 0.0021\n",
      "step 75900: loss: 0.0020\n",
      "step 76000: loss: 0.0024\n",
      "step 76100: loss: 0.0024\n",
      "step 76200: loss: 0.0020\n",
      "step 76300: loss: 0.0019\n",
      "step 76400: loss: 0.0018\n",
      "step 76500: loss: 0.0016\n",
      "step 76600: loss: 0.0018\n",
      "step 76700: loss: 0.0018\n",
      "step 76800: loss: 0.0016\n",
      "step 76900: loss: 0.0017\n",
      "step 77000: loss: 0.0016\n",
      "step 77100: loss: 0.0015\n",
      "step 77200: loss: 0.0017\n",
      "step 77300: loss: 0.0017\n",
      "step 77400: loss: 0.0018\n",
      "step 77500: loss: 0.0028\n",
      "step 77600: loss: 0.0024\n",
      "step 77700: loss: 0.0019\n",
      "step 77800: loss: 0.0018\n",
      "step 77900: loss: 0.0019\n",
      "step 78000: loss: 0.0019\n",
      "step 78100: loss: 0.0021\n",
      "step 78200: loss: 0.0018\n",
      "step 78300: loss: 0.0016\n",
      "step 78400: loss: 0.0015\n",
      "step 78500: loss: 0.0016\n",
      "step 78600: loss: 0.0018\n",
      "step 78700: loss: 0.0018\n",
      "step 78800: loss: 0.0018\n",
      "step 78900: loss: 0.0019\n",
      "step 79000: loss: 0.0018\n",
      "step 79100: loss: 0.0016\n",
      "step 79200: loss: 0.0015\n",
      "step 79300: loss: 0.0018\n",
      "step 79400: loss: 0.0017\n",
      "step 79500: loss: 0.0019\n",
      "step 79600: loss: 0.0018\n",
      "step 79700: loss: 0.0017\n",
      "step 79800: loss: 0.0015\n",
      "step 79900: loss: 0.0017\n",
      "step 80000: loss: 0.0017\n",
      "step 80100: loss: 0.0018\n",
      "step 80200: loss: 0.0021\n",
      "step 80300: loss: 0.0019\n",
      "step 80400: loss: 0.0018\n",
      "step 80500: loss: 0.0017\n",
      "step 80600: loss: 0.0021\n",
      "step 80700: loss: 0.0018\n",
      "step 80800: loss: 0.0016\n",
      "step 80900: loss: 0.0020\n",
      "step 81000: loss: 0.0018\n",
      "step 81100: loss: 0.0018\n",
      "step 81200: loss: 0.0019\n",
      "step 81300: loss: 0.0016\n",
      "step 81400: loss: 0.0017\n",
      "step 81500: loss: 0.0016\n",
      "step 81600: loss: 0.0016\n",
      "step 81700: loss: 0.0017\n",
      "step 81800: loss: 0.0018\n",
      "step 81900: loss: 0.0022\n",
      "step 82000: loss: 0.0021\n",
      "step 82100: loss: 0.0019\n",
      "step 82200: loss: 0.0020\n",
      "step 82300: loss: 0.0019\n",
      "step 82400: loss: 0.0018\n",
      "step 82500: loss: 0.0019\n",
      "step 82600: loss: 0.0019\n",
      "step 82700: loss: 0.0023\n",
      "step 82800: loss: 0.0020\n",
      "step 82900: loss: 0.0017\n",
      "step 83000: loss: 0.0017\n",
      "step 83100: loss: 0.0018\n",
      "step 83200: loss: 0.0020\n",
      "step 83300: loss: 0.0020\n",
      "step 83400: loss: 0.0017\n",
      "step 83500: loss: 0.0016\n",
      "step 83600: loss: 0.0018\n",
      "step 83700: loss: 0.0023\n",
      "step 83800: loss: 0.0021\n",
      "step 83900: loss: 0.0018\n",
      "step 84000: loss: 0.0018\n",
      "step 84100: loss: 0.0021\n",
      "step 84200: loss: 0.0020\n",
      "step 84300: loss: 0.0022\n",
      "step 84400: loss: 0.0018\n",
      "step 84500: loss: 0.0019\n",
      "step 84600: loss: 0.0016\n",
      "step 84700: loss: 0.0016\n",
      "step 84800: loss: 0.0017\n",
      "step 84900: loss: 0.0017\n",
      "step 85000: loss: 0.0016\n",
      "step 85100: loss: 0.0015\n",
      "step 85200: loss: 0.0014\n",
      "step 85300: loss: 0.0016\n",
      "step 85400: loss: 0.0017\n",
      "step 85500: loss: 0.0017\n",
      "step 85600: loss: 0.0016\n",
      "step 85700: loss: 0.0015\n",
      "step 85800: loss: 0.0016\n",
      "step 85900: loss: 0.0016\n",
      "step 86000: loss: 0.0019\n",
      "step 86100: loss: 0.0018\n",
      "step 86200: loss: 0.0019\n",
      "step 86300: loss: 0.0018\n",
      "step 86400: loss: 0.0019\n",
      "step 86500: loss: 0.0018\n",
      "step 86600: loss: 0.0016\n",
      "step 86700: loss: 0.0014\n",
      "step 86800: loss: 0.0014\n",
      "step 86900: loss: 0.0014\n",
      "step 87000: loss: 0.0015\n",
      "step 87100: loss: 0.0015\n",
      "step 87200: loss: 0.0014\n",
      "step 87300: loss: 0.0016\n",
      "step 87400: loss: 0.0014\n",
      "step 87500: loss: 0.0014\n",
      "step 87600: loss: 0.0013\n",
      "step 87700: loss: 0.0015\n",
      "step 87800: loss: 0.0015\n",
      "step 87900: loss: 0.0024\n",
      "step 88000: loss: 0.0021\n",
      "step 88100: loss: 0.0017\n",
      "step 88200: loss: 0.0015\n",
      "step 88300: loss: 0.0025\n",
      "step 88400: loss: 0.0019\n",
      "step 88500: loss: 0.0019\n",
      "step 88600: loss: 0.0018\n",
      "step 88700: loss: 0.0018\n",
      "step 88800: loss: 0.0017\n",
      "step 88900: loss: 0.0018\n",
      "step 89000: loss: 0.0017\n",
      "step 89100: loss: 0.0018\n",
      "step 89200: loss: 0.0015\n",
      "step 89300: loss: 0.0017\n",
      "step 89400: loss: 0.0017\n",
      "step 89500: loss: 0.0025\n",
      "step 89600: loss: 0.0026\n",
      "step 89700: loss: 0.0021\n",
      "step 89800: loss: 0.0020\n",
      "step 89900: loss: 0.0017\n",
      "step 90000: loss: 0.0015\n",
      "step 90100: loss: 0.0025\n",
      "step 90200: loss: 0.0018\n",
      "step 90300: loss: 0.0020\n",
      "step 90400: loss: 0.0020\n",
      "step 90500: loss: 0.0018\n",
      "step 90600: loss: 0.0016\n",
      "step 90700: loss: 0.0017\n",
      "step 90800: loss: 0.0016\n",
      "step 90900: loss: 0.0015\n",
      "step 91000: loss: 0.0014\n",
      "step 91100: loss: 0.0015\n",
      "step 91200: loss: 0.0015\n",
      "step 91300: loss: 0.0018\n",
      "step 91400: loss: 0.0017\n",
      "step 91500: loss: 0.0016\n",
      "step 91600: loss: 0.0017\n",
      "step 91700: loss: 0.0015\n",
      "step 91800: loss: 0.0017\n",
      "step 91900: loss: 0.0017\n",
      "step 92000: loss: 0.0016\n",
      "step 92100: loss: 0.0014\n",
      "step 92200: loss: 0.0013\n",
      "step 92300: loss: 0.0015\n",
      "step 92400: loss: 0.0016\n",
      "step 92500: loss: 0.0017\n",
      "step 92600: loss: 0.0017\n",
      "step 92700: loss: 0.0015\n",
      "step 92800: loss: 0.0015\n",
      "step 92900: loss: 0.0021\n",
      "step 93000: loss: 0.0018\n",
      "step 93100: loss: 0.0017\n",
      "step 93200: loss: 0.0017\n",
      "step 93300: loss: 0.0016\n",
      "step 93400: loss: 0.0015\n",
      "step 93500: loss: 0.0017\n",
      "step 93600: loss: 0.0018\n",
      "step 93700: loss: 0.0017\n",
      "step 93800: loss: 0.0019\n",
      "step 93900: loss: 0.0017\n",
      "step 94000: loss: 0.0017\n",
      "step 94100: loss: 0.0015\n",
      "step 94200: loss: 0.0019\n",
      "step 94300: loss: 0.0017\n",
      "step 94400: loss: 0.0016\n",
      "step 94500: loss: 0.0017\n",
      "step 94600: loss: 0.0016\n",
      "step 94700: loss: 0.0018\n",
      "step 94800: loss: 0.0017\n",
      "step 94900: loss: 0.0014\n",
      "step 95000: loss: 0.0014\n",
      "step 95100: loss: 0.0016\n",
      "step 95200: loss: 0.0015\n",
      "step 95300: loss: 0.0015\n",
      "step 95400: loss: 0.0016\n",
      "step 95500: loss: 0.0018\n",
      "step 95600: loss: 0.0017\n",
      "step 95700: loss: 0.0015\n",
      "step 95800: loss: 0.0014\n",
      "step 95900: loss: 0.0013\n",
      "step 96000: loss: 0.0015\n",
      "step 96100: loss: 0.0018\n",
      "step 96200: loss: 0.0017\n",
      "step 96300: loss: 0.0018\n",
      "step 96400: loss: 0.0016\n",
      "step 96500: loss: 0.0014\n",
      "step 96600: loss: 0.0014\n",
      "step 96700: loss: 0.0013\n",
      "step 96800: loss: 0.0014\n",
      "step 96900: loss: 0.0017\n",
      "step 97000: loss: 0.0018\n",
      "step 97100: loss: 0.0017\n",
      "step 97200: loss: 0.0015\n",
      "step 97300: loss: 0.0016\n",
      "step 97400: loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 97500: loss: 0.0018\n",
      "step 97600: loss: 0.0015\n",
      "step 97700: loss: 0.0019\n",
      "step 97800: loss: 0.0020\n",
      "step 97900: loss: 0.0019\n",
      "step 98000: loss: 0.0019\n",
      "step 98100: loss: 0.0020\n",
      "step 98200: loss: 0.0016\n",
      "step 98300: loss: 0.0017\n",
      "step 98400: loss: 0.0024\n",
      "step 98500: loss: 0.0025\n",
      "step 98600: loss: 0.0019\n",
      "step 98700: loss: 0.0016\n",
      "step 98800: loss: 0.0018\n",
      "step 98900: loss: 0.0015\n",
      "step 99000: loss: 0.0015\n",
      "step 99100: loss: 0.0014\n",
      "step 99200: loss: 0.0014\n",
      "step 99300: loss: 0.0014\n",
      "step 99400: loss: 0.0018\n",
      "step 99500: loss: 0.0016\n",
      "step 99600: loss: 0.0018\n",
      "step 99700: loss: 0.0016\n",
      "step 99800: loss: 0.0016\n",
      "step 99900: loss: 0.0016\n",
      "step 100000: loss: 0.0016\n",
      "\tstep 100000: saved model\n",
      "step 100100: loss: 0.0015\n",
      "step 100200: loss: 0.0014\n",
      "step 100300: loss: 0.0013\n",
      "step 100400: loss: 0.0014\n",
      "step 100500: loss: 0.0014\n",
      "step 100600: loss: 0.0014\n",
      "step 100700: loss: 0.0015\n",
      "step 100800: loss: 0.0016\n",
      "step 100900: loss: 0.0016\n",
      "step 101000: loss: 0.0018\n",
      "step 101100: loss: 0.0016\n",
      "step 101200: loss: 0.0014\n",
      "step 101300: loss: 0.0012\n",
      "step 101400: loss: 0.0013\n",
      "step 101500: loss: 0.0012\n",
      "step 101600: loss: 0.0011\n",
      "step 101700: loss: 0.0014\n",
      "step 101800: loss: 0.0015\n",
      "step 101900: loss: 0.0017\n",
      "step 102000: loss: 0.0016\n",
      "step 102100: loss: 0.0014\n",
      "step 102200: loss: 0.0014\n",
      "step 102300: loss: 0.0015\n",
      "step 102400: loss: 0.0019\n",
      "step 102500: loss: 0.0016\n",
      "step 102600: loss: 0.0018\n",
      "step 102700: loss: 0.0017\n",
      "step 102800: loss: 0.0017\n",
      "step 102900: loss: 0.0015\n",
      "step 103000: loss: 0.0021\n",
      "step 103100: loss: 0.0016\n",
      "step 103200: loss: 0.0016\n",
      "step 103300: loss: 0.0015\n",
      "step 103400: loss: 0.0014\n",
      "step 103500: loss: 0.0014\n",
      "step 103600: loss: 0.0014\n",
      "step 103700: loss: 0.0014\n",
      "step 103800: loss: 0.0014\n",
      "step 103900: loss: 0.0014\n",
      "step 104000: loss: 0.0015\n",
      "step 104100: loss: 0.0015\n",
      "step 104200: loss: 0.0017\n",
      "step 104300: loss: 0.0016\n",
      "step 104400: loss: 0.0014\n",
      "step 104500: loss: 0.0015\n",
      "step 104600: loss: 0.0017\n",
      "step 104700: loss: 0.0015\n",
      "step 104800: loss: 0.0014\n",
      "step 104900: loss: 0.0014\n",
      "step 105000: loss: 0.0012\n",
      "step 105100: loss: 0.0013\n",
      "step 105200: loss: 0.0013\n",
      "step 105300: loss: 0.0013\n",
      "step 105400: loss: 0.0013\n",
      "step 105500: loss: 0.0015\n",
      "step 105600: loss: 0.0014\n",
      "step 105700: loss: 0.0014\n",
      "step 105800: loss: 0.0014\n",
      "step 105900: loss: 0.0014\n",
      "step 106000: loss: 0.0011\n",
      "step 106100: loss: 0.0014\n",
      "step 106200: loss: 0.0015\n",
      "step 106300: loss: 0.0016\n",
      "step 106400: loss: 0.0014\n",
      "step 106500: loss: 0.0014\n",
      "step 106600: loss: 0.0014\n",
      "step 106700: loss: 0.0013\n",
      "step 106800: loss: 0.0012\n",
      "step 106900: loss: 0.0014\n",
      "step 107000: loss: 0.0015\n",
      "step 107100: loss: 0.0015\n",
      "step 107200: loss: 0.0013\n",
      "step 107300: loss: 0.0014\n",
      "step 107400: loss: 0.0016\n",
      "step 107500: loss: 0.0019\n",
      "step 107600: loss: 0.0018\n",
      "step 107700: loss: 0.0017\n",
      "step 107800: loss: 0.0020\n",
      "step 107900: loss: 0.0021\n",
      "step 108000: loss: 0.0017\n",
      "step 108100: loss: 0.0016\n",
      "step 108200: loss: 0.0016\n",
      "step 108300: loss: 0.0017\n",
      "step 108400: loss: 0.0015\n",
      "step 108500: loss: 0.0014\n",
      "step 108600: loss: 0.0013\n",
      "step 108700: loss: 0.0011\n",
      "step 108800: loss: 0.0014\n",
      "step 108900: loss: 0.0015\n",
      "step 109000: loss: 0.0014\n",
      "step 109100: loss: 0.0019\n",
      "step 109200: loss: 0.0018\n",
      "step 109300: loss: 0.0016\n",
      "step 109400: loss: 0.0016\n",
      "step 109500: loss: 0.0016\n",
      "step 109600: loss: 0.0018\n",
      "step 109700: loss: 0.0019\n",
      "step 109800: loss: 0.0020\n",
      "step 109900: loss: 0.0016\n",
      "step 110000: loss: 0.0017\n",
      "step 110100: loss: 0.0016\n",
      "step 110200: loss: 0.0014\n",
      "step 110300: loss: 0.0015\n",
      "step 110400: loss: 0.0015\n",
      "step 110500: loss: 0.0015\n",
      "step 110600: loss: 0.0014\n",
      "step 110700: loss: 0.0015\n",
      "step 110800: loss: 0.0016\n",
      "step 110900: loss: 0.0017\n",
      "step 111000: loss: 0.0018\n",
      "step 111100: loss: 0.0019\n",
      "step 111200: loss: 0.0016\n",
      "step 111300: loss: 0.0016\n",
      "step 111400: loss: 0.0015\n",
      "step 111500: loss: 0.0015\n",
      "step 111600: loss: 0.0014\n",
      "step 111700: loss: 0.0016\n",
      "step 111800: loss: 0.0017\n",
      "step 111900: loss: 0.0016\n",
      "step 112000: loss: 0.0020\n",
      "step 112100: loss: 0.0016\n",
      "step 112200: loss: 0.0015\n",
      "step 112300: loss: 0.0015\n",
      "step 112400: loss: 0.0015\n",
      "step 112500: loss: 0.0016\n",
      "step 112600: loss: 0.0019\n",
      "step 112700: loss: 0.0016\n",
      "step 112800: loss: 0.0015\n",
      "step 112900: loss: 0.0016\n",
      "step 113000: loss: 0.0017\n",
      "step 113100: loss: 0.0015\n",
      "step 113200: loss: 0.0016\n",
      "step 113300: loss: 0.0013\n",
      "step 113400: loss: 0.0015\n",
      "step 113500: loss: 0.0014\n",
      "step 113600: loss: 0.0013\n",
      "step 113700: loss: 0.0014\n",
      "step 113800: loss: 0.0012\n",
      "step 113900: loss: 0.0017\n",
      "step 114000: loss: 0.0016\n",
      "step 114100: loss: 0.0014\n",
      "step 114200: loss: 0.0014\n",
      "step 114300: loss: 0.0013\n",
      "step 114400: loss: 0.0012\n",
      "step 114500: loss: 0.0012\n",
      "step 114600: loss: 0.0012\n",
      "step 114700: loss: 0.0018\n",
      "step 114800: loss: 0.0017\n",
      "step 114900: loss: 0.0018\n",
      "step 115000: loss: 0.0016\n",
      "step 115100: loss: 0.0015\n",
      "step 115200: loss: 0.0014\n",
      "step 115300: loss: 0.0013\n",
      "step 115400: loss: 0.0014\n",
      "step 115500: loss: 0.0014\n",
      "step 115600: loss: 0.0013\n",
      "step 115700: loss: 0.0022\n",
      "step 115800: loss: 0.0019\n",
      "step 115900: loss: 0.0016\n",
      "step 116000: loss: 0.0019\n",
      "step 116100: loss: 0.0018\n",
      "step 116200: loss: 0.0018\n",
      "step 116300: loss: 0.0017\n",
      "step 116400: loss: 0.0015\n",
      "step 116500: loss: 0.0021\n",
      "step 116600: loss: 0.0018\n",
      "step 116700: loss: 0.0015\n",
      "step 116800: loss: 0.0016\n",
      "step 116900: loss: 0.0014\n",
      "step 117000: loss: 0.0013\n",
      "step 117100: loss: 0.0014\n",
      "step 117200: loss: 0.0014\n",
      "step 117300: loss: 0.0015\n",
      "step 117400: loss: 0.0015\n",
      "step 117500: loss: 0.0015\n",
      "step 117600: loss: 0.0014\n",
      "step 117700: loss: 0.0014\n",
      "step 117800: loss: 0.0015\n",
      "step 117900: loss: 0.0016\n",
      "step 118000: loss: 0.0016\n",
      "step 118100: loss: 0.0015\n",
      "step 118200: loss: 0.0016\n",
      "step 118300: loss: 0.0015\n",
      "step 118400: loss: 0.0012\n",
      "step 118500: loss: 0.0014\n",
      "step 118600: loss: 0.0016\n",
      "step 118700: loss: 0.0015\n",
      "step 118800: loss: 0.0013\n",
      "step 118900: loss: 0.0012\n",
      "step 119000: loss: 0.0012\n",
      "step 119100: loss: 0.0012\n",
      "step 119200: loss: 0.0013\n",
      "step 119300: loss: 0.0015\n",
      "step 119400: loss: 0.0015\n",
      "step 119500: loss: 0.0017\n",
      "step 119600: loss: 0.0016\n",
      "step 119700: loss: 0.0015\n",
      "step 119800: loss: 0.0015\n",
      "step 119900: loss: 0.0014\n",
      "step 120000: loss: 0.0019\n",
      "step 120100: loss: 0.0016\n",
      "step 120200: loss: 0.0015\n",
      "step 120300: loss: 0.0013\n",
      "step 120400: loss: 0.0013\n",
      "step 120500: loss: 0.0013\n",
      "step 120600: loss: 0.0013\n",
      "step 120700: loss: 0.0013\n",
      "step 120800: loss: 0.0014\n",
      "step 120900: loss: 0.0014\n",
      "step 121000: loss: 0.0014\n",
      "step 121100: loss: 0.0015\n",
      "step 121200: loss: 0.0014\n",
      "step 121300: loss: 0.0013\n",
      "step 121400: loss: 0.0016\n",
      "step 121500: loss: 0.0014\n",
      "step 121600: loss: 0.0012\n",
      "step 121700: loss: 0.0012\n",
      "step 121800: loss: 0.0013\n",
      "step 121900: loss: 0.0014\n",
      "step 122000: loss: 0.0013\n",
      "step 122100: loss: 0.0014\n",
      "step 122200: loss: 0.0015\n",
      "step 122300: loss: 0.0018\n",
      "step 122400: loss: 0.0016\n",
      "step 122500: loss: 0.0014\n",
      "step 122600: loss: 0.0012\n",
      "step 122700: loss: 0.0012\n",
      "step 122800: loss: 0.0014\n",
      "step 122900: loss: 0.0015\n",
      "step 123000: loss: 0.0012\n",
      "step 123100: loss: 0.0012\n",
      "step 123200: loss: 0.0013\n",
      "step 123300: loss: 0.0013\n",
      "step 123400: loss: 0.0013\n",
      "step 123500: loss: 0.0014\n",
      "step 123600: loss: 0.0013\n",
      "step 123700: loss: 0.0012\n",
      "step 123800: loss: 0.0013\n",
      "step 123900: loss: 0.0012\n",
      "step 124000: loss: 0.0014\n",
      "step 124100: loss: 0.0013\n",
      "step 124200: loss: 0.0014\n",
      "step 124300: loss: 0.0013\n",
      "step 124400: loss: 0.0012\n",
      "step 124500: loss: 0.0012\n",
      "step 124600: loss: 0.0012\n",
      "step 124700: loss: 0.0012\n",
      "step 124800: loss: 0.0014\n",
      "step 124900: loss: 0.0015\n",
      "step 125000: loss: 0.0013\n",
      "step 125100: loss: 0.0013\n",
      "step 125200: loss: 0.0017\n",
      "step 125300: loss: 0.0015\n",
      "step 125400: loss: 0.0014\n",
      "step 125500: loss: 0.0014\n",
      "step 125600: loss: 0.0015\n",
      "step 125700: loss: 0.0016\n",
      "step 125800: loss: 0.0013\n",
      "step 125900: loss: 0.0015\n",
      "step 126000: loss: 0.0014\n",
      "step 126100: loss: 0.0016\n",
      "step 126200: loss: 0.0017\n",
      "step 126300: loss: 0.0016\n",
      "step 126400: loss: 0.0016\n",
      "step 126500: loss: 0.0014\n",
      "step 126600: loss: 0.0014\n",
      "step 126700: loss: 0.0012\n",
      "step 126800: loss: 0.0013\n",
      "step 126900: loss: 0.0012\n",
      "step 127000: loss: 0.0014\n",
      "step 127100: loss: 0.0014\n",
      "step 127200: loss: 0.0015\n",
      "step 127300: loss: 0.0013\n",
      "step 127400: loss: 0.0012\n",
      "step 127500: loss: 0.0012\n",
      "step 127600: loss: 0.0013\n",
      "step 127700: loss: 0.0014\n",
      "step 127800: loss: 0.0013\n",
      "step 127900: loss: 0.0013\n",
      "step 128000: loss: 0.0013\n",
      "\tstep 128000: saved model\n",
      "step 128100: loss: 0.0015\n",
      "step 128200: loss: 0.0015\n",
      "step 128300: loss: 0.0014\n",
      "step 128400: loss: 0.0012\n",
      "step 128500: loss: 0.0011\n",
      "step 128600: loss: 0.0011\n",
      "step 128700: loss: 0.0012\n",
      "step 128800: loss: 0.0011\n",
      "step 128900: loss: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 129000: loss: 0.0013\n",
      "step 129100: loss: 0.0015\n",
      "step 129200: loss: 0.0014\n",
      "step 129300: loss: 0.0016\n",
      "step 129400: loss: 0.0014\n",
      "step 129500: loss: 0.0013\n",
      "step 129600: loss: 0.0013\n",
      "step 129700: loss: 0.0012\n",
      "step 129800: loss: 0.0012\n",
      "step 129900: loss: 0.0022\n",
      "step 130000: loss: 0.0018\n",
      "step 130100: loss: 0.0014\n",
      "step 130200: loss: 0.0014\n",
      "step 130300: loss: 0.0014\n",
      "step 130400: loss: 0.0015\n",
      "step 130500: loss: 0.0015\n",
      "step 130600: loss: 0.0013\n",
      "step 130700: loss: 0.0012\n",
      "step 130800: loss: 0.0015\n",
      "step 130900: loss: 0.0016\n",
      "step 131000: loss: 0.0014\n",
      "step 131100: loss: 0.0013\n",
      "step 131200: loss: 0.0013\n",
      "step 131300: loss: 0.0013\n",
      "step 131400: loss: 0.0012\n",
      "step 131500: loss: 0.0014\n",
      "step 131600: loss: 0.0013\n",
      "step 131700: loss: 0.0011\n",
      "step 131800: loss: 0.0014\n",
      "step 131900: loss: 0.0013\n",
      "step 132000: loss: 0.0013\n",
      "step 132100: loss: 0.0014\n",
      "step 132200: loss: 0.0012\n",
      "step 132300: loss: 0.0012\n",
      "step 132400: loss: 0.0014\n",
      "step 132500: loss: 0.0012\n",
      "step 132600: loss: 0.0011\n",
      "step 132700: loss: 0.0012\n",
      "step 132800: loss: 0.0011\n",
      "step 132900: loss: 0.0011\n",
      "step 133000: loss: 0.0013\n",
      "step 133100: loss: 0.0012\n",
      "step 133200: loss: 0.0014\n",
      "step 133300: loss: 0.0014\n",
      "step 133400: loss: 0.0013\n",
      "step 133500: loss: 0.0014\n",
      "step 133600: loss: 0.0012\n",
      "step 133700: loss: 0.0012\n",
      "step 133800: loss: 0.0013\n",
      "step 133900: loss: 0.0013\n",
      "step 134000: loss: 0.0013\n",
      "step 134100: loss: 0.0013\n",
      "step 134200: loss: 0.0012\n",
      "step 134300: loss: 0.0012\n",
      "step 134400: loss: 0.0014\n",
      "step 134500: loss: 0.0013\n",
      "step 134600: loss: 0.0012\n",
      "step 134700: loss: 0.0013\n",
      "step 134800: loss: 0.0011\n",
      "step 134900: loss: 0.0012\n",
      "step 135000: loss: 0.0011\n",
      "step 135100: loss: 0.0010\n",
      "step 135200: loss: 0.0011\n",
      "step 135300: loss: 0.0014\n",
      "step 135400: loss: 0.0012\n",
      "step 135500: loss: 0.0010\n",
      "step 135600: loss: 0.0012\n",
      "step 135700: loss: 0.0010\n",
      "step 135800: loss: 0.0011\n",
      "step 135900: loss: 0.0013\n",
      "step 136000: loss: 0.0013\n",
      "step 136100: loss: 0.0012\n",
      "step 136200: loss: 0.0011\n",
      "step 136300: loss: 0.0012\n",
      "step 136400: loss: 0.0011\n",
      "step 136500: loss: 0.0016\n",
      "step 136600: loss: 0.0015\n",
      "step 136700: loss: 0.0012\n",
      "step 136800: loss: 0.0012\n",
      "step 136900: loss: 0.0015\n",
      "step 137000: loss: 0.0015\n",
      "step 137100: loss: 0.0013\n",
      "step 137200: loss: 0.0014\n",
      "step 137300: loss: 0.0012\n",
      "step 137400: loss: 0.0012\n",
      "step 137500: loss: 0.0013\n",
      "step 137600: loss: 0.0013\n",
      "step 137700: loss: 0.0011\n",
      "step 137800: loss: 0.0012\n",
      "step 137900: loss: 0.0015\n",
      "step 138000: loss: 0.0014\n",
      "step 138100: loss: 0.0012\n",
      "step 138200: loss: 0.0012\n",
      "step 138300: loss: 0.0011\n",
      "step 138400: loss: 0.0013\n",
      "step 138500: loss: 0.0015\n",
      "step 138600: loss: 0.0013\n",
      "step 138700: loss: 0.0012\n",
      "step 138800: loss: 0.0012\n",
      "step 138900: loss: 0.0012\n",
      "step 139000: loss: 0.0013\n",
      "step 139100: loss: 0.0012\n",
      "step 139200: loss: 0.0014\n",
      "step 139300: loss: 0.0013\n",
      "step 139400: loss: 0.0015\n",
      "step 139500: loss: 0.0014\n",
      "step 139600: loss: 0.0014\n",
      "step 139700: loss: 0.0013\n",
      "step 139800: loss: 0.0012\n",
      "step 139900: loss: 0.0012\n",
      "step 140000: loss: 0.0011\n",
      "step 140100: loss: 0.0011\n",
      "step 140200: loss: 0.0014\n",
      "step 140300: loss: 0.0012\n",
      "step 140400: loss: 0.0011\n",
      "step 140500: loss: 0.0012\n",
      "step 140600: loss: 0.0014\n",
      "step 140700: loss: 0.0013\n",
      "step 140800: loss: 0.0012\n",
      "step 140900: loss: 0.0012\n",
      "step 141000: loss: 0.0012\n",
      "step 141100: loss: 0.0012\n",
      "step 141200: loss: 0.0012\n",
      "step 141300: loss: 0.0012\n",
      "step 141400: loss: 0.0011\n",
      "step 141500: loss: 0.0012\n",
      "step 141600: loss: 0.0012\n",
      "step 141700: loss: 0.0012\n",
      "step 141800: loss: 0.0013\n",
      "step 141900: loss: 0.0013\n",
      "step 142000: loss: 0.0013\n",
      "step 142100: loss: 0.0012\n",
      "step 142200: loss: 0.0012\n",
      "step 142300: loss: 0.0011\n",
      "step 142400: loss: 0.0012\n",
      "step 142500: loss: 0.0010\n",
      "step 142600: loss: 0.0010\n",
      "step 142700: loss: 0.0010\n",
      "step 142800: loss: 0.0012\n",
      "step 142900: loss: 0.0011\n",
      "step 143000: loss: 0.0011\n",
      "step 143100: loss: 0.0011\n",
      "step 143200: loss: 0.0012\n",
      "step 143300: loss: 0.0011\n",
      "step 143400: loss: 0.0011\n",
      "step 143500: loss: 0.0011\n",
      "step 143600: loss: 0.0011\n",
      "step 143700: loss: 0.0012\n",
      "step 143800: loss: 0.0012\n",
      "step 143900: loss: 0.0014\n",
      "step 144000: loss: 0.0012\n",
      "step 144100: loss: 0.0012\n",
      "step 144200: loss: 0.0012\n",
      "step 144300: loss: 0.0016\n",
      "step 144400: loss: 0.0017\n",
      "step 144500: loss: 0.0015\n",
      "step 144600: loss: 0.0014\n",
      "step 144700: loss: 0.0013\n",
      "step 144800: loss: 0.0011\n",
      "step 144900: loss: 0.0011\n",
      "step 145000: loss: 0.0013\n",
      "step 145100: loss: 0.0012\n",
      "step 145200: loss: 0.0012\n",
      "step 145300: loss: 0.0012\n",
      "step 145400: loss: 0.0016\n",
      "step 145500: loss: 0.0015\n",
      "step 145600: loss: 0.0013\n",
      "step 145700: loss: 0.0014\n",
      "step 145800: loss: 0.0014\n",
      "step 145900: loss: 0.0013\n",
      "step 146000: loss: 0.0011\n",
      "step 146100: loss: 0.0011\n",
      "step 146200: loss: 0.0011\n",
      "step 146300: loss: 0.0010\n",
      "step 146400: loss: 0.0011\n",
      "step 146500: loss: 0.0014\n",
      "step 146600: loss: 0.0013\n",
      "step 146700: loss: 0.0013\n",
      "step 146800: loss: 0.0014\n",
      "step 146900: loss: 0.0018\n",
      "step 147000: loss: 0.0015\n",
      "step 147100: loss: 0.0016\n",
      "step 147200: loss: 0.0017\n",
      "step 147300: loss: 0.0013\n",
      "step 147400: loss: 0.0012\n",
      "step 147500: loss: 0.0011\n",
      "step 147600: loss: 0.0012\n",
      "step 147700: loss: 0.0012\n",
      "step 147800: loss: 0.0011\n",
      "step 147900: loss: 0.0017\n",
      "step 148000: loss: 0.0015\n",
      "step 148100: loss: 0.0015\n",
      "step 148200: loss: 0.0013\n",
      "step 148300: loss: 0.0014\n",
      "step 148400: loss: 0.0012\n",
      "step 148500: loss: 0.0012\n",
      "step 148600: loss: 0.0015\n",
      "step 148700: loss: 0.0016\n",
      "step 148800: loss: 0.0013\n",
      "step 148900: loss: 0.0013\n",
      "step 149000: loss: 0.0013\n",
      "step 149100: loss: 0.0012\n",
      "step 149200: loss: 0.0013\n",
      "step 149300: loss: 0.0013\n",
      "step 149400: loss: 0.0013\n",
      "step 149500: loss: 0.0013\n",
      "step 149600: loss: 0.0012\n",
      "step 149700: loss: 0.0012\n",
      "step 149800: loss: 0.0012\n",
      "step 149900: loss: 0.0011\n",
      "step 150000: loss: 0.0011\n",
      "step 150100: loss: 0.0011\n",
      "step 150200: loss: 0.0011\n",
      "step 150300: loss: 0.0010\n",
      "step 150400: loss: 0.0010\n",
      "step 150500: loss: 0.0012\n",
      "step 150600: loss: 0.0013\n",
      "step 150700: loss: 0.0013\n",
      "step 150800: loss: 0.0014\n",
      "step 150900: loss: 0.0015\n",
      "step 151000: loss: 0.0012\n",
      "step 151100: loss: 0.0010\n",
      "step 151200: loss: 0.0010\n",
      "step 151300: loss: 0.0013\n",
      "step 151400: loss: 0.0011\n",
      "step 151500: loss: 0.0011\n",
      "step 151600: loss: 0.0012\n",
      "step 151700: loss: 0.0012\n",
      "step 151800: loss: 0.0011\n",
      "step 151900: loss: 0.0009\n",
      "step 152000: loss: 0.0010\n",
      "step 152100: loss: 0.0012\n",
      "step 152200: loss: 0.0012\n",
      "step 152300: loss: 0.0011\n",
      "step 152400: loss: 0.0011\n",
      "step 152500: loss: 0.0012\n",
      "step 152600: loss: 0.0011\n",
      "step 152700: loss: 0.0014\n",
      "step 152800: loss: 0.0013\n",
      "step 152900: loss: 0.0013\n",
      "step 153000: loss: 0.0012\n",
      "step 153100: loss: 0.0010\n",
      "step 153200: loss: 0.0011\n",
      "step 153300: loss: 0.0011\n",
      "step 153400: loss: 0.0011\n",
      "step 153500: loss: 0.0011\n",
      "step 153600: loss: 0.0010\n",
      "step 153700: loss: 0.0013\n",
      "step 153800: loss: 0.0012\n",
      "step 153900: loss: 0.0011\n",
      "step 154000: loss: 0.0015\n",
      "step 154100: loss: 0.0014\n",
      "step 154200: loss: 0.0013\n",
      "step 154300: loss: 0.0014\n",
      "step 154400: loss: 0.0012\n",
      "step 154500: loss: 0.0013\n",
      "step 154600: loss: 0.0011\n",
      "step 154700: loss: 0.0012\n",
      "step 154800: loss: 0.0010\n",
      "step 154900: loss: 0.0011\n",
      "step 155000: loss: 0.0013\n",
      "step 155100: loss: 0.0012\n",
      "step 155200: loss: 0.0011\n",
      "step 155300: loss: 0.0012\n",
      "step 155400: loss: 0.0012\n",
      "step 155500: loss: 0.0013\n",
      "step 155600: loss: 0.0013\n",
      "step 155700: loss: 0.0012\n",
      "step 155800: loss: 0.0011\n",
      "step 155900: loss: 0.0011\n",
      "step 156000: loss: 0.0014\n",
      "step 156100: loss: 0.0013\n",
      "step 156200: loss: 0.0016\n",
      "step 156300: loss: 0.0013\n",
      "step 156400: loss: 0.0014\n",
      "step 156500: loss: 0.0013\n",
      "step 156600: loss: 0.0011\n",
      "step 156700: loss: 0.0011\n",
      "step 156800: loss: 0.0012\n",
      "step 156900: loss: 0.0012\n",
      "step 157000: loss: 0.0012\n",
      "step 157100: loss: 0.0011\n",
      "step 157200: loss: 0.0011\n",
      "step 157300: loss: 0.0012\n",
      "step 157400: loss: 0.0011\n",
      "step 157500: loss: 0.0011\n",
      "step 157600: loss: 0.0012\n",
      "step 157700: loss: 0.0011\n",
      "step 157800: loss: 0.0012\n",
      "step 157900: loss: 0.0011\n",
      "step 158000: loss: 0.0012\n",
      "step 158100: loss: 0.0011\n",
      "step 158200: loss: 0.0010\n",
      "step 158300: loss: 0.0009\n",
      "step 158400: loss: 0.0010\n",
      "step 158500: loss: 0.0009\n",
      "step 158600: loss: 0.0011\n",
      "step 158700: loss: 0.0011\n",
      "step 158800: loss: 0.0010\n",
      "step 158900: loss: 0.0010\n",
      "step 159000: loss: 0.0015\n",
      "step 159100: loss: 0.0014\n",
      "step 159200: loss: 0.0013\n",
      "step 159300: loss: 0.0012\n",
      "step 159400: loss: 0.0012\n",
      "step 159500: loss: 0.0011\n",
      "step 159600: loss: 0.0012\n",
      "step 159700: loss: 0.0012\n",
      "step 159800: loss: 0.0019\n",
      "step 159900: loss: 0.0016\n",
      "step 160000: loss: 0.0015\n",
      "step 160100: loss: 0.0016\n",
      "step 160200: loss: 0.0015\n",
      "step 160300: loss: 0.0014\n",
      "step 160400: loss: 0.0013\n",
      "step 160500: loss: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 160600: loss: 0.0013\n",
      "step 160700: loss: 0.0012\n",
      "step 160800: loss: 0.0016\n",
      "step 160900: loss: 0.0015\n",
      "step 161000: loss: 0.0012\n",
      "step 161100: loss: 0.0012\n",
      "step 161200: loss: 0.0027\n",
      "step 161300: loss: 0.0019\n",
      "step 161400: loss: 0.0015\n",
      "step 161500: loss: 0.0012\n",
      "step 161600: loss: 0.0011\n",
      "step 161700: loss: 0.0012\n",
      "step 161800: loss: 0.0014\n",
      "step 161900: loss: 0.0012\n",
      "step 162000: loss: 0.0012\n",
      "step 162100: loss: 0.0011\n",
      "step 162200: loss: 0.0010\n",
      "step 162300: loss: 0.0010\n",
      "step 162400: loss: 0.0010\n",
      "step 162500: loss: 0.0011\n",
      "step 162600: loss: 0.0010\n",
      "step 162700: loss: 0.0010\n",
      "step 162800: loss: 0.0009\n",
      "step 162900: loss: 0.0010\n",
      "step 163000: loss: 0.0010\n",
      "step 163100: loss: 0.0011\n",
      "step 163200: loss: 0.0011\n",
      "step 163300: loss: 0.0014\n",
      "step 163400: loss: 0.0012\n",
      "step 163500: loss: 0.0017\n",
      "step 163600: loss: 0.0015\n",
      "step 163700: loss: 0.0012\n",
      "step 163800: loss: 0.0012\n",
      "step 163900: loss: 0.0012\n",
      "step 164000: loss: 0.0010\n",
      "step 164100: loss: 0.0010\n",
      "step 164200: loss: 0.0011\n",
      "step 164300: loss: 0.0011\n",
      "step 164400: loss: 0.0010\n",
      "step 164500: loss: 0.0012\n",
      "step 164600: loss: 0.0011\n",
      "step 164700: loss: 0.0012\n",
      "step 164800: loss: 0.0010\n",
      "step 164900: loss: 0.0010\n",
      "step 165000: loss: 0.0011\n",
      "step 165100: loss: 0.0011\n",
      "step 165200: loss: 0.0013\n",
      "step 165300: loss: 0.0012\n",
      "step 165400: loss: 0.0013\n",
      "step 165500: loss: 0.0016\n",
      "step 165600: loss: 0.0014\n",
      "step 165700: loss: 0.0013\n",
      "step 165800: loss: 0.0011\n",
      "step 165900: loss: 0.0011\n",
      "step 166000: loss: 0.0012\n",
      "step 166100: loss: 0.0010\n",
      "step 166200: loss: 0.0013\n",
      "step 166300: loss: 0.0013\n",
      "step 166400: loss: 0.0011\n",
      "step 166500: loss: 0.0011\n",
      "step 166600: loss: 0.0011\n",
      "step 166700: loss: 0.0011\n",
      "step 166800: loss: 0.0011\n",
      "step 166900: loss: 0.0012\n",
      "step 167000: loss: 0.0013\n",
      "step 167100: loss: 0.0014\n",
      "step 167200: loss: 0.0013\n",
      "step 167300: loss: 0.0012\n",
      "step 167400: loss: 0.0012\n",
      "step 167500: loss: 0.0010\n",
      "step 167600: loss: 0.0009\n",
      "step 167700: loss: 0.0010\n",
      "step 167800: loss: 0.0011\n",
      "step 167900: loss: 0.0012\n",
      "step 168000: loss: 0.0011\n",
      "step 168100: loss: 0.0012\n",
      "step 168200: loss: 0.0010\n",
      "step 168300: loss: 0.0011\n",
      "step 168400: loss: 0.0010\n",
      "step 168500: loss: 0.0013\n",
      "step 168600: loss: 0.0010\n",
      "step 168700: loss: 0.0010\n",
      "step 168800: loss: 0.0011\n",
      "step 168900: loss: 0.0011\n",
      "step 169000: loss: 0.0010\n",
      "step 169100: loss: 0.0009\n",
      "step 169200: loss: 0.0011\n",
      "step 169300: loss: 0.0011\n",
      "step 169400: loss: 0.0011\n",
      "step 169500: loss: 0.0010\n",
      "step 169600: loss: 0.0010\n",
      "step 169700: loss: 0.0010\n",
      "step 169800: loss: 0.0012\n",
      "step 169900: loss: 0.0012\n",
      "step 170000: loss: 0.0011\n",
      "step 170100: loss: 0.0010\n",
      "step 170200: loss: 0.0025\n",
      "step 170300: loss: 0.0017\n",
      "step 170400: loss: 0.0013\n",
      "step 170500: loss: 0.0012\n",
      "step 170600: loss: 0.0012\n",
      "step 170700: loss: 0.0010\n",
      "step 170800: loss: 0.0011\n",
      "step 170900: loss: 0.0010\n",
      "step 171000: loss: 0.0011\n",
      "step 171100: loss: 0.0010\n",
      "step 171200: loss: 0.0011\n",
      "step 171300: loss: 0.0012\n",
      "step 171400: loss: 0.0010\n",
      "step 171500: loss: 0.0013\n",
      "step 171600: loss: 0.0011\n",
      "step 171700: loss: 0.0012\n",
      "step 171800: loss: 0.0011\n",
      "step 171900: loss: 0.0013\n",
      "step 172000: loss: 0.0011\n",
      "step 172100: loss: 0.0010\n",
      "step 172200: loss: 0.0009\n",
      "step 172300: loss: 0.0011\n",
      "step 172400: loss: 0.0010\n",
      "step 172500: loss: 0.0013\n",
      "step 172600: loss: 0.0011\n",
      "step 172700: loss: 0.0011\n",
      "step 172800: loss: 0.0010\n",
      "step 172900: loss: 0.0010\n",
      "step 173000: loss: 0.0011\n",
      "step 173100: loss: 0.0010\n",
      "step 173200: loss: 0.0010\n",
      "step 173300: loss: 0.0010\n",
      "step 173400: loss: 0.0009\n",
      "step 173500: loss: 0.0009\n",
      "step 173600: loss: 0.0009\n",
      "step 173700: loss: 0.0009\n",
      "step 173800: loss: 0.0010\n",
      "step 173900: loss: 0.0009\n",
      "step 174000: loss: 0.0011\n",
      "step 174100: loss: 0.0012\n",
      "step 174200: loss: 0.0012\n",
      "step 174300: loss: 0.0013\n",
      "step 174400: loss: 0.0011\n",
      "step 174500: loss: 0.0010\n",
      "step 174600: loss: 0.0010\n",
      "step 174700: loss: 0.0010\n",
      "step 174800: loss: 0.0010\n",
      "step 174900: loss: 0.0013\n",
      "step 175000: loss: 0.0012\n",
      "step 175100: loss: 0.0013\n",
      "step 175200: loss: 0.0011\n",
      "step 175300: loss: 0.0011\n",
      "step 175400: loss: 0.0010\n",
      "step 175500: loss: 0.0010\n",
      "step 175600: loss: 0.0011\n",
      "step 175700: loss: 0.0010\n",
      "step 175800: loss: 0.0011\n",
      "step 175900: loss: 0.0010\n",
      "step 176000: loss: 0.0011\n",
      "step 176100: loss: 0.0011\n",
      "step 176200: loss: 0.0013\n",
      "step 176300: loss: 0.0012\n",
      "step 176400: loss: 0.0012\n",
      "step 176500: loss: 0.0012\n",
      "step 176600: loss: 0.0012\n",
      "step 176700: loss: 0.0012\n",
      "step 176800: loss: 0.0011\n",
      "step 176900: loss: 0.0010\n",
      "step 177000: loss: 0.0009\n",
      "step 177100: loss: 0.0009\n",
      "step 177200: loss: 0.0011\n",
      "step 177300: loss: 0.0010\n",
      "step 177400: loss: 0.0010\n",
      "step 177500: loss: 0.0011\n",
      "step 177600: loss: 0.0010\n",
      "step 177700: loss: 0.0010\n",
      "step 177800: loss: 0.0009\n",
      "step 177900: loss: 0.0009\n",
      "step 178000: loss: 0.0009\n",
      "step 178100: loss: 0.0009\n",
      "step 178200: loss: 0.0010\n",
      "step 178300: loss: 0.0010\n",
      "step 178400: loss: 0.0010\n",
      "step 178500: loss: 0.0009\n",
      "step 178600: loss: 0.0010\n",
      "step 178700: loss: 0.0010\n",
      "step 178800: loss: 0.0011\n",
      "step 178900: loss: 0.0012\n",
      "step 179000: loss: 0.0012\n",
      "step 179100: loss: 0.0012\n",
      "step 179200: loss: 0.0012\n",
      "step 179300: loss: 0.0011\n",
      "step 179400: loss: 0.0010\n",
      "step 179500: loss: 0.0011\n",
      "step 179600: loss: 0.0012\n",
      "step 179700: loss: 0.0012\n",
      "step 179800: loss: 0.0010\n",
      "step 179900: loss: 0.0011\n",
      "step 180000: loss: 0.0010\n",
      "step 180100: loss: 0.0010\n",
      "step 180200: loss: 0.0009\n",
      "step 180300: loss: 0.0010\n",
      "step 180400: loss: 0.0011\n",
      "step 180500: loss: 0.0009\n",
      "step 180600: loss: 0.0009\n",
      "step 180700: loss: 0.0008\n",
      "step 180800: loss: 0.0013\n",
      "step 180900: loss: 0.0012\n",
      "step 181000: loss: 0.0012\n",
      "step 181100: loss: 0.0010\n",
      "step 181200: loss: 0.0013\n",
      "step 181300: loss: 0.0011\n",
      "step 181400: loss: 0.0014\n",
      "step 181500: loss: 0.0014\n",
      "step 181600: loss: 0.0014\n",
      "step 181700: loss: 0.0014\n",
      "step 181800: loss: 0.0012\n",
      "step 181900: loss: 0.0011\n",
      "step 182000: loss: 0.0011\n",
      "step 182100: loss: 0.0012\n",
      "step 182200: loss: 0.0011\n",
      "step 182300: loss: 0.0010\n",
      "step 182400: loss: 0.0014\n",
      "step 182500: loss: 0.0013\n",
      "step 182600: loss: 0.0012\n",
      "step 182700: loss: 0.0012\n",
      "step 182800: loss: 0.0013\n",
      "step 182900: loss: 0.0011\n",
      "step 183000: loss: 0.0010\n",
      "step 183100: loss: 0.0011\n",
      "step 183200: loss: 0.0011\n",
      "step 183300: loss: 0.0011\n",
      "step 183400: loss: 0.0011\n",
      "step 183500: loss: 0.0011\n",
      "step 183600: loss: 0.0014\n",
      "step 183700: loss: 0.0014\n",
      "step 183800: loss: 0.0016\n",
      "step 183900: loss: 0.0012\n",
      "step 184000: loss: 0.0012\n",
      "step 184100: loss: 0.0010\n",
      "step 184200: loss: 0.0011\n",
      "step 184300: loss: 0.0011\n",
      "step 184400: loss: 0.0012\n",
      "step 184500: loss: 0.0011\n",
      "step 184600: loss: 0.0010\n",
      "step 184700: loss: 0.0011\n",
      "step 184800: loss: 0.0011\n",
      "step 184900: loss: 0.0015\n",
      "step 185000: loss: 0.0013\n",
      "step 185100: loss: 0.0010\n",
      "step 185200: loss: 0.0010\n",
      "step 185300: loss: 0.0010\n",
      "step 185400: loss: 0.0011\n",
      "step 185500: loss: 0.0011\n",
      "step 185600: loss: 0.0012\n",
      "step 185700: loss: 0.0012\n",
      "step 185800: loss: 0.0010\n",
      "step 185900: loss: 0.0011\n",
      "step 186000: loss: 0.0010\n",
      "step 186100: loss: 0.0010\n",
      "step 186200: loss: 0.0011\n",
      "step 186300: loss: 0.0010\n",
      "step 186400: loss: 0.0010\n",
      "step 186500: loss: 0.0009\n",
      "step 186600: loss: 0.0010\n",
      "step 186700: loss: 0.0012\n",
      "step 186800: loss: 0.0010\n",
      "step 186900: loss: 0.0010\n",
      "step 187000: loss: 0.0009\n",
      "step 187100: loss: 0.0009\n",
      "step 187200: loss: 0.0010\n",
      "step 187300: loss: 0.0010\n",
      "step 187400: loss: 0.0009\n",
      "step 187500: loss: 0.0009\n",
      "step 187600: loss: 0.0010\n",
      "step 187700: loss: 0.0010\n",
      "step 187800: loss: 0.0009\n",
      "step 187900: loss: 0.0009\n",
      "step 188000: loss: 0.0009\n",
      "step 188100: loss: 0.0010\n",
      "step 188200: loss: 0.0010\n",
      "step 188300: loss: 0.0010\n",
      "step 188400: loss: 0.0009\n",
      "step 188500: loss: 0.0009\n",
      "step 188600: loss: 0.0010\n",
      "step 188700: loss: 0.0009\n",
      "step 188800: loss: 0.0010\n",
      "step 188900: loss: 0.0009\n",
      "step 189000: loss: 0.0009\n",
      "step 189100: loss: 0.0010\n",
      "step 189200: loss: 0.0011\n",
      "step 189300: loss: 0.0011\n",
      "step 189400: loss: 0.0011\n",
      "step 189500: loss: 0.0010\n",
      "step 189600: loss: 0.0012\n",
      "step 189700: loss: 0.0010\n",
      "step 189800: loss: 0.0012\n",
      "step 189900: loss: 0.0011\n",
      "step 190000: loss: 0.0011\n",
      "step 190100: loss: 0.0010\n",
      "step 190200: loss: 0.0009\n",
      "step 190300: loss: 0.0010\n",
      "step 190400: loss: 0.0013\n",
      "step 190500: loss: 0.0014\n",
      "step 190600: loss: 0.0011\n",
      "step 190700: loss: 0.0012\n",
      "step 190800: loss: 0.0011\n",
      "step 190900: loss: 0.0011\n",
      "step 191000: loss: 0.0012\n",
      "step 191100: loss: 0.0010\n",
      "step 191200: loss: 0.0009\n",
      "step 191300: loss: 0.0009\n",
      "step 191400: loss: 0.0008\n",
      "step 191500: loss: 0.0008\n",
      "step 191600: loss: 0.0008\n",
      "step 191700: loss: 0.0012\n",
      "step 191800: loss: 0.0010\n",
      "step 191900: loss: 0.0012\n",
      "step 192000: loss: 0.0011\n",
      "step 192100: loss: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 192200: loss: 0.0012\n",
      "step 192300: loss: 0.0012\n",
      "step 192400: loss: 0.0012\n",
      "step 192500: loss: 0.0013\n",
      "step 192600: loss: 0.0013\n",
      "step 192700: loss: 0.0011\n",
      "step 192800: loss: 0.0011\n",
      "step 192900: loss: 0.0012\n",
      "step 193000: loss: 0.0010\n",
      "step 193100: loss: 0.0012\n",
      "step 193200: loss: 0.0011\n",
      "step 193300: loss: 0.0015\n",
      "step 193400: loss: 0.0012\n",
      "step 193500: loss: 0.0011\n",
      "step 193600: loss: 0.0010\n",
      "step 193700: loss: 0.0010\n",
      "step 193800: loss: 0.0011\n",
      "step 193900: loss: 0.0012\n",
      "step 194000: loss: 0.0013\n",
      "step 194100: loss: 0.0012\n",
      "step 194200: loss: 0.0012\n",
      "step 194300: loss: 0.0010\n",
      "step 194400: loss: 0.0011\n",
      "step 194500: loss: 0.0010\n",
      "step 194600: loss: 0.0009\n",
      "step 194700: loss: 0.0010\n",
      "step 194800: loss: 0.0010\n",
      "step 194900: loss: 0.0013\n",
      "step 195000: loss: 0.0011\n",
      "step 195100: loss: 0.0009\n",
      "step 195200: loss: 0.0009\n",
      "step 195300: loss: 0.0012\n",
      "step 195400: loss: 0.0015\n",
      "step 195500: loss: 0.0012\n",
      "step 195600: loss: 0.0013\n",
      "step 195700: loss: 0.0021\n",
      "step 195800: loss: 0.0017\n",
      "step 195900: loss: 0.0013\n",
      "step 196000: loss: 0.0013\n",
      "step 196100: loss: 0.0011\n",
      "step 196200: loss: 0.0010\n",
      "step 196300: loss: 0.0011\n",
      "step 196400: loss: 0.0011\n",
      "step 196500: loss: 0.0012\n",
      "step 196600: loss: 0.0018\n",
      "step 196700: loss: 0.0014\n",
      "step 196800: loss: 0.0013\n",
      "step 196900: loss: 0.0011\n",
      "step 197000: loss: 0.0011\n",
      "step 197100: loss: 0.0009\n",
      "step 197200: loss: 0.0013\n",
      "step 197300: loss: 0.0013\n",
      "step 197400: loss: 0.0013\n",
      "step 197500: loss: 0.0011\n",
      "step 197600: loss: 0.0013\n",
      "step 197700: loss: 0.0012\n",
      "step 197800: loss: 0.0012\n",
      "step 197900: loss: 0.0010\n",
      "step 198000: loss: 0.0011\n",
      "step 198100: loss: 0.0010\n",
      "step 198200: loss: 0.0009\n",
      "step 198300: loss: 0.0010\n",
      "step 198400: loss: 0.0010\n",
      "step 198500: loss: 0.0010\n",
      "step 198600: loss: 0.0009\n",
      "step 198700: loss: 0.0010\n",
      "step 198800: loss: 0.0009\n",
      "step 198900: loss: 0.0011\n",
      "step 199000: loss: 0.0010\n",
      "step 199100: loss: 0.0011\n",
      "step 199200: loss: 0.0009\n",
      "step 199300: loss: 0.0010\n",
      "step 199400: loss: 0.0009\n",
      "step 199500: loss: 0.0011\n",
      "step 199600: loss: 0.0012\n",
      "step 199700: loss: 0.0011\n",
      "step 199800: loss: 0.0016\n",
      "step 199900: loss: 0.0014\n",
      "step 200000: loss: 0.0012\n",
      "step 200100: loss: 0.0011\n",
      "step 200200: loss: 0.0011\n",
      "step 200300: loss: 0.0011\n",
      "step 200400: loss: 0.0012\n",
      "step 200500: loss: 0.0010\n",
      "step 200600: loss: 0.0010\n",
      "step 200700: loss: 0.0010\n",
      "step 200800: loss: 0.0009\n",
      "step 200900: loss: 0.0011\n",
      "step 201000: loss: 0.0012\n",
      "step 201100: loss: 0.0019\n",
      "step 201200: loss: 0.0014\n",
      "step 201300: loss: 0.0010\n",
      "step 201400: loss: 0.0009\n",
      "step 201500: loss: 0.0009\n",
      "step 201600: loss: 0.0009\n",
      "step 201700: loss: 0.0011\n",
      "step 201800: loss: 0.0011\n",
      "step 201900: loss: 0.0012\n",
      "step 202000: loss: 0.0011\n",
      "step 202100: loss: 0.0010\n",
      "step 202200: loss: 0.0009\n",
      "step 202300: loss: 0.0009\n",
      "step 202400: loss: 0.0010\n",
      "step 202500: loss: 0.0009\n",
      "step 202600: loss: 0.0010\n",
      "step 202700: loss: 0.0009\n",
      "step 202800: loss: 0.0010\n",
      "step 202900: loss: 0.0009\n",
      "step 203000: loss: 0.0009\n",
      "step 203100: loss: 0.0009\n",
      "step 203200: loss: 0.0011\n",
      "step 203300: loss: 0.0010\n",
      "step 203400: loss: 0.0010\n",
      "step 203500: loss: 0.0010\n",
      "step 203600: loss: 0.0009\n",
      "step 203700: loss: 0.0011\n",
      "step 203800: loss: 0.0010\n",
      "step 203900: loss: 0.0010\n",
      "step 204000: loss: 0.0010\n",
      "step 204100: loss: 0.0009\n",
      "step 204200: loss: 0.0009\n",
      "step 204300: loss: 0.0011\n",
      "step 204400: loss: 0.0009\n",
      "step 204500: loss: 0.0011\n",
      "step 204600: loss: 0.0010\n",
      "step 204700: loss: 0.0010\n",
      "step 204800: loss: 0.0009\n",
      "step 204900: loss: 0.0010\n",
      "step 205000: loss: 0.0009\n",
      "step 205100: loss: 0.0011\n",
      "step 205200: loss: 0.0010\n",
      "step 205300: loss: 0.0011\n",
      "step 205400: loss: 0.0012\n",
      "step 205500: loss: 0.0011\n",
      "step 205600: loss: 0.0010\n",
      "step 205700: loss: 0.0011\n",
      "step 205800: loss: 0.0009\n",
      "step 205900: loss: 0.0012\n",
      "step 206000: loss: 0.0010\n",
      "step 206100: loss: 0.0010\n",
      "step 206200: loss: 0.0010\n",
      "step 206300: loss: 0.0009\n",
      "step 206400: loss: 0.0009\n",
      "step 206500: loss: 0.0009\n",
      "step 206600: loss: 0.0010\n",
      "step 206700: loss: 0.0010\n",
      "step 206800: loss: 0.0009\n",
      "step 206900: loss: 0.0009\n",
      "step 207000: loss: 0.0009\n",
      "step 207100: loss: 0.0011\n",
      "step 207200: loss: 0.0012\n",
      "step 207300: loss: 0.0010\n",
      "step 207400: loss: 0.0010\n",
      "step 207500: loss: 0.0009\n",
      "step 207600: loss: 0.0008\n",
      "step 207700: loss: 0.0009\n",
      "step 207800: loss: 0.0011\n",
      "step 207900: loss: 0.0011\n",
      "step 208000: loss: 0.0010\n",
      "step 208100: loss: 0.0009\n",
      "step 208200: loss: 0.0012\n",
      "step 208300: loss: 0.0012\n",
      "step 208400: loss: 0.0010\n",
      "step 208500: loss: 0.0010\n",
      "step 208600: loss: 0.0012\n",
      "step 208700: loss: 0.0011\n",
      "step 208800: loss: 0.0010\n",
      "step 208900: loss: 0.0010\n",
      "step 209000: loss: 0.0010\n",
      "step 209100: loss: 0.0010\n",
      "step 209200: loss: 0.0011\n",
      "step 209300: loss: 0.0014\n",
      "step 209400: loss: 0.0013\n",
      "step 209500: loss: 0.0011\n",
      "step 209600: loss: 0.0011\n",
      "step 209700: loss: 0.0009\n",
      "step 209800: loss: 0.0009\n",
      "step 209900: loss: 0.0010\n",
      "step 210000: loss: 0.0010\n",
      "step 210100: loss: 0.0009\n",
      "step 210200: loss: 0.0008\n",
      "step 210300: loss: 0.0010\n",
      "step 210400: loss: 0.0010\n",
      "step 210500: loss: 0.0010\n",
      "step 210600: loss: 0.0009\n",
      "step 210700: loss: 0.0010\n",
      "step 210800: loss: 0.0010\n",
      "step 210900: loss: 0.0010\n",
      "step 211000: loss: 0.0009\n",
      "step 211100: loss: 0.0009\n",
      "step 211200: loss: 0.0008\n",
      "step 211300: loss: 0.0010\n",
      "step 211400: loss: 0.0009\n",
      "step 211500: loss: 0.0008\n",
      "step 211600: loss: 0.0009\n",
      "step 211700: loss: 0.0012\n",
      "step 211800: loss: 0.0010\n",
      "step 211900: loss: 0.0011\n",
      "step 212000: loss: 0.0010\n",
      "step 212100: loss: 0.0010\n",
      "step 212200: loss: 0.0010\n",
      "step 212300: loss: 0.0010\n",
      "step 212400: loss: 0.0009\n",
      "step 212500: loss: 0.0008\n",
      "step 212600: loss: 0.0009\n",
      "step 212700: loss: 0.0010\n",
      "step 212800: loss: 0.0010\n",
      "step 212900: loss: 0.0009\n",
      "step 213000: loss: 0.0010\n",
      "step 213100: loss: 0.0010\n",
      "step 213200: loss: 0.0011\n",
      "step 213300: loss: 0.0013\n",
      "step 213400: loss: 0.0010\n",
      "step 213500: loss: 0.0011\n",
      "step 213600: loss: 0.0010\n",
      "step 213700: loss: 0.0010\n",
      "step 213800: loss: 0.0010\n",
      "step 213900: loss: 0.0010\n",
      "step 214000: loss: 0.0013\n",
      "step 214100: loss: 0.0010\n",
      "step 214200: loss: 0.0010\n",
      "step 214300: loss: 0.0010\n",
      "step 214400: loss: 0.0010\n",
      "step 214500: loss: 0.0010\n",
      "step 214600: loss: 0.0010\n",
      "step 214700: loss: 0.0011\n",
      "step 214800: loss: 0.0011\n",
      "step 214900: loss: 0.0010\n",
      "step 215000: loss: 0.0008\n",
      "step 215100: loss: 0.0009\n",
      "step 215200: loss: 0.0009\n",
      "step 215300: loss: 0.0010\n",
      "step 215400: loss: 0.0010\n",
      "step 215500: loss: 0.0009\n",
      "step 215600: loss: 0.0009\n",
      "step 215700: loss: 0.0009\n",
      "step 215800: loss: 0.0009\n",
      "step 215900: loss: 0.0010\n",
      "step 216000: loss: 0.0010\n",
      "step 216100: loss: 0.0009\n",
      "step 216200: loss: 0.0008\n",
      "step 216300: loss: 0.0008\n",
      "step 216400: loss: 0.0009\n",
      "step 216500: loss: 0.0011\n",
      "step 216600: loss: 0.0010\n",
      "step 216700: loss: 0.0008\n",
      "step 216800: loss: 0.0010\n",
      "step 216900: loss: 0.0009\n",
      "step 217000: loss: 0.0010\n",
      "step 217100: loss: 0.0010\n",
      "step 217200: loss: 0.0010\n",
      "step 217300: loss: 0.0009\n",
      "step 217400: loss: 0.0009\n",
      "step 217500: loss: 0.0008\n",
      "step 217600: loss: 0.0010\n",
      "step 217700: loss: 0.0010\n",
      "step 217800: loss: 0.0009\n",
      "step 217900: loss: 0.0010\n",
      "step 218000: loss: 0.0009\n",
      "step 218100: loss: 0.0010\n",
      "step 218200: loss: 0.0009\n",
      "step 218300: loss: 0.0009\n",
      "step 218400: loss: 0.0009\n",
      "step 218500: loss: 0.0008\n",
      "step 218600: loss: 0.0009\n",
      "step 218700: loss: 0.0010\n",
      "step 218800: loss: 0.0009\n",
      "step 218900: loss: 0.0011\n",
      "step 219000: loss: 0.0010\n",
      "step 219100: loss: 0.0009\n",
      "step 219200: loss: 0.0009\n",
      "step 219300: loss: 0.0009\n",
      "step 219400: loss: 0.0008\n",
      "step 219500: loss: 0.0008\n",
      "step 219600: loss: 0.0009\n",
      "step 219700: loss: 0.0008\n",
      "step 219800: loss: 0.0011\n",
      "step 219900: loss: 0.0011\n",
      "step 220000: loss: 0.0009\n",
      "step 220100: loss: 0.0010\n",
      "step 220200: loss: 0.0009\n",
      "step 220300: loss: 0.0010\n",
      "step 220400: loss: 0.0011\n",
      "step 220500: loss: 0.0010\n",
      "step 220600: loss: 0.0012\n",
      "step 220700: loss: 0.0010\n",
      "step 220800: loss: 0.0009\n",
      "step 220900: loss: 0.0008\n",
      "step 221000: loss: 0.0007\n",
      "step 221100: loss: 0.0008\n",
      "step 221200: loss: 0.0010\n",
      "step 221300: loss: 0.0009\n",
      "step 221400: loss: 0.0008\n",
      "step 221500: loss: 0.0010\n",
      "step 221600: loss: 0.0009\n",
      "step 221700: loss: 0.0009\n",
      "step 221800: loss: 0.0010\n",
      "step 221900: loss: 0.0011\n",
      "step 222000: loss: 0.0008\n",
      "step 222100: loss: 0.0009\n",
      "step 222200: loss: 0.0008\n",
      "step 222300: loss: 0.0007\n",
      "step 222400: loss: 0.0007\n",
      "step 222500: loss: 0.0008\n",
      "step 222600: loss: 0.0008\n",
      "step 222700: loss: 0.0011\n",
      "step 222800: loss: 0.0009\n",
      "step 222900: loss: 0.0009\n",
      "step 223000: loss: 0.0010\n",
      "step 223100: loss: 0.0010\n",
      "step 223200: loss: 0.0011\n",
      "step 223300: loss: 0.0012\n",
      "step 223400: loss: 0.0011\n",
      "step 223500: loss: 0.0010\n",
      "step 223600: loss: 0.0009\n",
      "step 223700: loss: 0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 223800: loss: 0.0010\n",
      "step 223900: loss: 0.0009\n",
      "step 224000: loss: 0.0011\n",
      "step 224100: loss: 0.0014\n",
      "step 224200: loss: 0.0011\n",
      "step 224300: loss: 0.0011\n",
      "step 224400: loss: 0.0010\n",
      "step 224500: loss: 0.0009\n",
      "step 224600: loss: 0.0010\n",
      "step 224700: loss: 0.0010\n",
      "step 224800: loss: 0.0009\n",
      "step 224900: loss: 0.0009\n",
      "step 225000: loss: 0.0010\n",
      "step 225100: loss: 0.0010\n",
      "step 225200: loss: 0.0011\n",
      "step 225300: loss: 0.0009\n",
      "step 225400: loss: 0.0019\n",
      "step 225500: loss: 0.0012\n",
      "step 225600: loss: 0.0011\n",
      "step 225700: loss: 0.0010\n",
      "step 225800: loss: 0.0009\n",
      "step 225900: loss: 0.0009\n",
      "step 226000: loss: 0.0009\n",
      "step 226100: loss: 0.0009\n",
      "step 226200: loss: 0.0010\n",
      "step 226300: loss: 0.0009\n",
      "step 226400: loss: 0.0009\n",
      "step 226500: loss: 0.0009\n",
      "step 226600: loss: 0.0008\n",
      "step 226700: loss: 0.0009\n",
      "step 226800: loss: 0.0009\n",
      "step 226900: loss: 0.0010\n",
      "step 227000: loss: 0.0009\n",
      "step 227100: loss: 0.0010\n",
      "step 227200: loss: 0.0009\n",
      "step 227300: loss: 0.0012\n",
      "step 227400: loss: 0.0011\n",
      "step 227500: loss: 0.0010\n",
      "step 227600: loss: 0.0009\n",
      "step 227700: loss: 0.0010\n",
      "step 227800: loss: 0.0010\n",
      "step 227900: loss: 0.0009\n",
      "step 228000: loss: 0.0008\n",
      "step 228100: loss: 0.0010\n",
      "step 228200: loss: 0.0008\n",
      "step 228300: loss: 0.0013\n",
      "step 228400: loss: 0.0012\n",
      "step 228500: loss: 0.0011\n",
      "step 228600: loss: 0.0010\n",
      "step 228700: loss: 0.0011\n",
      "step 228800: loss: 0.0010\n",
      "step 228900: loss: 0.0010\n",
      "step 229000: loss: 0.0008\n",
      "step 229100: loss: 0.0010\n",
      "step 229200: loss: 0.0009\n",
      "step 229300: loss: 0.0011\n",
      "step 229400: loss: 0.0010\n",
      "step 229500: loss: 0.0009\n",
      "step 229600: loss: 0.0009\n",
      "step 229700: loss: 0.0008\n",
      "step 229800: loss: 0.0009\n",
      "step 229900: loss: 0.0009\n",
      "step 230000: loss: 0.0009\n",
      "step 230100: loss: 0.0009\n",
      "step 230200: loss: 0.0010\n",
      "step 230300: loss: 0.0009\n",
      "step 230400: loss: 0.0008\n",
      "step 230500: loss: 0.0008\n",
      "step 230600: loss: 0.0008\n",
      "step 230700: loss: 0.0008\n",
      "step 230800: loss: 0.0008\n",
      "step 230900: loss: 0.0010\n",
      "step 231000: loss: 0.0010\n",
      "step 231100: loss: 0.0010\n",
      "step 231200: loss: 0.0010\n",
      "step 231300: loss: 0.0012\n",
      "step 231400: loss: 0.0009\n",
      "step 231500: loss: 0.0009\n",
      "step 231600: loss: 0.0009\n",
      "step 231700: loss: 0.0009\n",
      "step 231800: loss: 0.0009\n",
      "step 231900: loss: 0.0009\n",
      "step 232000: loss: 0.0009\n",
      "step 232100: loss: 0.0009\n",
      "step 232200: loss: 0.0008\n",
      "step 232300: loss: 0.0010\n",
      "step 232400: loss: 0.0008\n",
      "step 232500: loss: 0.0009\n",
      "step 232600: loss: 0.0010\n",
      "step 232700: loss: 0.0008\n",
      "step 232800: loss: 0.0009\n",
      "step 232900: loss: 0.0012\n",
      "step 233000: loss: 0.0010\n",
      "step 233100: loss: 0.0009\n",
      "step 233200: loss: 0.0009\n",
      "step 233300: loss: 0.0008\n",
      "step 233400: loss: 0.0009\n",
      "step 233500: loss: 0.0008\n",
      "step 233600: loss: 0.0008\n",
      "step 233700: loss: 0.0012\n",
      "step 233800: loss: 0.0010\n",
      "step 233900: loss: 0.0008\n",
      "step 234000: loss: 0.0009\n",
      "step 234100: loss: 0.0009\n",
      "step 234200: loss: 0.0009\n",
      "step 234300: loss: 0.0009\n",
      "step 234400: loss: 0.0009\n",
      "step 234500: loss: 0.0008\n",
      "step 234600: loss: 0.0008\n",
      "step 234700: loss: 0.0008\n",
      "step 234800: loss: 0.0009\n",
      "step 234900: loss: 0.0010\n",
      "step 235000: loss: 0.0009\n",
      "step 235100: loss: 0.0009\n",
      "step 235200: loss: 0.0010\n",
      "step 235300: loss: 0.0010\n",
      "step 235400: loss: 0.0010\n",
      "step 235500: loss: 0.0009\n",
      "step 235600: loss: 0.0011\n",
      "step 235700: loss: 0.0010\n",
      "step 235800: loss: 0.0009\n",
      "step 235900: loss: 0.0009\n",
      "step 236000: loss: 0.0008\n",
      "step 236100: loss: 0.0010\n",
      "step 236200: loss: 0.0009\n",
      "step 236300: loss: 0.0009\n",
      "step 236400: loss: 0.0009\n",
      "step 236500: loss: 0.0009\n",
      "step 236600: loss: 0.0009\n",
      "step 236700: loss: 0.0010\n",
      "step 236800: loss: 0.0009\n",
      "step 236900: loss: 0.0009\n",
      "step 237000: loss: 0.0009\n",
      "step 237100: loss: 0.0009\n",
      "step 237200: loss: 0.0009\n",
      "step 237300: loss: 0.0009\n",
      "step 237400: loss: 0.0009\n",
      "step 237500: loss: 0.0010\n",
      "step 237600: loss: 0.0009\n",
      "step 237700: loss: 0.0010\n",
      "step 237800: loss: 0.0011\n",
      "step 237900: loss: 0.0009\n",
      "step 238000: loss: 0.0009\n",
      "step 238100: loss: 0.0008\n",
      "step 238200: loss: 0.0008\n",
      "step 238300: loss: 0.0008\n",
      "step 238400: loss: 0.0007\n",
      "step 238500: loss: 0.0008\n",
      "step 238600: loss: 0.0008\n",
      "step 238700: loss: 0.0008\n",
      "step 238800: loss: 0.0011\n",
      "step 238900: loss: 0.0009\n",
      "step 239000: loss: 0.0009\n",
      "step 239100: loss: 0.0009\n",
      "step 239200: loss: 0.0011\n",
      "step 239300: loss: 0.0012\n",
      "step 239400: loss: 0.0013\n",
      "step 239500: loss: 0.0012\n",
      "step 239600: loss: 0.0010\n",
      "step 239700: loss: 0.0009\n",
      "step 239800: loss: 0.0009\n",
      "step 239900: loss: 0.0015\n",
      "step 240000: loss: 0.0016\n",
      "step 240100: loss: 0.0014\n",
      "step 240200: loss: 0.0011\n",
      "step 240300: loss: 0.0009\n",
      "step 240400: loss: 0.0009\n",
      "step 240500: loss: 0.0009\n",
      "step 240600: loss: 0.0009\n",
      "step 240700: loss: 0.0009\n",
      "step 240800: loss: 0.0009\n",
      "step 240900: loss: 0.0012\n",
      "step 241000: loss: 0.0009\n",
      "step 241100: loss: 0.0008\n",
      "step 241200: loss: 0.0008\n",
      "step 241300: loss: 0.0008\n",
      "step 241400: loss: 0.0008\n",
      "step 241500: loss: 0.0009\n",
      "step 241600: loss: 0.0009\n",
      "step 241700: loss: 0.0008\n",
      "step 241800: loss: 0.0010\n",
      "step 241900: loss: 0.0013\n",
      "step 242000: loss: 0.0012\n",
      "step 242100: loss: 0.0010\n",
      "step 242200: loss: 0.0011\n",
      "step 242300: loss: 0.0011\n",
      "step 242400: loss: 0.0009\n",
      "step 242500: loss: 0.0010\n",
      "step 242600: loss: 0.0009\n",
      "step 242700: loss: 0.0010\n",
      "step 242800: loss: 0.0009\n",
      "step 242900: loss: 0.0010\n",
      "step 243000: loss: 0.0010\n",
      "step 243100: loss: 0.0010\n",
      "step 243200: loss: 0.0011\n",
      "step 243300: loss: 0.0010\n",
      "step 243400: loss: 0.0009\n",
      "step 243500: loss: 0.0008\n",
      "step 243600: loss: 0.0008\n",
      "step 243700: loss: 0.0009\n",
      "step 243800: loss: 0.0009\n",
      "step 243900: loss: 0.0010\n",
      "step 244000: loss: 0.0009\n",
      "step 244100: loss: 0.0008\n",
      "step 244200: loss: 0.0008\n",
      "step 244300: loss: 0.0009\n",
      "step 244400: loss: 0.0008\n",
      "step 244500: loss: 0.0012\n",
      "step 244600: loss: 0.0010\n",
      "step 244700: loss: 0.0009\n",
      "step 244800: loss: 0.0008\n",
      "step 244900: loss: 0.0008\n",
      "step 245000: loss: 0.0008\n",
      "step 245100: loss: 0.0009\n",
      "step 245200: loss: 0.0008\n",
      "step 245300: loss: 0.0008\n",
      "step 245400: loss: 0.0010\n",
      "step 245500: loss: 0.0008\n",
      "step 245600: loss: 0.0009\n",
      "step 245700: loss: 0.0008\n",
      "step 245800: loss: 0.0010\n",
      "step 245900: loss: 0.0011\n",
      "step 246000: loss: 0.0012\n",
      "step 246100: loss: 0.0013\n",
      "step 246200: loss: 0.0012\n",
      "step 246300: loss: 0.0012\n",
      "step 246400: loss: 0.0010\n",
      "step 246500: loss: 0.0010\n",
      "step 246600: loss: 0.0010\n",
      "step 246700: loss: 0.0011\n",
      "step 246800: loss: 0.0010\n",
      "step 246900: loss: 0.0009\n",
      "step 247000: loss: 0.0009\n",
      "step 247100: loss: 0.0008\n",
      "step 247200: loss: 0.0009\n",
      "step 247300: loss: 0.0011\n",
      "step 247400: loss: 0.0012\n",
      "step 247500: loss: 0.0009\n",
      "step 247600: loss: 0.0011\n",
      "step 247700: loss: 0.0010\n",
      "step 247800: loss: 0.0009\n",
      "step 247900: loss: 0.0010\n",
      "step 248000: loss: 0.0009\n",
      "step 248100: loss: 0.0010\n",
      "step 248200: loss: 0.0010\n",
      "step 248300: loss: 0.0012\n",
      "step 248400: loss: 0.0021\n",
      "step 248500: loss: 0.0013\n",
      "step 248600: loss: 0.0011\n",
      "step 248700: loss: 0.0009\n",
      "step 248800: loss: 0.0009\n",
      "step 248900: loss: 0.0008\n",
      "step 249000: loss: 0.0007\n",
      "step 249100: loss: 0.0009\n",
      "step 249200: loss: 0.0008\n",
      "step 249300: loss: 0.0007\n",
      "step 249400: loss: 0.0008\n",
      "step 249500: loss: 0.0008\n",
      "step 249600: loss: 0.0010\n",
      "step 249700: loss: 0.0009\n",
      "step 249800: loss: 0.0009\n",
      "step 249900: loss: 0.0015\n",
      "step 250000: loss: 0.0010\n",
      "step 250100: loss: 0.0009\n",
      "step 250200: loss: 0.0009\n",
      "step 250300: loss: 0.0009\n",
      "step 250400: loss: 0.0008\n",
      "step 250500: loss: 0.0009\n",
      "step 250600: loss: 0.0009\n",
      "step 250700: loss: 0.0009\n",
      "step 250800: loss: 0.0008\n",
      "step 250900: loss: 0.0008\n",
      "step 251000: loss: 0.0009\n",
      "step 251100: loss: 0.0009\n",
      "step 251200: loss: 0.0009\n",
      "step 251300: loss: 0.0010\n",
      "step 251400: loss: 0.0008\n",
      "step 251500: loss: 0.0009\n",
      "step 251600: loss: 0.0009\n",
      "step 251700: loss: 0.0009\n",
      "step 251800: loss: 0.0009\n",
      "step 251900: loss: 0.0008\n",
      "step 252000: loss: 0.0008\n",
      "step 252100: loss: 0.0009\n",
      "step 252200: loss: 0.0008\n",
      "step 252300: loss: 0.0020\n",
      "step 252400: loss: 0.0015\n",
      "step 252500: loss: 0.0011\n",
      "step 252600: loss: 0.0011\n",
      "step 252700: loss: 0.0009\n",
      "step 252800: loss: 0.0009\n",
      "step 252900: loss: 0.0010\n",
      "step 253000: loss: 0.0010\n",
      "step 253100: loss: 0.0024\n",
      "step 253200: loss: 0.0014\n",
      "step 253300: loss: 0.0011\n",
      "step 253400: loss: 0.0011\n",
      "step 253500: loss: 0.0010\n",
      "step 253600: loss: 0.0009\n",
      "step 253700: loss: 0.0011\n",
      "step 253800: loss: 0.0009\n",
      "step 253900: loss: 0.0009\n",
      "step 254000: loss: 0.0010\n",
      "step 254100: loss: 0.0010\n",
      "step 254200: loss: 0.0010\n",
      "step 254300: loss: 0.0009\n",
      "step 254400: loss: 0.0009\n",
      "step 254500: loss: 0.0009\n",
      "step 254600: loss: 0.0010\n",
      "step 254700: loss: 0.0009\n",
      "step 254800: loss: 0.0010\n",
      "step 254900: loss: 0.0010\n",
      "step 255000: loss: 0.0009\n",
      "step 255100: loss: 0.0008\n",
      "step 255200: loss: 0.0008\n",
      "step 255300: loss: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 255400: loss: 0.0010\n",
      "step 255500: loss: 0.0010\n",
      "step 255600: loss: 0.0008\n",
      "step 255700: loss: 0.0007\n",
      "step 255800: loss: 0.0009\n",
      "step 255900: loss: 0.0009\n",
      "step 256000: loss: 0.0008\n",
      "step 256100: loss: 0.0009\n",
      "step 256200: loss: 0.0008\n",
      "step 256300: loss: 0.0010\n",
      "step 256400: loss: 0.0009\n",
      "step 256500: loss: 0.0008\n",
      "step 256600: loss: 0.0007\n",
      "step 256700: loss: 0.0008\n",
      "step 256800: loss: 0.0008\n",
      "step 256900: loss: 0.0009\n",
      "step 257000: loss: 0.0009\n",
      "step 257100: loss: 0.0008\n",
      "step 257200: loss: 0.0009\n",
      "step 257300: loss: 0.0009\n",
      "step 257400: loss: 0.0009\n",
      "step 257500: loss: 0.0008\n",
      "step 257600: loss: 0.0009\n",
      "step 257700: loss: 0.0008\n",
      "step 257800: loss: 0.0009\n",
      "step 257900: loss: 0.0008\n",
      "step 258000: loss: 0.0008\n",
      "step 258100: loss: 0.0008\n",
      "step 258200: loss: 0.0008\n",
      "step 258300: loss: 0.0007\n",
      "step 258400: loss: 0.0008\n",
      "step 258500: loss: 0.0009\n",
      "step 258600: loss: 0.0009\n",
      "step 258700: loss: 0.0010\n",
      "step 258800: loss: 0.0008\n",
      "step 258900: loss: 0.0009\n",
      "step 259000: loss: 0.0009\n",
      "step 259100: loss: 0.0010\n",
      "step 259200: loss: 0.0011\n",
      "step 259300: loss: 0.0012\n",
      "step 259400: loss: 0.0011\n",
      "step 259500: loss: 0.0009\n",
      "step 259600: loss: 0.0010\n",
      "step 259700: loss: 0.0009\n",
      "step 259800: loss: 0.0009\n",
      "step 259900: loss: 0.0010\n",
      "step 260000: loss: 0.0009\n",
      "step 260100: loss: 0.0010\n",
      "step 260200: loss: 0.0009\n",
      "step 260300: loss: 0.0013\n",
      "step 260400: loss: 0.0010\n",
      "step 260500: loss: 0.0012\n",
      "step 260600: loss: 0.0011\n",
      "step 260700: loss: 0.0011\n",
      "step 260800: loss: 0.0009\n",
      "step 260900: loss: 0.0008\n",
      "step 261000: loss: 0.0008\n",
      "step 261100: loss: 0.0009\n",
      "step 261200: loss: 0.0010\n",
      "step 261300: loss: 0.0008\n",
      "step 261400: loss: 0.0010\n",
      "step 261500: loss: 0.0010\n",
      "step 261600: loss: 0.0008\n",
      "step 261700: loss: 0.0009\n",
      "step 261800: loss: 0.0010\n",
      "step 261900: loss: 0.0008\n",
      "step 262000: loss: 0.0009\n",
      "step 262100: loss: 0.0010\n",
      "step 262200: loss: 0.0011\n",
      "step 262300: loss: 0.0010\n",
      "step 262400: loss: 0.0009\n",
      "step 262500: loss: 0.0008\n",
      "step 262600: loss: 0.0008\n",
      "step 262700: loss: 0.0008\n",
      "step 262800: loss: 0.0008\n",
      "step 262900: loss: 0.0008\n",
      "step 263000: loss: 0.0008\n",
      "step 263100: loss: 0.0009\n",
      "step 263200: loss: 0.0008\n",
      "step 263300: loss: 0.0008\n",
      "step 263400: loss: 0.0014\n",
      "step 263500: loss: 0.0010\n",
      "step 263600: loss: 0.0010\n",
      "step 263700: loss: 0.0009\n",
      "step 263800: loss: 0.0009\n",
      "step 263900: loss: 0.0011\n",
      "step 264000: loss: 0.0009\n",
      "step 264100: loss: 0.0010\n",
      "step 264200: loss: 0.0010\n",
      "step 264300: loss: 0.0009\n",
      "step 264400: loss: 0.0009\n",
      "step 264500: loss: 0.0009\n",
      "step 264600: loss: 0.0009\n",
      "step 264700: loss: 0.0009\n",
      "step 264800: loss: 0.0009\n",
      "step 264900: loss: 0.0008\n",
      "step 265000: loss: 0.0010\n",
      "step 265100: loss: 0.0009\n",
      "step 265200: loss: 0.0009\n",
      "step 265300: loss: 0.0008\n",
      "step 265400: loss: 0.0007\n",
      "step 265500: loss: 0.0008\n",
      "step 265600: loss: 0.0008\n",
      "step 265700: loss: 0.0015\n",
      "step 265800: loss: 0.0015\n",
      "step 265900: loss: 0.0010\n",
      "step 266000: loss: 0.0010\n",
      "step 266100: loss: 0.0010\n",
      "step 266200: loss: 0.0009\n",
      "step 266300: loss: 0.0009\n",
      "step 266400: loss: 0.0007\n",
      "step 266500: loss: 0.0007\n",
      "step 266600: loss: 0.0007\n",
      "step 266700: loss: 0.0008\n",
      "step 266800: loss: 0.0010\n",
      "step 266900: loss: 0.0010\n",
      "step 267000: loss: 0.0009\n",
      "step 267100: loss: 0.0009\n",
      "step 267200: loss: 0.0009\n",
      "step 267300: loss: 0.0009\n",
      "step 267400: loss: 0.0008\n",
      "step 267500: loss: 0.0008\n",
      "step 267600: loss: 0.0009\n",
      "step 267700: loss: 0.0009\n",
      "step 267800: loss: 0.0009\n",
      "step 267900: loss: 0.0010\n",
      "step 268000: loss: 0.0011\n",
      "step 268100: loss: 0.0010\n",
      "step 268200: loss: 0.0010\n",
      "step 268300: loss: 0.0009\n",
      "step 268400: loss: 0.0024\n",
      "step 268500: loss: 0.0016\n",
      "step 268600: loss: 0.0013\n",
      "step 268700: loss: 0.0010\n",
      "step 268800: loss: 0.0010\n",
      "step 268900: loss: 0.0010\n",
      "step 269000: loss: 0.0008\n",
      "step 269100: loss: 0.0010\n",
      "step 269200: loss: 0.0010\n",
      "step 269300: loss: 0.0009\n",
      "step 269400: loss: 0.0009\n",
      "step 269500: loss: 0.0009\n",
      "step 269600: loss: 0.0008\n",
      "step 269700: loss: 0.0008\n",
      "step 269800: loss: 0.0009\n",
      "step 269900: loss: 0.0010\n",
      "step 270000: loss: 0.0009\n",
      "step 270100: loss: 0.0008\n",
      "step 270200: loss: 0.0009\n",
      "step 270300: loss: 0.0008\n",
      "step 270400: loss: 0.0007\n",
      "step 270500: loss: 0.0008\n",
      "step 270600: loss: 0.0008\n",
      "step 270700: loss: 0.0008\n",
      "step 270800: loss: 0.0011\n",
      "step 270900: loss: 0.0010\n",
      "step 271000: loss: 0.0009\n",
      "step 271100: loss: 0.0008\n",
      "step 271200: loss: 0.0009\n",
      "step 271300: loss: 0.0009\n",
      "step 271400: loss: 0.0010\n",
      "step 271500: loss: 0.0009\n",
      "step 271600: loss: 0.0009\n",
      "step 271700: loss: 0.0008\n",
      "step 271800: loss: 0.0009\n",
      "step 271900: loss: 0.0008\n",
      "step 272000: loss: 0.0010\n",
      "step 272100: loss: 0.0008\n",
      "step 272200: loss: 0.0009\n",
      "step 272300: loss: 0.0008\n",
      "step 272400: loss: 0.0010\n",
      "step 272500: loss: 0.0008\n",
      "step 272600: loss: 0.0007\n",
      "step 272700: loss: 0.0009\n",
      "step 272800: loss: 0.0008\n",
      "step 272900: loss: 0.0008\n",
      "step 273000: loss: 0.0008\n",
      "step 273100: loss: 0.0008\n",
      "step 273200: loss: 0.0009\n",
      "step 273300: loss: 0.0010\n",
      "step 273400: loss: 0.0008\n",
      "step 273500: loss: 0.0010\n",
      "step 273600: loss: 0.0010\n",
      "step 273700: loss: 0.0008\n",
      "step 273800: loss: 0.0008\n",
      "step 273900: loss: 0.0009\n",
      "step 274000: loss: 0.0008\n",
      "step 274100: loss: 0.0008\n",
      "step 274200: loss: 0.0007\n",
      "step 274300: loss: 0.0009\n",
      "step 274400: loss: 0.0008\n",
      "step 274500: loss: 0.0010\n",
      "step 274600: loss: 0.0008\n",
      "step 274700: loss: 0.0008\n",
      "step 274800: loss: 0.0008\n",
      "step 274900: loss: 0.0008\n",
      "step 275000: loss: 0.0009\n",
      "step 275100: loss: 0.0009\n",
      "step 275200: loss: 0.0009\n",
      "step 275300: loss: 0.0009\n",
      "step 275400: loss: 0.0008\n",
      "step 275500: loss: 0.0007\n",
      "step 275600: loss: 0.0008\n",
      "step 275700: loss: 0.0007\n",
      "step 275800: loss: 0.0007\n",
      "step 275900: loss: 0.0007\n",
      "step 276000: loss: 0.0008\n",
      "step 276100: loss: 0.0007\n",
      "step 276200: loss: 0.0008\n",
      "step 276300: loss: 0.0007\n",
      "step 276400: loss: 0.0011\n",
      "step 276500: loss: 0.0010\n",
      "step 276600: loss: 0.0009\n",
      "step 276700: loss: 0.0008\n",
      "step 276800: loss: 0.0008\n",
      "step 276900: loss: 0.0007\n",
      "step 277000: loss: 0.0009\n",
      "step 277100: loss: 0.0008\n",
      "step 277200: loss: 0.0008\n",
      "step 277300: loss: 0.0008\n",
      "step 277400: loss: 0.0010\n",
      "step 277500: loss: 0.0012\n",
      "step 277600: loss: 0.0011\n",
      "step 277700: loss: 0.0010\n",
      "step 277800: loss: 0.0009\n",
      "step 277900: loss: 0.0008\n",
      "step 278000: loss: 0.0008\n",
      "step 278100: loss: 0.0008\n",
      "step 278200: loss: 0.0007\n",
      "step 278300: loss: 0.0010\n",
      "step 278400: loss: 0.0011\n",
      "step 278500: loss: 0.0009\n",
      "step 278600: loss: 0.0009\n",
      "step 278700: loss: 0.0010\n",
      "step 278800: loss: 0.0010\n",
      "step 278900: loss: 0.0009\n",
      "step 279000: loss: 0.0010\n",
      "step 279100: loss: 0.0009\n",
      "step 279200: loss: 0.0010\n",
      "step 279300: loss: 0.0011\n",
      "step 279400: loss: 0.0010\n",
      "step 279500: loss: 0.0010\n",
      "step 279600: loss: 0.0009\n",
      "step 279700: loss: 0.0008\n",
      "step 279800: loss: 0.0007\n",
      "step 279900: loss: 0.0007\n",
      "step 280000: loss: 0.0007\n",
      "step 280100: loss: 0.0006\n",
      "step 280200: loss: 0.0007\n",
      "step 280300: loss: 0.0007\n",
      "step 280400: loss: 0.0011\n",
      "step 280500: loss: 0.0009\n",
      "step 280600: loss: 0.0010\n",
      "step 280700: loss: 0.0011\n",
      "step 280800: loss: 0.0009\n",
      "step 280900: loss: 0.0014\n",
      "step 281000: loss: 0.0012\n",
      "step 281100: loss: 0.0010\n",
      "step 281200: loss: 0.0011\n",
      "step 281300: loss: 0.0010\n",
      "step 281400: loss: 0.0009\n",
      "step 281500: loss: 0.0009\n",
      "step 281600: loss: 0.0009\n",
      "step 281700: loss: 0.0008\n",
      "step 281800: loss: 0.0008\n",
      "step 281900: loss: 0.0012\n",
      "step 282000: loss: 0.0010\n",
      "step 282100: loss: 0.0010\n",
      "step 282200: loss: 0.0010\n",
      "step 282300: loss: 0.0009\n",
      "step 282400: loss: 0.0009\n",
      "step 282500: loss: 0.0008\n",
      "step 282600: loss: 0.0010\n",
      "step 282700: loss: 0.0009\n",
      "step 282800: loss: 0.0009\n",
      "step 282900: loss: 0.0010\n",
      "step 283000: loss: 0.0009\n",
      "step 283100: loss: 0.0009\n",
      "step 283200: loss: 0.0009\n",
      "step 283300: loss: 0.0008\n",
      "step 283400: loss: 0.0008\n",
      "step 283500: loss: 0.0008\n",
      "step 283600: loss: 0.0009\n",
      "step 283700: loss: 0.0008\n",
      "step 283800: loss: 0.0007\n",
      "step 283900: loss: 0.0008\n",
      "step 284000: loss: 0.0008\n",
      "step 284100: loss: 0.0009\n",
      "step 284200: loss: 0.0009\n",
      "step 284300: loss: 0.0008\n",
      "step 284400: loss: 0.0008\n",
      "step 284500: loss: 0.0007\n",
      "step 284600: loss: 0.0007\n",
      "step 284700: loss: 0.0008\n",
      "step 284800: loss: 0.0007\n",
      "step 284900: loss: 0.0007\n",
      "step 285000: loss: 0.0008\n",
      "step 285100: loss: 0.0010\n",
      "step 285200: loss: 0.0009\n",
      "step 285300: loss: 0.0008\n",
      "step 285400: loss: 0.0010\n",
      "step 285500: loss: 0.0009\n",
      "step 285600: loss: 0.0010\n",
      "step 285700: loss: 0.0010\n",
      "step 285800: loss: 0.0010\n",
      "step 285900: loss: 0.0009\n",
      "step 286000: loss: 0.0009\n",
      "step 286100: loss: 0.0011\n",
      "step 286200: loss: 0.0009\n",
      "step 286300: loss: 0.0009\n",
      "step 286400: loss: 0.0008\n",
      "step 286500: loss: 0.0008\n",
      "step 286600: loss: 0.0008\n",
      "step 286700: loss: 0.0007\n",
      "step 286800: loss: 0.0007\n",
      "step 286900: loss: 0.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 287000: loss: 0.0007\n",
      "step 287100: loss: 0.0008\n",
      "step 287200: loss: 0.0008\n",
      "step 287300: loss: 0.0007\n",
      "step 287400: loss: 0.0007\n",
      "step 287500: loss: 0.0006\n",
      "step 287600: loss: 0.0006\n",
      "step 287700: loss: 0.0007\n",
      "step 287800: loss: 0.0009\n",
      "step 287900: loss: 0.0009\n",
      "step 288000: loss: 0.0009\n",
      "step 288100: loss: 0.0009\n",
      "step 288200: loss: 0.0010\n",
      "step 288300: loss: 0.0010\n",
      "step 288400: loss: 0.0009\n",
      "step 288500: loss: 0.0010\n",
      "step 288600: loss: 0.0009\n",
      "step 288700: loss: 0.0008\n",
      "step 288800: loss: 0.0010\n",
      "step 288900: loss: 0.0011\n",
      "step 289000: loss: 0.0009\n",
      "step 289100: loss: 0.0008\n",
      "step 289200: loss: 0.0010\n",
      "step 289300: loss: 0.0010\n",
      "step 289400: loss: 0.0009\n",
      "step 289500: loss: 0.0011\n",
      "step 289600: loss: 0.0009\n",
      "step 289700: loss: 0.0008\n",
      "step 289800: loss: 0.0008\n",
      "step 289900: loss: 0.0008\n",
      "step 290000: loss: 0.0008\n",
      "step 290100: loss: 0.0007\n",
      "step 290200: loss: 0.0009\n",
      "step 290300: loss: 0.0008\n",
      "step 290400: loss: 0.0007\n",
      "step 290500: loss: 0.0007\n",
      "step 290600: loss: 0.0008\n",
      "step 290700: loss: 0.0008\n",
      "step 290800: loss: 0.0008\n",
      "step 290900: loss: 0.0008\n",
      "step 291000: loss: 0.0008\n",
      "step 291100: loss: 0.0009\n",
      "step 291200: loss: 0.0008\n",
      "step 291300: loss: 0.0008\n",
      "step 291400: loss: 0.0008\n",
      "step 291500: loss: 0.0009\n",
      "step 291600: loss: 0.0008\n",
      "step 291700: loss: 0.0007\n",
      "step 291800: loss: 0.0008\n",
      "step 291900: loss: 0.0008\n",
      "step 292000: loss: 0.0008\n",
      "step 292100: loss: 0.0007\n",
      "step 292200: loss: 0.0007\n",
      "step 292300: loss: 0.0007\n",
      "step 292400: loss: 0.0006\n",
      "step 292500: loss: 0.0007\n",
      "step 292600: loss: 0.0012\n",
      "step 292700: loss: 0.0011\n",
      "step 292800: loss: 0.0010\n",
      "step 292900: loss: 0.0008\n",
      "step 293000: loss: 0.0007\n",
      "step 293100: loss: 0.0008\n",
      "step 293200: loss: 0.0008\n",
      "step 293300: loss: 0.0009\n",
      "step 293400: loss: 0.0008\n",
      "step 293500: loss: 0.0008\n",
      "step 293600: loss: 0.0007\n",
      "step 293700: loss: 0.0007\n",
      "step 293800: loss: 0.0007\n",
      "step 293900: loss: 0.0008\n",
      "step 294000: loss: 0.0007\n",
      "step 294100: loss: 0.0008\n",
      "step 294200: loss: 0.0009\n",
      "step 294300: loss: 0.0008\n",
      "step 294400: loss: 0.0007\n",
      "step 294500: loss: 0.0008\n",
      "step 294600: loss: 0.0007\n",
      "step 294700: loss: 0.0008\n",
      "step 294800: loss: 0.0007\n",
      "step 294900: loss: 0.0007\n",
      "step 295000: loss: 0.0007\n",
      "step 295100: loss: 0.0008\n",
      "step 295200: loss: 0.0009\n",
      "step 295300: loss: 0.0008\n",
      "step 295400: loss: 0.0008\n",
      "step 295500: loss: 0.0008\n",
      "step 295600: loss: 0.0008\n",
      "step 295700: loss: 0.0008\n",
      "step 295800: loss: 0.0008\n",
      "step 295900: loss: 0.0008\n",
      "step 296000: loss: 0.0008\n",
      "step 296100: loss: 0.0007\n",
      "step 296200: loss: 0.0008\n",
      "step 296300: loss: 0.0008\n",
      "step 296400: loss: 0.0008\n",
      "step 296500: loss: 0.0007\n",
      "step 296600: loss: 0.0009\n",
      "step 296700: loss: 0.0009\n",
      "step 296800: loss: 0.0008\n",
      "step 296900: loss: 0.0008\n",
      "step 297000: loss: 0.0007\n",
      "step 297100: loss: 0.0008\n",
      "step 297200: loss: 0.0008\n",
      "step 297300: loss: 0.0009\n",
      "step 297400: loss: 0.0008\n",
      "step 297500: loss: 0.0010\n",
      "step 297600: loss: 0.0010\n",
      "step 297700: loss: 0.0013\n",
      "step 297800: loss: 0.0014\n",
      "step 297900: loss: 0.0010\n",
      "step 298000: loss: 0.0010\n",
      "step 298100: loss: 0.0010\n",
      "step 298200: loss: 0.0008\n",
      "step 298300: loss: 0.0011\n",
      "step 298400: loss: 0.0011\n",
      "step 298500: loss: 0.0008\n",
      "step 298600: loss: 0.0007\n",
      "step 298700: loss: 0.0008\n",
      "step 298800: loss: 0.0008\n",
      "step 298900: loss: 0.0008\n",
      "step 299000: loss: 0.0008\n",
      "step 299100: loss: 0.0008\n",
      "step 299200: loss: 0.0008\n",
      "step 299300: loss: 0.0008\n",
      "step 299400: loss: 0.0009\n",
      "step 299500: loss: 0.0009\n",
      "step 299600: loss: 0.0009\n",
      "step 299700: loss: 0.0008\n",
      "step 299800: loss: 0.0009\n",
      "step 299900: loss: 0.0008\n",
      "step 300000: loss: 0.0008\n",
      "\tstep 300000: saved model\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "running_loss = None\n",
    "model_zoo = []\n",
    "# generic train loop\n",
    "for global_step in range(global_step, total_steps+global_step+1):\n",
    "    \n",
    "    # ======== DISCRIMINATOR STEP ======== #\n",
    "    # forward\n",
    "    np_H, np_e0, np_psi0 = next_batch(D_side, batch_size)\n",
    "    real_H = Variable(torch.Tensor(np_H))\n",
    "    real_e0 = Variable(torch.Tensor(np_e0))\n",
    "    real_psi0 = Variable(torch.Tensor(np_psi0))\n",
    "    \n",
    "    psi0_hat = model(real_H, batch_size) \n",
    "\n",
    "    # backward\n",
    "#     loss = .5*torch.sum(cost_func(psi0_hat, real_psi0)**2) / batch_size\n",
    "    loss = cost_func(real_e0, psi0_hat, real_H, batch_size)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    running_loss = loss.data.numpy()[0] if running_loss is None else .99*running_loss + (1-.99)*loss.data.numpy()[0]\n",
    "\n",
    "    # ======== DISPLAY PROGRESS ======== #\n",
    "    if global_step % print_every == 0:\n",
    "        print('step {}: loss: {:.4f}'.format(global_step, running_loss))\n",
    "    if global_step in checkpoint_steps:\n",
    "        print('\\tstep {}: saved model'.format(global_step))\n",
    "        torch.save({'state_dict': model.state_dict(),\n",
    "                   'global_step': global_step}\n",
    "                   , save_dir + '/model.{}.tar'.format(global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model: H2psi0-models/model.10.tar\n",
      "loaded model: H2psi0-models/model.100.tar\n",
      "loaded model: H2psi0-models/model.1000.tar\n",
      "loaded model: H2psi0-models/model.10000.tar\n",
      "loaded model: H2psi0-models/model.100000.tar\n",
      "loaded model: H2psi0-models/model.30.tar\n",
      "loaded model: H2psi0-models/model.300.tar\n",
      "loaded model: H2psi0-models/model.3000.tar\n",
      "loaded model: H2psi0-models/model.30000.tar\n",
      "loaded model: H2psi0-models/model.300000.tar\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "model_zoo = []\n",
    "paths = glob.glob(save_dir + '/*.tar')\n",
    "\n",
    "try:\n",
    "    for s in paths:\n",
    "        checkpoint = torch.load(s)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        global_step = checkpoint['global_step']\n",
    "        model_zoo.append((global_step, copy.deepcopy(model)))\n",
    "        print(\"loaded model: {}\".format(s))\n",
    "    model_zoo = sorted(model_zoo, key=lambda tup: tup[0])\n",
    "    global_step, model = model_zoo[-1]\n",
    "except:\n",
    "    print(\"no saved model to load.\") ; model_zoo = []\n",
    "    load_was_success = False\n",
    "else:\n",
    "    if len(paths) is 0: print(\"no saved model to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.48100428655743599, 0.51647604629397392]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAACsCAYAAAB8QXjYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKCIiInpSgqCIiIjoSQmCIiIioif9f6WryVB4ojhZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109f0fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = 1\n",
    "np_H, np_e0, np_psi0 = next_batch(D_side, batch_size=b)\n",
    "real_H = Variable(torch.Tensor(np_H))\n",
    "real_psi0 = Variable(torch.Tensor(np_psi0))\n",
    "psi0_hat = model(real_H, batch_size=b) \n",
    "\n",
    "np_real_psi0 = real_psi0.data.numpy()\n",
    "np_psi0_hat = psi0_hat.data.numpy()\n",
    "\n",
    "sd, mn = np.std(np_real_psi0), np.mean(np_real_psi0)\n",
    "cl = [mn-2*sd,mn+2*sd]\n",
    "use_cl = True\n",
    "if use_cl: print(cl)\n",
    "\n",
    "f2 = plt.figure(figsize=[8,3])\n",
    "plt.subplot(211)\n",
    "plt.imshow(np_real_psi0) ; plt.title('exact $\\psi_0$')\n",
    "if use_cl: plt.clim(*cl)\n",
    "plt.subplot(212)\n",
    "im = plt.imshow(np_psi0_hat) ; plt.title('low-energy estimate $\\hat \\psi_{NN}$')\n",
    "if use_cl: plt.clim(*cl)\n",
    "\n",
    "cax = f2.add_axes([0.95, 0.15, 0.02, 0.7])\n",
    "cb = f2.colorbar(im, cax=cax, orientation='vertical')\n",
    "cb.set_label('color scale')\n",
    "\n",
    "plt.show() ; f2.savefig('./figures/H2psi0.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psi0_e: -1.2861 / e0_e: -1.2861\n",
      "psi1_e: -1.2384 / e1_e: -1.2384\n",
      "psi2_e: -1.0004 / e2_e: -1.0004\n",
      "hat psi0_e: -1.2603\n",
      "rand psi_e: 0.0032\n",
      "\n",
      "mean err nn  : 0.0258\n",
      "mean err rand: 1.2894\n",
      "\n",
      "mean err nn  : 1.9669%\n",
      "mean err rand: 0.2518%\n"
     ]
    }
   ],
   "source": [
    "# b=5000\n",
    "\n",
    "# np_H, np_e0, np_psi0, np_e1, np_psi1, np_e2, np_psi2 = next_batch(D_side, batch_size=b, just_ground=False)\n",
    "# real_H = Variable(torch.Tensor(np_H))\n",
    "# real_e0 = Variable(torch.Tensor(np_e0))\n",
    "# real_psi0 = Variable(torch.Tensor(np_psi0))\n",
    "# real_e1 = Variable(torch.Tensor(np_e1))\n",
    "# real_psi1 = Variable(torch.Tensor(np_psi1))\n",
    "# real_e2 = Variable(torch.Tensor(np_e2))\n",
    "# real_psi2 = Variable(torch.Tensor(np_psi2))\n",
    "# psi0_hat = model(real_H, batch_size=b) \n",
    "\n",
    "# psis = []\n",
    "# for _ in range(b):\n",
    "#     psis.append(rand_psi(D_side))\n",
    "# rand_psis = Variable(torch.Tensor(np.vstack(psis)))\n",
    "\n",
    "# psi0_e = energy_func(real_psi0, real_H, batch_size=b).data.numpy()\n",
    "# psi1_e = energy_func(real_psi1, real_H, batch_size=b).data.numpy()\n",
    "# psi2_e = energy_func(real_psi2, real_H, batch_size=b).data.numpy()\n",
    "# e0_e = real_e0.data.numpy()\n",
    "# e1_e = real_e1.data.numpy()\n",
    "# e2_e = real_e2.data.numpy()\n",
    "# psi0_hat_e = energy_func(psi0_hat, real_H, batch_size=b).data.numpy()\n",
    "# rpsi_e = energy_func(rand_psis, real_H, batch_size=b).data.numpy()\n",
    "\n",
    "nn_e = np.mean(psi0_hat_e)\n",
    "mean_rpsi_e = np.mean(rpsi_e)\n",
    "\n",
    "mean_psi0_hat_e = np.mean(psi0_hat_e.ravel())\n",
    "mean_err_rand = np.mean(rpsi_e.ravel())\n",
    "\n",
    "mpe_nn = mean_err_nn / np.mean(np.abs(e0_e)) * 100\n",
    "mpe_rand = mean_err_rand / np.mean(np.abs(e0_e)) * 100\n",
    "\n",
    "print(\"psi0_e: {:.4f} / e0_e: {:.4f}\".format(np.mean(psi0_e), np.mean(e0_e)))\n",
    "print(\"psi1_e: {:.4f} / e1_e: {:.4f}\".format(np.mean(psi1_e), np.mean(e1_e)))\n",
    "print(\"psi2_e: {:.4f} / e2_e: {:.4f}\".format(np.mean(psi2_e), np.mean(e2_e)))\n",
    "print(\"hat psi0_e: {:.4f}\".format(mean_psi0_hat_e))\n",
    "print(\"rand psi_e: {:.4f}\\n\".format(mean_err_rand))\n",
    "\n",
    "err_nn = np.mean(np.abs(e0_e.ravel() - psi0_hat_e.ravel()))\n",
    "err_rand = np.mean(np.abs(e0_e.ravel() - rpsi_e.ravel()))\n",
    "\n",
    "print(\"mean err nn  : {:.4f}\".format( err_nn) )\n",
    "print(\"mean err rand: {:.4f}\\n\".format( err_rand))\n",
    "\n",
    "print(\"mean err nn  : {:.4f}%\".format(mpe_nn))\n",
    "print(\"mean err rand: {:.4f}%\".format(mpe_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVcAAAIRCAYAAAD+2P70AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d9221d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "energies = [mean_psi0_hat_e, np.mean(e2_e), np.mean(e1_e), np.mean(e0_e)] #+ [mean_err_rand]\n",
    "labels = ['$E_{\\psi_{NN}}$', '$E_2$', '$E_1$', '$E_0$'] #+ ['$E_{rand}$', ]\n",
    "energies = np.vstack([energies]*2).T\n",
    "\n",
    "f3 = plt.figure(figsize=[3,9])\n",
    "for i, e in enumerate(energies):\n",
    "    plt.plot(e, linewidth=6.0)\n",
    "plt.title(\"Average energy\", fontsize=16)\n",
    "plt.legend(labels, ncol=1, fontsize=16, loc=(1.1,.33)) #, loc='upper right'\n",
    "plt.gca().axes.get_xaxis().set_visible(False)\n",
    "plt.setp(plt.gca().axes.get_yticklabels(), fontsize=16)\n",
    "plt.show() ; f3.savefig('./figures/spectra.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the distribution of percent error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ab28940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpes = np.abs(e0_e.ravel() - psi0_hat_e.ravel()) / np.mean(np.abs(e0_e)) * 100\n",
    "plt.hist(mpes, bins=40)\n",
    "plt.ylabel('Frequency') ; plt.xlabel('Percent error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the model sometimes choose second eigenvalues?\n",
    "\n",
    "Running this cell a couple (5-10 times) should reveal a prediction or two which is more similar to the first excited state than to the ground state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.43025119230151176, 0.54367779567837715]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAADjCAYAAABaQ2RrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmJRIkqROMCmRJEmd8P8B2joKQyyk5noAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a727cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b=1\n",
    "\n",
    "np_H, np_e0, np_psi0, np_e1, np_psi1, _, _ = next_batch(D_side, batch_size=b, just_ground=False)\n",
    "real_H = Variable(torch.Tensor(np_H))\n",
    "real_e0 = Variable(torch.Tensor(np_e0))\n",
    "real_psi0 = Variable(torch.Tensor(np_psi0))\n",
    "real_e1 = Variable(torch.Tensor(np_e1))\n",
    "real_psi1 = Variable(torch.Tensor(np_psi1))\n",
    "psi0_hat = model(real_H, batch_size=b) \n",
    "\n",
    "diff = psi0_hat - real_psi0\n",
    "psi1_hat = dot_norm(diff)\n",
    "\n",
    "np_real_psi0 = real_psi0.data.numpy()\n",
    "np_real_psi1 = real_psi1.data.numpy()\n",
    "np_psi1_hat = psi1_hat.data.numpy()\n",
    "\n",
    "sd, mn = np.std(np_real_psi1), np.mean(np_real_psi1)\n",
    "cl = [mn-2*sd,mn+2*sd]\n",
    "use_cl = True\n",
    "if use_cl: print(cl)\n",
    "\n",
    "f3 = plt.figure(figsize=[8,4])\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.imshow(np_real_psi0) ; plt.title('exact $\\psi_0$')\n",
    "if use_cl: plt.clim(*cl)\n",
    "plt.gca().axes.get_xaxis().set_visible(False)\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.subplot(312)\n",
    "im = plt.imshow(np_real_psi1) ; plt.title('exact $\\psi_1$')\n",
    "if use_cl: plt.clim(*cl)\n",
    "plt.gca().axes.get_xaxis().set_visible(False)\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.subplot(313)\n",
    "im = plt.imshow(np_psi1_hat) ; plt.title('estimated $\\psi_0$')\n",
    "if use_cl: plt.clim(*cl)\n",
    "plt.gca().axes.get_xaxis().set_visible(False)\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "\n",
    "cax = f3.add_axes([0.95, 0.15, 0.02, 0.7])\n",
    "cb = f3.colorbar(im, cax=cax, orientation='vertical')\n",
    "cb.set_label('color scale')\n",
    "\n",
    "plt.show() ; f3.savefig('./figures/psi2-compare.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
