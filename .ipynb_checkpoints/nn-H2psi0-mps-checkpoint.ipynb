{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to find $\\psi_0$ by predicting MPS coefficients\n",
    "\n",
    "Sam Greydanus. March 2017. MIT License.\n",
    "\n",
    "Brief explanation and live demo of autoencoder [here](https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html). Wikipedia article [here](https://en.wikipedia.org/wiki/Autoencoder).\n",
    "\n",
    "<img src=\"static/autoencoder.png\" alt=\"Finite DMRG base case\" style=\"width: 40%;\"/>\n",
    "\n",
    "I modify the decoder part. Instead of being a neural network, I use the definition of reconstructing a state using MPS coefficients\n",
    "\n",
    "**Note:** This is a PyTorch implementation. I also wrote a TensorFlow implementation of this autoencoder; it's in the `_deprecated` folder. I like PyTorch; it's easier to write and debug. That said, both frameworks work fine for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import glob, copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import kron, identity\n",
    "\n",
    "np.random.seed(seed=123) # for reproducibility\n",
    "ms = torch.manual_seed(123) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym = lambda M: M + M.T\n",
    "local_op = lambda d: sym(np.random.randn(d,d))\n",
    "    \n",
    "class FreeSite():\n",
    "    def __init__(self, J=None, Jz=None, rand=False):\n",
    "        self.length = 1 # length\n",
    "        self.ops = ops = {}\n",
    "        self.J = J ; self.Jz = Jz\n",
    "        \n",
    "        # build operator dictionary\n",
    "        ops[\"H\"] = np.zeros((2,2)) # local Hamiltonian np.random.randn(2,2)\n",
    "        ops[\"Sz\"] = np.array([[0.5, 0], [0, -0.5]]) # z spin (S^z) operator\n",
    "        ops[\"Sp\"] = np.array([[0.0, 1.0], [0.0, 0.0]]) # raising (S^+) operator\n",
    "        \n",
    "        if rand:\n",
    "            ops[\"Sz\"] = np.random.randn()*np.array([[0, 0.5], [0.5, 0.]]) + np.random.randn()*ops[\"Sz\"]\n",
    "            ops[\"Sp\"] *= np.random.randn()\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return list(self.ops.values())[0].shape[0] # all ops should have same dimensionality\n",
    "        \n",
    "    def enlarge(self, site):\n",
    "        '''Enlarge block by a single site'''\n",
    "        \n",
    "        D1, H1, Sz1, Sp1 = self.get_dim(), self.ops['H'], self.ops['Sz'], self.ops['Sp'] # this block\n",
    "        D2, H2, Sz2, Sp2 = site.get_dim(), site.ops['H'], site.ops['Sz'], site.ops['Sp'] # another block (ie free site)\n",
    "\n",
    "        enlarged = copy.deepcopy(self)\n",
    "        enlarged.length += site.length\n",
    "        ops = enlarged.ops\n",
    "\n",
    "        ops['H'] = kron(H1, identity(D2)) + kron(identity(D1), H2) + self.interaction_H(site)\n",
    "        ops['Sz'] = kron(identity(D1), Sz2)\n",
    "        ops['Sp'] = kron(identity(D1), Sp2)\n",
    "\n",
    "        return enlarged\n",
    "    \n",
    "    def interaction_H(self, site):\n",
    "        '''Given another block, returns two-site term in the \n",
    "        Hamiltonain that joins the two sites.'''\n",
    "        Sz1, Sp1 = self.ops[\"Sz\"], self.ops[\"Sp\"] # this block\n",
    "        Sz2, Sp2 = site.ops[\"Sz\"], site.ops[\"Sp\"] # another block\n",
    "        \n",
    "        J = 1.*np.random.randn() if self.J is None else self.J\n",
    "        Jz = 1.*np.random.randn() if self.Jz is None else self.Jz\n",
    "        \n",
    "        join_Sp = (J/2)*(kron(Sp1, Sp2.conjugate().transpose()) + kron(Sp1.conjugate().transpose(), Sp2))\n",
    "        join_Sz = Jz*kron(Sz1, Sz2)\n",
    "        return (join_Sp + join_Sz)\n",
    "    \n",
    "    def rotate_ops(self, transformation_matrix):\n",
    "        # rotate and truncate each operator.\n",
    "        new_ops = {}\n",
    "        for name, op in self.ops.items():\n",
    "            new_ops[name] = self.rotate_and_truncate(op, transformation_matrix)\n",
    "        self.ops = new_ops\n",
    "    \n",
    "    @staticmethod\n",
    "    def rotate_and_truncate(S, O):\n",
    "        '''Transforms the operator to a new (possibly truncated) basis'''\n",
    "        return O.conjugate().transpose().dot(S.dot(O)) # eqn 7 in arXiv:cond-mat/0603842v2\n",
    "    \n",
    "def ham(J, Jz, rand=False):\n",
    "    sys = FreeSite(J=J, Jz=Jz, rand=rand)\n",
    "    for _ in range(chi-1):\n",
    "        fs = FreeSite(J=J, Jz=Jz, rand=rand)\n",
    "        sys = sys.enlarge(fs)\n",
    "    return sys.ops['H'].todense()\n",
    "\n",
    "def next_batch(D, batch_size):\n",
    "    H_list = [] ; e0_list = [] ; psi0_list = []\n",
    "    for _ in range(batch_size):\n",
    "        H = ham(None, None, rand=True)\n",
    "        e0, psi0 = eigsh(H,k=1, which=\"SA\")\n",
    "        H_list.append(np.asarray(H).ravel()) ; e0_list.append(e0.ravel()) ; psi0_list.append(psi0.ravel())\n",
    "    return np.vstack(H_list), np.vstack(e0_list), np.vstack(psi0_list)\n",
    "\n",
    "def rand_psi(D_side):\n",
    "    psi = np.random.rand(1, D_side)\n",
    "    psi /= np.sqrt(np.dot(psi, psi.T))\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "Simple functions that might be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def baseN(num, b, numerals=\"0123456789abcdefghijklmnopqrstuvwxyz\"):\n",
    "    return ((num == 0) and numerals[0]) or (baseN(num // b, b, numerals).lstrip(numerals[0]) + numerals[num % b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 'decode' portion\n",
    "These functions are written for PyTorch tensors. PyTorch uses lazy execution, meaning that it builds an optimized computational graph and only executes the operations in that graph after a request. See http://pytorch.org/ for a good series of introductory tutorials. Unlike TensorFlow, PyTorch supports dynamic graphs. This makes it more flexible and, in my opinion, easier to write and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mps2state(A_list):\n",
    "    '''given a list of mps coefficients, returns the state of the full system.\n",
    "    The input A_list is indexed according to A_list[{local state}][{local site}].'''\n",
    "    d = len(A_list) # number of possible states\n",
    "    chi = len(A_list[0]) # number of possible sites\n",
    "    c = [] # the state\n",
    "    for sys_state in range(d**chi): # loop over \n",
    "        ix = baseN(sys_state, d).zfill(len(A_list[0]))\n",
    "        for site in range(chi):\n",
    "            site_state = A_list[int(ix[site])][site]\n",
    "            prod = site_state if site is 0 else torch.mm(prod, site_state) # matrix multiplication (contract tensors)\n",
    "        c.append(torch.trace(prod))\n",
    "    return torch.stack(c)\n",
    "\n",
    "def coeff2mps(coeff, d, chi):\n",
    "    '''given a vector of coefficients of length chi*d^3, reshapes them into an A_list\n",
    "    that can be passed to mps2state function above'''\n",
    "    splits = torch.split(coeff,d*d,dim=1)\n",
    "    dxd_splits = [c.resize(d,d) for c in splits]\n",
    "    A_list = [dxd_splits[d_i*chi:(1+d_i)*chi] for d_i in range(d)]\n",
    "    return A_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 'encode' portion\n",
    "We'll use a 2-layer neural network with 1024 hidden units and relu activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "lr = 3e-4 # learning rate\n",
    "batch_size = 1 # batch size for minibatch gradient descent\n",
    "save_dir = 'H2psi0-mps-models'\n",
    "checkpoint_steps = [1, 10,30,100,300,1000,3000,10000,30000,100000,300000]\n",
    "checkpoint_steps += [250, 500, 1000, 2000, 4000, 16000, 64000, 128000]\n",
    "total_steps = max(checkpoint_steps + [10000])\n",
    "print_every = 250\n",
    "global_step = 0 # keeps track of current step of gradient descent\n",
    "d = 2 # number of quantum states available to each site\n",
    "chi = 4 # number of sites (ie electrons)\n",
    "\n",
    "D_side = d**chi # autoencoder input dimensionality\n",
    "D_hidden = 512 # dimensionality of hidden layer(s)\n",
    "D_mps = d*chi*d*d # dimensionality of MPS 'coeff' vector\n",
    "cost_func = nn.PairwiseDistance() # square and sum to get L2 loss (see train loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a neural network with three hidden layers\n",
    "class SimpleNN3(torch.nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, h_dim, output_dim):\n",
    "        super(SimpleNN3, self).__init__()\n",
    "        self.W1 = nn.Parameter(torch.randn(input_dim, h_dim)*0.075)\n",
    "        self.b1 = nn.Parameter(torch.randn(h_dim)*0.075)\n",
    "        self.W2 = nn.Parameter(torch.randn(h_dim, h_dim)*0.075)\n",
    "        self.b2 = nn.Parameter(torch.randn(h_dim)*0.075)\n",
    "        self.W3 = nn.Parameter(torch.randn(h_dim, output_dim)*0.075)\n",
    "        self.b3 = nn.Parameter(torch.randn(output_dim)*0.075)\n",
    "        self.arch = 'SimpleNN3'\n",
    "\n",
    "    def forward(self, X, batch_size):\n",
    "        h1 = F.relu(X.mm(self.W1) + self.b1.repeat(X.size(0), 1))\n",
    "        h2 = F.relu(h1.mm(self.W2) + self.b2.repeat(X.size(0), 1))\n",
    "        h3 = h2.mm(self.W3) + self.b3.repeat(X.size(0), 1)\n",
    "#         h3 = F.tanh(h3)\n",
    "        return h3\n",
    "    \n",
    "model = SimpleNN3(batch_size=1, input_dim=D_side**2, h_dim=D_hidden, output_dim=D_mps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full Autoencoder forward pass\n",
    "First encode with the neural network, then decode using the definition of MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_norm(u):\n",
    "    m, n = u.size()\n",
    "    d = u.mm(u.t())\n",
    "    div_by = torch.sqrt(torch.diag(d))\n",
    "    v = u / div_by.resize(m, 1).repeat(1,n)\n",
    "    return v\n",
    "\n",
    "def forward(X, model):\n",
    "    '''defines one forward pass of the neural network encoder. The input is the\n",
    "    full-dimension state vector of the system and the output is an attempt at\n",
    "    reconstructing the input vector after compressing it as a set of MPS coefficients'''\n",
    "    coeff = model(X, batch_size)\n",
    "    \n",
    "    A_list = coeff2mps(coeff, d, chi) # repackage output vector as a list of A matrices\n",
    "    psi_hat = mps2state(A_list) # reconstruct the original state vector from these A matrices by definition of MPS\n",
    "    psi_hat = dot_norm(psi_hat.t()) ; psi_hat = psi_hat.t()\n",
    "    return psi_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "In order to train the model, we need data. In this case, I'll just choose each example state vector to be a random d^chi vector such that the dot product of the vector with its conjugate transpose is normalized (a necessary property of a state vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saved model to load.\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "model_zoo = []\n",
    "paths = glob.glob(save_dir + '/*.tar')\n",
    "try:\n",
    "    for s in paths:\n",
    "        checkpoint = torch.load(s)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        global_step = checkpoint['global_step']\n",
    "        model_zoo.append((global_step, copy.deepcopy(model)))\n",
    "        print(\"loaded model: {}\".format(s))\n",
    "    model_zoo = sorted(model_zoo, key=lambda tup: tup[0])\n",
    "    global_step, model = model_zoo[-1]\n",
    "except:\n",
    "    print(\"no saved model to load.\") ; model_zoo = []\n",
    "    load_was_success = False\n",
    "else:\n",
    "    if len(paths) is 0: print(\"no saved model to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Display the loss every so often. It should be decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def energy_func(psi, H, batch_size):\n",
    "    psi = psi.resize(batch_size,1,D_side)\n",
    "    H = H.resize(batch_size,D_side,D_side)\n",
    "    e0_hat = psi.bmm(H).bmm(torch.transpose(psi, 2,1))\n",
    "    return e0_hat\n",
    "\n",
    "def cost_func(e, psi, H, batch_size):\n",
    "    e = e.resize(batch_size,1,1)\n",
    "    e0_hat = energy_func(psi, H, batch_size)\n",
    "    return .5*torch.sum((e0_hat-e)**2) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss: 2.5588\n",
      "\tstep 1: saved model\n",
      "\tstep 10: saved model\n",
      "\tstep 30: saved model\n",
      "\tstep 100: saved model\n",
      "step 250: loss: 0.8304\n",
      "\tstep 250: saved model\n",
      "\tstep 300: saved model\n",
      "step 500: loss: 0.4575\n",
      "\tstep 500: saved model\n",
      "step 750: loss: 0.3316\n",
      "step 1000: loss: 0.1986\n",
      "\tstep 1000: saved model\n",
      "step 1250: loss: 0.1378\n",
      "step 1500: loss: 0.1432\n",
      "step 1750: loss: 0.1515\n",
      "step 2000: loss: 0.1244\n",
      "\tstep 2000: saved model\n",
      "step 2250: loss: 0.0851\n",
      "step 2500: loss: 0.0911\n",
      "step 2750: loss: 0.1140\n",
      "step 3000: loss: 0.0981\n",
      "\tstep 3000: saved model\n",
      "step 3250: loss: 0.0758\n",
      "step 3500: loss: 0.1273\n",
      "step 3750: loss: 0.1270\n",
      "step 4000: loss: 0.2280\n",
      "\tstep 4000: saved model\n",
      "step 4250: loss: 0.1711\n",
      "step 4500: loss: 0.1264\n",
      "step 4750: loss: 0.1114\n",
      "step 5000: loss: 0.1374\n",
      "step 5250: loss: 0.0714\n",
      "step 5500: loss: 0.0510\n",
      "step 5750: loss: 0.0780\n",
      "step 6000: loss: 0.0680\n",
      "step 6250: loss: 0.0898\n",
      "step 6500: loss: 0.0697\n",
      "step 6750: loss: 0.0567\n",
      "step 7000: loss: 0.0722\n",
      "step 7250: loss: 0.0471\n",
      "step 7500: loss: 0.0929\n",
      "step 7750: loss: 0.0634\n",
      "step 8000: loss: 0.0597\n",
      "step 8250: loss: 0.0528\n",
      "step 8500: loss: 0.0771\n",
      "step 8750: loss: 0.0458\n",
      "step 9000: loss: 0.1147\n",
      "step 9250: loss: 0.1183\n",
      "step 9500: loss: 0.1041\n",
      "step 9750: loss: 0.1086\n",
      "step 10000: loss: 0.0906\n",
      "\tstep 10000: saved model\n",
      "step 10250: loss: 0.1189\n",
      "step 10500: loss: 0.0707\n",
      "step 10750: loss: 0.0569\n",
      "step 11000: loss: 0.0653\n",
      "step 11250: loss: 0.0536\n",
      "step 11500: loss: 0.0753\n",
      "step 11750: loss: 0.0508\n",
      "step 12000: loss: 0.0495\n",
      "step 12250: loss: 0.0847\n",
      "step 12500: loss: 0.0600\n",
      "step 12750: loss: 0.0346\n",
      "step 13000: loss: 0.0932\n",
      "step 13250: loss: 0.0692\n",
      "step 13500: loss: 0.0585\n",
      "step 13750: loss: 0.0814\n",
      "step 14000: loss: 0.0604\n",
      "step 14250: loss: 0.0512\n",
      "step 14500: loss: 0.0495\n",
      "step 14750: loss: 0.0524\n",
      "step 15000: loss: 0.0482\n",
      "step 15250: loss: 0.0323\n",
      "step 15500: loss: 0.0429\n",
      "step 15750: loss: 0.0285\n",
      "step 16000: loss: 0.0408\n",
      "\tstep 16000: saved model\n",
      "step 16250: loss: 0.0375\n",
      "step 16500: loss: 0.0647\n",
      "step 16750: loss: 0.0332\n",
      "step 17000: loss: 0.0465\n",
      "step 17250: loss: 0.0774\n",
      "step 17500: loss: 0.0538\n",
      "step 17750: loss: 0.0558\n",
      "step 18000: loss: 0.0359\n",
      "step 18250: loss: 0.0448\n",
      "step 18500: loss: 0.0403\n",
      "step 18750: loss: 0.0395\n",
      "step 19000: loss: 0.0320\n",
      "step 19250: loss: 0.0323\n",
      "step 19500: loss: 0.1282\n",
      "step 19750: loss: 0.0442\n",
      "step 20000: loss: 0.0577\n",
      "step 20250: loss: 0.0514\n",
      "step 20500: loss: 0.0340\n",
      "step 20750: loss: 0.0337\n",
      "step 21000: loss: 0.0366\n",
      "step 21250: loss: 0.0409\n",
      "step 21500: loss: 0.0287\n",
      "step 21750: loss: 0.0615\n",
      "step 22000: loss: 0.0328\n",
      "step 22250: loss: 0.0388\n",
      "step 22500: loss: 0.0325\n",
      "step 22750: loss: 0.0323\n",
      "step 23000: loss: 0.0252\n",
      "step 23250: loss: 0.0429\n",
      "step 23500: loss: 0.0411\n",
      "step 23750: loss: 0.0245\n",
      "step 24000: loss: 0.0369\n",
      "step 24250: loss: 0.0281\n",
      "step 24500: loss: 0.0273\n",
      "step 24750: loss: 0.0333\n",
      "step 25000: loss: 0.0337\n",
      "step 25250: loss: 0.0445\n",
      "step 25500: loss: 0.0377\n",
      "step 25750: loss: 0.0291\n",
      "step 26000: loss: 0.0362\n",
      "step 26250: loss: 0.0407\n",
      "step 26500: loss: 0.0314\n",
      "step 26750: loss: 0.0339\n",
      "step 27000: loss: 0.0870\n",
      "step 27250: loss: 0.0309\n",
      "step 27500: loss: 0.0255\n",
      "step 27750: loss: 0.0233\n",
      "step 28000: loss: 0.0363\n",
      "step 28250: loss: 0.0272\n",
      "step 28500: loss: 0.0421\n",
      "step 28750: loss: 0.0387\n",
      "step 29000: loss: 0.0316\n",
      "step 29250: loss: 0.0287\n",
      "step 29500: loss: 0.0293\n",
      "step 29750: loss: 0.0374\n",
      "step 30000: loss: 0.0318\n",
      "\tstep 30000: saved model\n",
      "step 30250: loss: 0.0316\n",
      "step 30500: loss: 0.0281\n",
      "step 30750: loss: 0.0190\n",
      "step 31000: loss: 0.0183\n",
      "step 31250: loss: 0.0215\n",
      "step 31500: loss: 0.0232\n",
      "step 31750: loss: 0.0224\n",
      "step 32000: loss: 0.0215\n",
      "step 32250: loss: 0.0192\n",
      "step 32500: loss: 0.0217\n",
      "step 32750: loss: 0.0232\n",
      "step 33000: loss: 0.0468\n",
      "step 33250: loss: 0.0213\n",
      "step 33500: loss: 0.0572\n",
      "step 33750: loss: 0.0343\n",
      "step 34000: loss: 0.0256\n",
      "step 34250: loss: 0.0196\n",
      "step 34500: loss: 0.0186\n",
      "step 34750: loss: 0.0322\n",
      "step 35000: loss: 0.0301\n",
      "step 35250: loss: 0.0177\n",
      "step 35500: loss: 0.0289\n",
      "step 35750: loss: 0.0174\n",
      "step 36000: loss: 0.0288\n",
      "step 36250: loss: 0.0449\n",
      "step 36500: loss: 0.0238\n",
      "step 36750: loss: 0.0328\n",
      "step 37000: loss: 0.0288\n",
      "step 37250: loss: 0.0309\n",
      "step 37500: loss: 0.0201\n",
      "step 37750: loss: 0.0172\n",
      "step 38000: loss: 0.0172\n",
      "step 38250: loss: 0.0227\n",
      "step 38500: loss: 0.0303\n",
      "step 38750: loss: 0.0309\n",
      "step 39000: loss: 0.0162\n",
      "step 39250: loss: 0.0150\n",
      "step 39500: loss: 0.0154\n",
      "step 39750: loss: 0.0266\n",
      "step 40000: loss: 0.0231\n",
      "step 40250: loss: 0.0164\n",
      "step 40500: loss: 0.0202\n",
      "step 40750: loss: 0.0165\n",
      "step 41000: loss: 0.0157\n",
      "step 41250: loss: 0.0254\n",
      "step 41500: loss: 0.0238\n",
      "step 41750: loss: 0.0146\n",
      "step 42000: loss: 0.0193\n",
      "step 42250: loss: 0.0231\n",
      "step 42500: loss: 0.0352\n",
      "step 42750: loss: 0.0230\n",
      "step 43000: loss: 0.0184\n",
      "step 43250: loss: 0.0212\n",
      "step 43500: loss: 0.0289\n",
      "step 43750: loss: 0.0204\n",
      "step 44000: loss: 0.0199\n",
      "step 44250: loss: 0.0219\n",
      "step 44500: loss: 0.0282\n",
      "step 44750: loss: 0.0257\n",
      "step 45000: loss: 0.0212\n",
      "step 45250: loss: 0.0175\n",
      "step 45500: loss: 0.0233\n",
      "step 45750: loss: 0.0197\n",
      "step 46000: loss: 0.0120\n",
      "step 46250: loss: 0.0124\n",
      "step 46500: loss: 0.0271\n",
      "step 46750: loss: 0.0144\n",
      "step 47000: loss: 0.0145\n",
      "step 47250: loss: 0.0175\n",
      "step 47500: loss: 0.0218\n",
      "step 47750: loss: 0.0233\n",
      "step 48000: loss: 0.0202\n",
      "step 48250: loss: 0.0238\n",
      "step 48500: loss: 0.0243\n",
      "step 48750: loss: 0.0265\n",
      "step 49000: loss: 0.0183\n",
      "step 49250: loss: 0.0116\n",
      "step 49500: loss: 0.0186\n",
      "step 49750: loss: 0.0099\n",
      "step 50000: loss: 0.0128\n",
      "step 50250: loss: 0.0397\n",
      "step 50500: loss: 0.0172\n",
      "step 50750: loss: 0.0156\n",
      "step 51000: loss: 0.0102\n",
      "step 51250: loss: 0.0120\n",
      "step 51500: loss: 0.0152\n",
      "step 51750: loss: 0.0130\n",
      "step 52000: loss: 0.0141\n",
      "step 52250: loss: 0.0165\n",
      "step 52500: loss: 0.0157\n",
      "step 52750: loss: 0.0128\n",
      "step 53000: loss: 0.0291\n",
      "step 53250: loss: 0.0112\n",
      "step 53500: loss: 0.0160\n",
      "step 53750: loss: 0.0176\n",
      "step 54000: loss: 0.0163\n",
      "step 54250: loss: 0.0136\n",
      "step 54500: loss: 0.0131\n",
      "step 54750: loss: 0.0172\n",
      "step 55000: loss: 0.0188\n",
      "step 55250: loss: 0.0135\n",
      "step 55500: loss: 0.0222\n",
      "step 55750: loss: 0.0164\n",
      "step 56000: loss: 0.0101\n",
      "step 56250: loss: 0.0152\n",
      "step 56500: loss: 0.0126\n",
      "step 56750: loss: 0.0136\n",
      "step 57000: loss: 0.0122\n",
      "step 57250: loss: 0.0116\n",
      "step 57500: loss: 0.0118\n",
      "step 57750: loss: 0.0268\n",
      "step 58000: loss: 0.0111\n",
      "step 58250: loss: 0.0159\n",
      "step 58500: loss: 0.0170\n",
      "step 58750: loss: 0.0217\n",
      "step 59000: loss: 0.0122\n",
      "step 59250: loss: 0.0409\n",
      "step 59500: loss: 0.0236\n",
      "step 59750: loss: 0.0162\n",
      "step 60000: loss: 0.0195\n",
      "step 60250: loss: 0.0202\n",
      "step 60500: loss: 0.0160\n",
      "step 60750: loss: 0.0158\n",
      "step 61000: loss: 0.0201\n",
      "step 61250: loss: 0.0124\n",
      "step 61500: loss: 0.0132\n",
      "step 61750: loss: 0.0168\n",
      "step 62000: loss: 0.0152\n",
      "step 62250: loss: 0.0191\n",
      "step 62500: loss: 0.0107\n",
      "step 62750: loss: 0.0114\n",
      "step 63000: loss: 0.0183\n",
      "step 63250: loss: 0.0103\n",
      "step 63500: loss: 0.0141\n",
      "step 63750: loss: 0.0145\n",
      "step 64000: loss: 0.0149\n",
      "\tstep 64000: saved model\n",
      "step 64250: loss: 0.0194\n",
      "step 64500: loss: 0.0120\n",
      "step 64750: loss: 0.0140\n",
      "step 65000: loss: 0.0114\n",
      "step 65250: loss: 0.0122\n",
      "step 65500: loss: 0.0191\n",
      "step 65750: loss: 0.0115\n",
      "step 66000: loss: 0.0118\n",
      "step 66250: loss: 0.0081\n",
      "step 66500: loss: 0.0085\n",
      "step 66750: loss: 0.0178\n",
      "step 67000: loss: 0.0123\n",
      "step 67250: loss: 0.0095\n",
      "step 67500: loss: 0.0063\n",
      "step 67750: loss: 0.0211\n",
      "step 68000: loss: 0.0099\n",
      "step 68250: loss: 0.0197\n",
      "step 68500: loss: 0.0140\n",
      "step 68750: loss: 0.0178\n",
      "step 69000: loss: 0.0106\n",
      "step 69250: loss: 0.0118\n",
      "step 69500: loss: 0.0203\n",
      "step 69750: loss: 0.0178\n",
      "step 70000: loss: 0.0131\n",
      "step 70250: loss: 0.0124\n",
      "step 70500: loss: 0.0183\n",
      "step 70750: loss: 0.0123\n",
      "step 71000: loss: 0.0098\n",
      "step 71250: loss: 0.0191\n",
      "step 71500: loss: 0.1036\n",
      "step 71750: loss: 0.0243\n",
      "step 72000: loss: 0.0147\n",
      "step 72250: loss: 0.0130\n",
      "step 72500: loss: 0.0122\n",
      "step 72750: loss: 0.0154\n",
      "step 73000: loss: 0.0107\n",
      "step 73250: loss: 0.0239\n",
      "step 73500: loss: 0.0153\n",
      "step 73750: loss: 0.0137\n",
      "step 74000: loss: 0.0142\n",
      "step 74250: loss: 0.0137\n",
      "step 74500: loss: 0.0148\n",
      "step 74750: loss: 0.0109\n",
      "step 75000: loss: 0.0105\n",
      "step 75250: loss: 0.0111\n",
      "step 75500: loss: 0.0092\n",
      "step 75750: loss: 0.0115\n",
      "step 76000: loss: 0.0209\n",
      "step 76250: loss: 0.0253\n",
      "step 76500: loss: 0.0155\n",
      "step 76750: loss: 0.0193\n",
      "step 77000: loss: 0.0109\n",
      "step 77250: loss: 0.0135\n",
      "step 77500: loss: 0.0211\n",
      "step 77750: loss: 0.0153\n",
      "step 78000: loss: 0.0177\n",
      "step 78250: loss: 0.0202\n",
      "step 78500: loss: 0.0104\n",
      "step 78750: loss: 0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 79000: loss: 0.0139\n",
      "step 79250: loss: 0.0138\n",
      "step 79500: loss: 0.0115\n",
      "step 79750: loss: 0.0097\n",
      "step 80000: loss: 0.0100\n",
      "step 80250: loss: 0.0081\n",
      "step 80500: loss: 0.0110\n",
      "step 80750: loss: 0.0067\n",
      "step 81000: loss: 0.0093\n",
      "step 81250: loss: 0.0109\n",
      "step 81500: loss: 0.0147\n",
      "step 81750: loss: 0.0176\n",
      "step 82000: loss: 0.0083\n",
      "step 82250: loss: 0.0110\n",
      "step 82500: loss: 0.0072\n",
      "step 82750: loss: 0.0071\n",
      "step 83000: loss: 0.0130\n",
      "step 83250: loss: 0.0150\n",
      "step 83500: loss: 0.0118\n",
      "step 83750: loss: 0.0089\n",
      "step 84000: loss: 0.0152\n",
      "step 84250: loss: 0.0131\n",
      "step 84500: loss: 0.0119\n",
      "step 84750: loss: 0.0207\n",
      "step 85000: loss: 0.0145\n",
      "step 85250: loss: 0.0077\n",
      "step 85500: loss: 0.0070\n",
      "step 85750: loss: 0.0144\n",
      "step 86000: loss: 0.0082\n",
      "step 86250: loss: 0.0070\n",
      "step 86500: loss: 0.0087\n",
      "step 86750: loss: 0.0159\n",
      "step 87000: loss: 0.0106\n",
      "step 87250: loss: 0.0080\n",
      "step 87500: loss: 0.0093\n",
      "step 87750: loss: 0.0060\n",
      "step 88000: loss: 0.0072\n",
      "step 88250: loss: 0.0109\n",
      "step 88500: loss: 0.0091\n",
      "step 88750: loss: 0.0147\n",
      "step 89000: loss: 0.0168\n",
      "step 89250: loss: 0.0127\n",
      "step 89500: loss: 0.0065\n",
      "step 89750: loss: 0.0135\n",
      "step 90000: loss: 0.0127\n",
      "step 90250: loss: 0.0185\n",
      "step 90500: loss: 0.0091\n",
      "step 90750: loss: 0.0207\n",
      "step 91000: loss: 0.0093\n",
      "step 91250: loss: 0.0097\n",
      "step 91500: loss: 0.0085\n",
      "step 91750: loss: 0.0089\n",
      "step 92000: loss: 0.0154\n",
      "step 92250: loss: 0.0122\n",
      "step 92500: loss: 0.0140\n",
      "step 92750: loss: 0.0069\n",
      "step 93000: loss: 0.0136\n",
      "step 93250: loss: 0.0098\n",
      "step 93500: loss: 0.0117\n",
      "step 93750: loss: 0.0110\n",
      "step 94000: loss: 0.0143\n",
      "step 94250: loss: 0.0085\n",
      "step 94500: loss: 0.0137\n",
      "step 94750: loss: 0.0110\n",
      "step 95000: loss: 0.0096\n",
      "step 95250: loss: 0.0137\n",
      "step 95500: loss: 0.0122\n",
      "step 95750: loss: 0.0211\n",
      "step 96000: loss: 0.0103\n",
      "step 96250: loss: 0.0081\n",
      "step 96500: loss: 0.0112\n",
      "step 96750: loss: 0.0072\n",
      "step 97000: loss: 0.0080\n",
      "step 97250: loss: 0.0104\n",
      "step 97500: loss: 0.0087\n",
      "step 97750: loss: 0.0108\n",
      "step 98000: loss: 0.0080\n",
      "step 98250: loss: 0.0066\n",
      "step 98500: loss: 0.0130\n",
      "step 98750: loss: 0.0150\n",
      "step 99000: loss: 0.0118\n",
      "step 99250: loss: 0.0129\n",
      "step 99500: loss: 0.0094\n",
      "step 99750: loss: 0.0337\n",
      "step 100000: loss: 0.0124\n",
      "\tstep 100000: saved model\n",
      "step 100250: loss: 0.0111\n",
      "step 100500: loss: 0.0079\n",
      "step 100750: loss: 0.0078\n",
      "step 101000: loss: 0.0120\n",
      "step 101250: loss: 0.0099\n",
      "step 101500: loss: 0.0090\n",
      "step 101750: loss: 0.0061\n",
      "step 102000: loss: 0.0089\n",
      "step 102250: loss: 0.0105\n",
      "step 102500: loss: 0.0099\n",
      "step 102750: loss: 0.0074\n",
      "step 103000: loss: 0.0076\n",
      "step 103250: loss: 0.0062\n",
      "step 103500: loss: 0.0080\n",
      "step 103750: loss: 0.0149\n",
      "step 104000: loss: 0.0136\n",
      "step 104250: loss: 0.0081\n",
      "step 104500: loss: 0.0106\n",
      "step 104750: loss: 0.0143\n",
      "step 105000: loss: 0.0083\n",
      "step 105250: loss: 0.0083\n",
      "step 105500: loss: 0.0170\n",
      "step 105750: loss: 0.0114\n",
      "step 106000: loss: 0.0121\n",
      "step 106250: loss: 0.0127\n",
      "step 106500: loss: 0.0090\n",
      "step 106750: loss: 0.0072\n",
      "step 107000: loss: 0.0087\n",
      "step 107250: loss: 0.0060\n",
      "step 107500: loss: 0.0083\n",
      "step 107750: loss: 0.0105\n",
      "step 108000: loss: 0.0099\n",
      "step 108250: loss: 0.0167\n",
      "step 108500: loss: 0.0135\n",
      "step 108750: loss: 0.0182\n",
      "step 109000: loss: 0.0122\n",
      "step 109250: loss: 0.0151\n",
      "step 109500: loss: 0.0109\n",
      "step 109750: loss: 0.0085\n",
      "step 110000: loss: 0.0112\n",
      "step 110250: loss: 0.0109\n",
      "step 110500: loss: 0.0141\n",
      "step 110750: loss: 0.0106\n",
      "step 111000: loss: 0.0096\n",
      "step 111250: loss: 0.0060\n",
      "step 111500: loss: 0.0062\n",
      "step 111750: loss: 0.0054\n",
      "step 112000: loss: 0.0034\n",
      "step 112250: loss: 0.0058\n",
      "step 112500: loss: 0.0079\n",
      "step 112750: loss: 0.0115\n",
      "step 113000: loss: 0.0101\n",
      "step 113250: loss: 0.0066\n",
      "step 113500: loss: 0.0071\n",
      "step 113750: loss: 0.0060\n",
      "step 114000: loss: 0.0053\n",
      "step 114250: loss: 0.0101\n",
      "step 114500: loss: 0.0064\n",
      "step 114750: loss: 0.0092\n",
      "step 115000: loss: 0.0071\n",
      "step 115250: loss: 0.0136\n",
      "step 115500: loss: 0.0066\n",
      "step 115750: loss: 0.0055\n",
      "step 116000: loss: 0.0077\n",
      "step 116250: loss: 0.0055\n",
      "step 116500: loss: 0.0075\n",
      "step 116750: loss: 0.0069\n",
      "step 117000: loss: 0.0075\n",
      "step 117250: loss: 0.0073\n",
      "step 117500: loss: 0.0083\n",
      "step 117750: loss: 0.0196\n",
      "step 118000: loss: 0.0125\n",
      "step 118250: loss: 0.0093\n",
      "step 118500: loss: 0.0089\n",
      "step 118750: loss: 0.0077\n",
      "step 119000: loss: 0.0125\n",
      "step 119250: loss: 0.0108\n",
      "step 119500: loss: 0.0111\n",
      "step 119750: loss: 0.0092\n",
      "step 120000: loss: 0.0070\n",
      "step 120250: loss: 0.0092\n",
      "step 120500: loss: 0.0077\n",
      "step 120750: loss: 0.0131\n",
      "step 121000: loss: 0.0076\n",
      "step 121250: loss: 0.0086\n",
      "step 121500: loss: 0.0158\n",
      "step 121750: loss: 0.0071\n",
      "step 122000: loss: 0.0099\n",
      "step 122250: loss: 0.0121\n",
      "step 122500: loss: 0.0135\n",
      "step 122750: loss: 0.0084\n",
      "step 123000: loss: 0.0062\n",
      "step 123250: loss: 0.0081\n",
      "step 123500: loss: 0.0105\n",
      "step 123750: loss: 0.0108\n",
      "step 124000: loss: 0.0063\n",
      "step 124250: loss: 0.0064\n",
      "step 124500: loss: 0.0081\n",
      "step 124750: loss: 0.0062\n",
      "step 125000: loss: 0.0080\n",
      "step 125250: loss: 0.0116\n",
      "step 125500: loss: 0.0142\n",
      "step 125750: loss: 0.0099\n",
      "step 126000: loss: 0.0080\n",
      "step 126250: loss: 0.0083\n",
      "step 126500: loss: 0.0134\n",
      "step 126750: loss: 0.0089\n",
      "step 127000: loss: 0.0067\n",
      "step 127250: loss: 0.0074\n",
      "step 127500: loss: 0.0086\n",
      "step 127750: loss: 0.0082\n",
      "step 128000: loss: 0.0061\n",
      "\tstep 128000: saved model\n",
      "step 128250: loss: 0.0105\n",
      "step 128500: loss: 0.0165\n",
      "step 128750: loss: 0.0075\n",
      "step 129000: loss: 0.0058\n",
      "step 129250: loss: 0.0058\n",
      "step 129500: loss: 0.0061\n",
      "step 129750: loss: 0.0068\n",
      "step 130000: loss: 0.0043\n",
      "step 130250: loss: 0.0072\n",
      "step 130500: loss: 0.0086\n",
      "step 130750: loss: 0.0066\n",
      "step 131000: loss: 0.0068\n",
      "step 131250: loss: 0.0091\n",
      "step 131500: loss: 0.0080\n",
      "step 131750: loss: 0.0109\n",
      "step 132000: loss: 0.0150\n",
      "step 132250: loss: 0.0068\n",
      "step 132500: loss: 0.0046\n",
      "step 132750: loss: 0.0102\n",
      "step 133000: loss: 0.0086\n",
      "step 133250: loss: 0.0073\n",
      "step 133500: loss: 0.0067\n",
      "step 133750: loss: 0.0106\n",
      "step 134000: loss: 0.0073\n",
      "step 134250: loss: 0.0063\n",
      "step 134500: loss: 0.0056\n",
      "step 134750: loss: 0.0069\n",
      "step 135000: loss: 0.0060\n",
      "step 135250: loss: 0.0139\n",
      "step 135500: loss: 0.0341\n",
      "step 135750: loss: 0.0119\n",
      "step 136000: loss: 0.0095\n",
      "step 136250: loss: 0.0059\n",
      "step 136500: loss: 0.0062\n",
      "step 136750: loss: 0.0074\n",
      "step 137000: loss: 0.0110\n",
      "step 137250: loss: 0.0090\n",
      "step 137500: loss: 0.0093\n",
      "step 137750: loss: 0.0054\n",
      "step 138000: loss: 0.0154\n",
      "step 138250: loss: 0.0059\n",
      "step 138500: loss: 0.0096\n",
      "step 138750: loss: 0.0070\n",
      "step 139000: loss: 0.0086\n",
      "step 139250: loss: 0.0069\n",
      "step 139500: loss: 0.0101\n",
      "step 139750: loss: 0.0060\n",
      "step 140000: loss: 0.0193\n",
      "step 140250: loss: 0.0067\n",
      "step 140500: loss: 0.0056\n",
      "step 140750: loss: 0.0071\n",
      "step 141000: loss: 0.0079\n",
      "step 141250: loss: 0.0060\n",
      "step 141500: loss: 0.0077\n",
      "step 141750: loss: 0.0061\n",
      "step 142000: loss: 0.0070\n",
      "step 142250: loss: 0.0051\n",
      "step 142500: loss: 0.0086\n",
      "step 142750: loss: 0.0060\n",
      "step 143000: loss: 0.0072\n",
      "step 143250: loss: 0.0067\n",
      "step 143500: loss: 0.0092\n",
      "step 143750: loss: 0.0198\n",
      "step 144000: loss: 0.0095\n",
      "step 144250: loss: 0.0121\n",
      "step 144500: loss: 0.0090\n",
      "step 144750: loss: 0.0061\n",
      "step 145000: loss: 0.0079\n",
      "step 145250: loss: 0.0349\n",
      "step 145500: loss: 0.0124\n",
      "step 145750: loss: 0.0083\n",
      "step 146000: loss: 0.0076\n",
      "step 146250: loss: 0.0076\n",
      "step 146500: loss: 0.0070\n",
      "step 146750: loss: 0.0090\n",
      "step 147000: loss: 0.0100\n",
      "step 147250: loss: 0.0121\n",
      "step 147500: loss: 0.0062\n",
      "step 147750: loss: 0.0103\n",
      "step 148000: loss: 0.0087\n",
      "step 148250: loss: 0.0058\n",
      "step 148500: loss: 0.0074\n",
      "step 148750: loss: 0.0085\n",
      "step 149000: loss: 0.0049\n",
      "step 149250: loss: 0.0138\n",
      "step 149500: loss: 0.0084\n",
      "step 149750: loss: 0.0082\n",
      "step 150000: loss: 0.0050\n",
      "step 150250: loss: 0.0152\n",
      "step 150500: loss: 0.0057\n",
      "step 150750: loss: 0.0063\n",
      "step 151000: loss: 0.0058\n",
      "step 151250: loss: 0.0057\n",
      "step 151500: loss: 0.0090\n",
      "step 151750: loss: 0.0058\n",
      "step 152000: loss: 0.0073\n",
      "step 152250: loss: 0.0094\n",
      "step 152500: loss: 0.0074\n",
      "step 152750: loss: 0.0066\n",
      "step 153000: loss: 0.0094\n",
      "step 153250: loss: 0.0060\n",
      "step 153500: loss: 0.0043\n",
      "step 153750: loss: 0.0093\n",
      "step 154000: loss: 0.0138\n",
      "step 154250: loss: 0.0109\n",
      "step 154500: loss: 0.0142\n",
      "step 154750: loss: 0.0066\n",
      "step 155000: loss: 0.0062\n",
      "step 155250: loss: 0.0086\n",
      "step 155500: loss: 0.0102\n",
      "step 155750: loss: 0.0094\n",
      "step 156000: loss: 0.0065\n",
      "step 156250: loss: 0.0065\n",
      "step 156500: loss: 0.0066\n",
      "step 156750: loss: 0.0098\n",
      "step 157000: loss: 0.0049\n",
      "step 157250: loss: 0.0072\n",
      "step 157500: loss: 0.0069\n",
      "step 157750: loss: 0.0071\n",
      "step 158000: loss: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 158250: loss: 0.0063\n",
      "step 158500: loss: 0.0046\n",
      "step 158750: loss: 0.0052\n",
      "step 159000: loss: 0.0069\n",
      "step 159250: loss: 0.0045\n",
      "step 159500: loss: 0.0089\n",
      "step 159750: loss: 0.0054\n",
      "step 160000: loss: 0.0044\n",
      "step 160250: loss: 0.0078\n",
      "step 160500: loss: 0.0060\n",
      "step 160750: loss: 0.0066\n",
      "step 161000: loss: 0.0063\n",
      "step 161250: loss: 0.0073\n",
      "step 161500: loss: 0.0042\n",
      "step 161750: loss: 0.0148\n",
      "step 162000: loss: 0.0107\n",
      "step 162250: loss: 0.0051\n",
      "step 162500: loss: 0.0078\n",
      "step 162750: loss: 0.0062\n",
      "step 163000: loss: 0.0090\n",
      "step 163250: loss: 0.0066\n",
      "step 163500: loss: 0.0044\n",
      "step 163750: loss: 0.0051\n",
      "step 164000: loss: 0.0036\n",
      "step 164250: loss: 0.0068\n",
      "step 164500: loss: 0.0068\n",
      "step 164750: loss: 0.0096\n",
      "step 165000: loss: 0.0073\n",
      "step 165250: loss: 0.0047\n",
      "step 165500: loss: 0.0072\n",
      "step 165750: loss: 0.0062\n",
      "step 166000: loss: 0.0060\n",
      "step 166250: loss: 0.0043\n",
      "step 166500: loss: 0.0149\n",
      "step 166750: loss: 0.0059\n",
      "step 167000: loss: 0.0064\n",
      "step 167250: loss: 0.0044\n",
      "step 167500: loss: 0.0041\n",
      "step 167750: loss: 0.0060\n",
      "step 168000: loss: 0.0093\n",
      "step 168250: loss: 0.0081\n",
      "step 168500: loss: 0.0062\n",
      "step 168750: loss: 0.0060\n",
      "step 169000: loss: 0.0043\n",
      "step 169250: loss: 0.0054\n",
      "step 169500: loss: 0.0055\n",
      "step 169750: loss: 0.0061\n",
      "step 170000: loss: 0.0112\n",
      "step 170250: loss: 0.0055\n",
      "step 170500: loss: 0.0050\n",
      "step 170750: loss: 0.0057\n",
      "step 171000: loss: 0.0081\n",
      "step 171250: loss: 0.0042\n",
      "step 171500: loss: 0.0044\n",
      "step 171750: loss: 0.0098\n",
      "step 172000: loss: 0.0085\n",
      "step 172250: loss: 0.0068\n",
      "step 172500: loss: 0.0055\n",
      "step 172750: loss: 0.0093\n",
      "step 173000: loss: 0.0101\n",
      "step 173250: loss: 0.0051\n",
      "step 173500: loss: 0.0042\n",
      "step 173750: loss: 0.0049\n",
      "step 174000: loss: 0.0054\n",
      "step 174250: loss: 0.0056\n",
      "step 174500: loss: 0.0081\n",
      "step 174750: loss: 0.0068\n",
      "step 175000: loss: 0.0069\n",
      "step 175250: loss: 0.0055\n",
      "step 175500: loss: 0.0033\n",
      "step 175750: loss: 0.0051\n",
      "step 176000: loss: 0.0059\n",
      "step 176250: loss: 0.0076\n",
      "step 176500: loss: 0.0053\n",
      "step 176750: loss: 0.0073\n",
      "step 177000: loss: 0.0069\n",
      "step 177250: loss: 0.0052\n",
      "step 177500: loss: 0.0059\n",
      "step 177750: loss: 0.0027\n",
      "step 178000: loss: 0.0046\n",
      "step 178250: loss: 0.0098\n",
      "step 178500: loss: 0.0074\n",
      "step 178750: loss: 0.0085\n",
      "step 179000: loss: 0.0038\n",
      "step 179250: loss: 0.0048\n",
      "step 179500: loss: 0.0074\n",
      "step 179750: loss: 0.0071\n",
      "step 180000: loss: 0.0057\n",
      "step 180250: loss: 0.0067\n",
      "step 180500: loss: 0.0058\n",
      "step 180750: loss: 0.0042\n",
      "step 181000: loss: 0.0052\n",
      "step 181250: loss: 0.0076\n",
      "step 181500: loss: 0.0108\n",
      "step 181750: loss: 0.0177\n",
      "step 182000: loss: 0.0052\n",
      "step 182250: loss: 0.0056\n",
      "step 182500: loss: 0.0051\n",
      "step 182750: loss: 0.0066\n",
      "step 183000: loss: 0.0045\n",
      "step 183250: loss: 0.0037\n",
      "step 183500: loss: 0.0044\n",
      "step 183750: loss: 0.0054\n",
      "step 184000: loss: 0.0042\n",
      "step 184250: loss: 0.0129\n",
      "step 184500: loss: 0.0077\n",
      "step 184750: loss: 0.0052\n",
      "step 185000: loss: 0.0043\n",
      "step 185250: loss: 0.0037\n",
      "step 185500: loss: 0.0054\n",
      "step 185750: loss: 0.0049\n",
      "step 186000: loss: 0.0038\n",
      "step 186250: loss: 0.0067\n",
      "step 186500: loss: 0.0091\n",
      "step 186750: loss: 0.0042\n",
      "step 187000: loss: 0.0025\n",
      "step 187250: loss: 0.0063\n",
      "step 187500: loss: 0.0052\n",
      "step 187750: loss: 0.0052\n",
      "step 188000: loss: 0.0049\n",
      "step 188250: loss: 0.0095\n",
      "step 188500: loss: 0.0078\n",
      "step 188750: loss: 0.0061\n",
      "step 189000: loss: 0.0071\n",
      "step 189250: loss: 0.0058\n",
      "step 189500: loss: 0.0057\n",
      "step 189750: loss: 0.0088\n",
      "step 190000: loss: 0.0068\n",
      "step 190250: loss: 0.0069\n",
      "step 190500: loss: 0.0068\n",
      "step 190750: loss: 0.0069\n",
      "step 191000: loss: 0.0055\n",
      "step 191250: loss: 0.0035\n",
      "step 191500: loss: 0.0044\n",
      "step 191750: loss: 0.0038\n",
      "step 192000: loss: 0.0046\n",
      "step 192250: loss: 0.0058\n",
      "step 192500: loss: 0.0056\n",
      "step 192750: loss: 0.0039\n",
      "step 193000: loss: 0.0096\n",
      "step 193250: loss: 0.0062\n",
      "step 193500: loss: 0.0043\n",
      "step 193750: loss: 0.0052\n",
      "step 194000: loss: 0.0052\n",
      "step 194250: loss: 0.0034\n",
      "step 194500: loss: 0.0028\n",
      "step 194750: loss: 0.0036\n",
      "step 195000: loss: 0.0031\n",
      "step 195250: loss: 0.0034\n",
      "step 195500: loss: 0.0034\n",
      "step 195750: loss: 0.0062\n",
      "step 196000: loss: 0.0076\n",
      "step 196250: loss: 0.0053\n",
      "step 196500: loss: 0.0046\n",
      "step 196750: loss: 0.0041\n",
      "step 197000: loss: 0.0043\n",
      "step 197250: loss: 0.0037\n",
      "step 197500: loss: 0.0027\n",
      "step 197750: loss: 0.0038\n",
      "step 198000: loss: 0.0054\n",
      "step 198250: loss: 0.0065\n",
      "step 198500: loss: 0.0059\n",
      "step 198750: loss: 0.0059\n",
      "step 199000: loss: 0.0066\n",
      "step 199250: loss: 0.0074\n",
      "step 199500: loss: 0.0048\n",
      "step 199750: loss: 0.0066\n",
      "step 200000: loss: 0.0065\n",
      "step 200250: loss: 0.0056\n",
      "step 200500: loss: 0.0038\n",
      "step 200750: loss: 0.0063\n",
      "step 201000: loss: 0.0030\n",
      "step 201250: loss: 0.0085\n",
      "step 201500: loss: 0.0067\n",
      "step 201750: loss: 0.0053\n",
      "step 202000: loss: 0.0041\n",
      "step 202250: loss: 0.0043\n",
      "step 202500: loss: 0.0049\n",
      "step 202750: loss: 0.0037\n",
      "step 203000: loss: 0.0067\n",
      "step 203250: loss: 0.0052\n",
      "step 203500: loss: 0.0039\n",
      "step 203750: loss: 0.0045\n",
      "step 204000: loss: 0.0034\n",
      "step 204250: loss: 0.0030\n",
      "step 204500: loss: 0.0049\n",
      "step 204750: loss: 0.0038\n",
      "step 205000: loss: 0.0088\n",
      "step 205250: loss: 0.0061\n",
      "step 205500: loss: 0.0038\n",
      "step 205750: loss: 0.0032\n",
      "step 206000: loss: 0.0053\n",
      "step 206250: loss: 0.0050\n",
      "step 206500: loss: 0.0137\n",
      "step 206750: loss: 0.0075\n",
      "step 207000: loss: 0.0081\n",
      "step 207250: loss: 0.0082\n",
      "step 207500: loss: 0.0056\n",
      "step 207750: loss: 0.0051\n",
      "step 208000: loss: 0.0046\n",
      "step 208250: loss: 0.0079\n",
      "step 208500: loss: 0.0047\n",
      "step 208750: loss: 0.0036\n",
      "step 209000: loss: 0.0041\n",
      "step 209250: loss: 0.0038\n",
      "step 209500: loss: 0.0067\n",
      "step 209750: loss: 0.0063\n",
      "step 210000: loss: 0.0042\n",
      "step 210250: loss: 0.0087\n",
      "step 210500: loss: 0.0039\n",
      "step 210750: loss: 0.0059\n",
      "step 211000: loss: 0.0040\n",
      "step 211250: loss: 0.0033\n",
      "step 211500: loss: 0.0070\n",
      "step 211750: loss: 0.0062\n",
      "step 212000: loss: 0.0058\n",
      "step 212250: loss: 0.0074\n",
      "step 212500: loss: 0.0075\n",
      "step 212750: loss: 0.0080\n",
      "step 213000: loss: 0.0070\n",
      "step 213250: loss: 0.0041\n",
      "step 213500: loss: 0.0042\n",
      "step 213750: loss: 0.0049\n",
      "step 214000: loss: 0.0036\n",
      "step 214250: loss: 0.0075\n",
      "step 214500: loss: 0.0048\n",
      "step 214750: loss: 0.0035\n",
      "step 215000: loss: 0.0027\n",
      "step 215250: loss: 0.0051\n",
      "step 215500: loss: 0.0048\n",
      "step 215750: loss: 0.0090\n",
      "step 216000: loss: 0.0039\n",
      "step 216250: loss: 0.0063\n",
      "step 216500: loss: 0.0064\n",
      "step 216750: loss: 0.0047\n",
      "step 217000: loss: 0.0038\n",
      "step 217250: loss: 0.0056\n",
      "step 217500: loss: 0.0048\n",
      "step 217750: loss: 0.0059\n",
      "step 218000: loss: 0.0057\n",
      "step 218250: loss: 0.0046\n",
      "step 218500: loss: 0.0053\n",
      "step 218750: loss: 0.0053\n",
      "step 219000: loss: 0.0066\n",
      "step 219250: loss: 0.0034\n",
      "step 219500: loss: 0.0037\n",
      "step 219750: loss: 0.0040\n",
      "step 220000: loss: 0.0042\n",
      "step 220250: loss: 0.0039\n",
      "step 220500: loss: 0.0047\n",
      "step 220750: loss: 0.0053\n",
      "step 221000: loss: 0.0055\n",
      "step 221250: loss: 0.0045\n",
      "step 221500: loss: 0.0039\n",
      "step 221750: loss: 0.0077\n",
      "step 222000: loss: 0.0042\n",
      "step 222250: loss: 0.0041\n",
      "step 222500: loss: 0.0051\n",
      "step 222750: loss: 0.0038\n",
      "step 223000: loss: 0.0047\n",
      "step 223250: loss: 0.0030\n",
      "step 223500: loss: 0.0056\n",
      "step 223750: loss: 0.0084\n",
      "step 224000: loss: 0.0083\n",
      "step 224250: loss: 0.0042\n",
      "step 224500: loss: 0.0040\n",
      "step 224750: loss: 0.0031\n",
      "step 225000: loss: 0.0049\n",
      "step 225250: loss: 0.0055\n",
      "step 225500: loss: 0.0050\n",
      "step 225750: loss: 0.0063\n",
      "step 226000: loss: 0.0053\n",
      "step 226250: loss: 0.0058\n",
      "step 226500: loss: 0.0061\n",
      "step 226750: loss: 0.0061\n",
      "step 227000: loss: 0.0079\n",
      "step 227250: loss: 0.0042\n",
      "step 227500: loss: 0.0063\n",
      "step 227750: loss: 0.0059\n",
      "step 228000: loss: 0.0050\n",
      "step 228250: loss: 0.0077\n",
      "step 228500: loss: 0.0034\n",
      "step 228750: loss: 0.0055\n",
      "step 229000: loss: 0.0086\n",
      "step 229250: loss: 0.0051\n",
      "step 229500: loss: 0.0075\n",
      "step 229750: loss: 0.0128\n",
      "step 230000: loss: 0.0072\n",
      "step 230250: loss: 0.0041\n",
      "step 230500: loss: 0.0062\n",
      "step 230750: loss: 0.0079\n",
      "step 231000: loss: 0.0057\n",
      "step 231250: loss: 0.0049\n",
      "step 231500: loss: 0.0132\n",
      "step 231750: loss: 0.0141\n",
      "step 232000: loss: 0.0060\n",
      "step 232250: loss: 0.0075\n",
      "step 232500: loss: 0.0035\n",
      "step 232750: loss: 0.0038\n",
      "step 233000: loss: 0.0072\n",
      "step 233250: loss: 0.0045\n",
      "step 233500: loss: 0.0050\n",
      "step 233750: loss: 0.0063\n",
      "step 234000: loss: 0.0048\n",
      "step 234250: loss: 0.0025\n",
      "step 234500: loss: 0.0037\n",
      "step 234750: loss: 0.0048\n",
      "step 235000: loss: 0.0064\n",
      "step 235250: loss: 0.0054\n",
      "step 235500: loss: 0.0038\n",
      "step 235750: loss: 0.0067\n",
      "step 236000: loss: 0.0062\n",
      "step 236250: loss: 0.0043\n",
      "step 236500: loss: 0.0039\n",
      "step 236750: loss: 0.0034\n",
      "step 237000: loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 237250: loss: 0.0036\n",
      "step 237500: loss: 0.0037\n",
      "step 237750: loss: 0.0040\n",
      "step 238000: loss: 0.0072\n",
      "step 238250: loss: 0.0037\n",
      "step 238500: loss: 0.0044\n",
      "step 238750: loss: 0.0049\n",
      "step 239000: loss: 0.0036\n",
      "step 239250: loss: 0.0035\n",
      "step 239500: loss: 0.0033\n",
      "step 239750: loss: 0.0028\n",
      "step 240000: loss: 0.0041\n",
      "step 240250: loss: 0.0056\n",
      "step 240500: loss: 0.0052\n",
      "step 240750: loss: 0.0042\n",
      "step 241000: loss: 0.0048\n",
      "step 241250: loss: 0.0044\n",
      "step 241500: loss: 0.0074\n",
      "step 241750: loss: 0.0050\n",
      "step 242000: loss: 0.0041\n",
      "step 242250: loss: 0.0050\n",
      "step 242500: loss: 0.0036\n",
      "step 242750: loss: 0.0063\n",
      "step 243000: loss: 0.0066\n",
      "step 243250: loss: 0.0041\n",
      "step 243500: loss: 0.0033\n",
      "step 243750: loss: 0.0029\n",
      "step 244000: loss: 0.0057\n",
      "step 244250: loss: 0.0061\n",
      "step 244500: loss: 0.0058\n",
      "step 244750: loss: 0.0048\n",
      "step 245000: loss: 0.0047\n",
      "step 245250: loss: 0.0039\n",
      "step 245500: loss: 0.0036\n",
      "step 245750: loss: 0.0086\n",
      "step 246000: loss: 0.0037\n",
      "step 246250: loss: 0.0040\n",
      "step 246500: loss: 0.0046\n",
      "step 246750: loss: 0.0049\n",
      "step 247000: loss: 0.0045\n",
      "step 247250: loss: 0.0030\n",
      "step 247500: loss: 0.0051\n",
      "step 247750: loss: 0.0034\n",
      "step 248000: loss: 0.0071\n",
      "step 248250: loss: 0.0047\n",
      "step 248500: loss: 0.0064\n",
      "step 248750: loss: 0.0045\n",
      "step 249000: loss: 0.0039\n",
      "step 249250: loss: 0.0028\n",
      "step 249500: loss: 0.0033\n",
      "step 249750: loss: 0.0033\n",
      "step 250000: loss: 0.0037\n",
      "step 250250: loss: 0.0039\n",
      "step 250500: loss: 0.0028\n",
      "step 250750: loss: 0.0033\n",
      "step 251000: loss: 0.0110\n",
      "step 251250: loss: 0.0045\n",
      "step 251500: loss: 0.0047\n",
      "step 251750: loss: 0.0036\n",
      "step 252000: loss: 0.0030\n",
      "step 252250: loss: 0.0053\n",
      "step 252500: loss: 0.0038\n",
      "step 252750: loss: 0.0068\n",
      "step 253000: loss: 0.0033\n",
      "step 253250: loss: 0.0022\n",
      "step 253500: loss: 0.0037\n",
      "step 253750: loss: 0.0026\n",
      "step 254000: loss: 0.0120\n",
      "step 254250: loss: 0.0044\n",
      "step 254500: loss: 0.0028\n",
      "step 254750: loss: 0.0041\n",
      "step 255000: loss: 0.0027\n",
      "step 255250: loss: 0.0041\n",
      "step 255500: loss: 0.0040\n",
      "step 255750: loss: 0.0030\n",
      "step 256000: loss: 0.0030\n",
      "step 256250: loss: 0.0049\n",
      "step 256500: loss: 0.0033\n",
      "step 256750: loss: 0.0074\n",
      "step 257000: loss: 0.0059\n",
      "step 257250: loss: 0.0066\n",
      "step 257500: loss: 0.0034\n",
      "step 257750: loss: 0.0040\n",
      "step 258000: loss: 0.0062\n",
      "step 258250: loss: 0.0030\n",
      "step 258500: loss: 0.0030\n",
      "step 258750: loss: 0.0028\n",
      "step 259000: loss: 0.0033\n",
      "step 259250: loss: 0.0051\n",
      "step 259500: loss: 0.0044\n",
      "step 259750: loss: 0.0036\n",
      "step 260000: loss: 0.0107\n",
      "step 260250: loss: 0.0042\n",
      "step 260500: loss: 0.0065\n",
      "step 260750: loss: 0.0063\n",
      "step 261000: loss: 0.0052\n",
      "step 261250: loss: 0.0034\n",
      "step 261500: loss: 0.0071\n",
      "step 261750: loss: 0.0052\n",
      "step 262000: loss: 0.0034\n",
      "step 262250: loss: 0.0037\n",
      "step 262500: loss: 0.0043\n",
      "step 262750: loss: 0.0050\n",
      "step 263000: loss: 0.0108\n",
      "step 263250: loss: 0.0036\n",
      "step 263500: loss: 0.0050\n",
      "step 263750: loss: 0.0081\n",
      "step 264000: loss: 0.0065\n",
      "step 264250: loss: 0.0040\n",
      "step 264500: loss: 0.0029\n",
      "step 264750: loss: 0.0026\n",
      "step 265000: loss: 0.0063\n",
      "step 265250: loss: 0.0043\n",
      "step 265500: loss: 0.0114\n",
      "step 265750: loss: 0.0056\n",
      "step 266000: loss: 0.0039\n",
      "step 266250: loss: 0.0041\n",
      "step 266500: loss: 0.0041\n",
      "step 266750: loss: 0.0031\n",
      "step 267000: loss: 0.0069\n",
      "step 267250: loss: 0.0040\n",
      "step 267500: loss: 0.0035\n",
      "step 267750: loss: 0.0060\n",
      "step 268000: loss: 0.0048\n",
      "step 268250: loss: 0.0053\n",
      "step 268500: loss: 0.0036\n",
      "step 268750: loss: 0.0028\n",
      "step 269000: loss: 0.0031\n",
      "step 269250: loss: 0.0049\n",
      "step 269500: loss: 0.0032\n",
      "step 269750: loss: 0.0104\n",
      "step 270000: loss: 0.0039\n",
      "step 270250: loss: 0.0027\n",
      "step 270500: loss: 0.0053\n",
      "step 270750: loss: 0.0029\n",
      "step 271000: loss: 0.0041\n",
      "step 271250: loss: 0.0037\n",
      "step 271500: loss: 0.0035\n",
      "step 271750: loss: 0.0045\n",
      "step 272000: loss: 0.0026\n",
      "step 272250: loss: 0.0034\n",
      "step 272500: loss: 0.0029\n",
      "step 272750: loss: 0.0053\n",
      "step 273000: loss: 0.0035\n",
      "step 273250: loss: 0.0040\n",
      "step 273500: loss: 0.0029\n",
      "step 273750: loss: 0.0022\n",
      "step 274000: loss: 0.0040\n",
      "step 274250: loss: 0.0025\n",
      "step 274500: loss: 0.0058\n",
      "step 274750: loss: 0.0027\n",
      "step 275000: loss: 0.0031\n",
      "step 275250: loss: 0.0042\n",
      "step 275500: loss: 0.0040\n",
      "step 275750: loss: 0.0032\n",
      "step 276000: loss: 0.0049\n",
      "step 276250: loss: 0.0028\n",
      "step 276500: loss: 0.0044\n",
      "step 276750: loss: 0.0046\n",
      "step 277000: loss: 0.0032\n",
      "step 277250: loss: 0.0031\n",
      "step 277500: loss: 0.0049\n",
      "step 277750: loss: 0.0032\n",
      "step 278000: loss: 0.0042\n",
      "step 278250: loss: 0.0037\n",
      "step 278500: loss: 0.0038\n",
      "step 278750: loss: 0.0040\n",
      "step 279000: loss: 0.0035\n",
      "step 279250: loss: 0.0062\n",
      "step 279500: loss: 0.0058\n",
      "step 279750: loss: 0.0026\n",
      "step 280000: loss: 0.0022\n",
      "step 280250: loss: 0.0027\n",
      "step 280500: loss: 0.0036\n",
      "step 280750: loss: 0.0056\n",
      "step 281000: loss: 0.0036\n",
      "step 281250: loss: 0.0044\n",
      "step 281500: loss: 0.0037\n",
      "step 281750: loss: 0.0046\n",
      "step 282000: loss: 0.0022\n",
      "step 282250: loss: 0.0042\n",
      "step 282500: loss: 0.0023\n",
      "step 282750: loss: 0.0034\n",
      "step 283000: loss: 0.0034\n",
      "step 283250: loss: 0.0050\n",
      "step 283500: loss: 0.0032\n",
      "step 283750: loss: 0.0061\n",
      "step 284000: loss: 0.0064\n",
      "step 284250: loss: 0.0031\n",
      "step 284500: loss: 0.0041\n",
      "step 284750: loss: 0.0072\n",
      "step 285000: loss: 0.0037\n",
      "step 285250: loss: 0.0041\n",
      "step 285500: loss: 0.0022\n",
      "step 285750: loss: 0.0040\n",
      "step 286000: loss: 0.0031\n",
      "step 286250: loss: 0.0059\n",
      "step 286500: loss: 0.0045\n",
      "step 286750: loss: 0.0025\n",
      "step 287000: loss: 0.0023\n",
      "step 287250: loss: 0.0034\n",
      "step 287500: loss: 0.0034\n",
      "step 287750: loss: 0.0026\n",
      "step 288000: loss: 0.0043\n",
      "step 288250: loss: 0.0027\n",
      "step 288500: loss: 0.0024\n",
      "step 288750: loss: 0.0037\n",
      "step 289000: loss: 0.0054\n",
      "step 289250: loss: 0.0024\n",
      "step 289500: loss: 0.0027\n",
      "step 289750: loss: 0.0076\n",
      "step 290000: loss: 0.0038\n",
      "step 290250: loss: 0.0056\n",
      "step 290500: loss: 0.0041\n",
      "step 290750: loss: 0.0047\n",
      "step 291000: loss: 0.0034\n",
      "step 291250: loss: 0.0048\n",
      "step 291500: loss: 0.0036\n",
      "step 291750: loss: 0.0021\n",
      "step 292000: loss: 0.0082\n",
      "step 292250: loss: 0.0039\n",
      "step 292500: loss: 0.0024\n",
      "step 292750: loss: 0.0025\n",
      "step 293000: loss: 0.0019\n",
      "step 293250: loss: 0.0031\n",
      "step 293500: loss: 0.0027\n",
      "step 293750: loss: 0.0063\n",
      "step 294000: loss: 0.0079\n",
      "step 294250: loss: 0.0040\n",
      "step 294500: loss: 0.0045\n",
      "step 294750: loss: 0.0045\n",
      "step 295000: loss: 0.0074\n",
      "step 295250: loss: 0.0043\n",
      "step 295500: loss: 0.0055\n",
      "step 295750: loss: 0.0042\n",
      "step 296000: loss: 0.0030\n",
      "step 296250: loss: 0.0106\n",
      "step 296500: loss: 0.0053\n",
      "step 296750: loss: 0.0053\n",
      "step 297000: loss: 0.0035\n",
      "step 297250: loss: 0.0380\n",
      "step 297500: loss: 0.0073\n",
      "step 297750: loss: 0.0033\n",
      "step 298000: loss: 0.0029\n",
      "step 298250: loss: 0.0028\n",
      "step 298500: loss: 0.0031\n",
      "step 298750: loss: 0.0040\n",
      "step 299000: loss: 0.0022\n",
      "step 299250: loss: 0.0039\n",
      "step 299500: loss: 0.0037\n",
      "step 299750: loss: 0.0028\n",
      "step 300000: loss: 0.0025\n",
      "\tstep 300000: saved model\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "running_loss = None\n",
    "# generic train loop\n",
    "for global_step in range(global_step, total_steps+global_step+1):\n",
    "    \n",
    "    # forward\n",
    "    np_H, np_e0, np_psi0 = next_batch(D_side, batch_size)\n",
    "    real_H = Variable(torch.Tensor(np_H))\n",
    "    real_e0 = Variable(torch.Tensor(np_e0))\n",
    "    real_psi0 = Variable(torch.Tensor(np_psi0))\n",
    "    psi0_hat = forward(real_H, model).t()\n",
    "\n",
    "    # backward\n",
    "#     loss = .5*torch.sum(cost_func(psi0_hat, real_psi0)**2) / batch_size\n",
    "    loss = cost_func(real_e0, psi0_hat, real_H, batch_size) #.5*torch.sum((psi0_hat - real_psi0)**2) / batch_size\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    running_loss = loss.data.numpy()[0] if running_loss is None else .99*running_loss + (1-.99)*loss.data.numpy()[0]\n",
    "\n",
    "    # ======== DISPLAY PROGRESS ======== #\n",
    "    if global_step % print_every == 0:\n",
    "        print('step {}: loss: {:.4f}'.format(global_step, running_loss))\n",
    "    if global_step in checkpoint_steps:\n",
    "        print('\\tstep {}: saved model'.format(global_step))\n",
    "        torch.save({'state_dict': model.state_dict(),\n",
    "                   'global_step': global_step}\n",
    "                   , save_dir + '/model.{}.tar'.format(global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model: H2psi0_mps_models/model.1.tar\n",
      "loaded model: H2psi0_mps_models/model.10.tar\n",
      "loaded model: H2psi0_mps_models/model.100.tar\n",
      "loaded model: H2psi0_mps_models/model.1000.tar\n",
      "loaded model: H2psi0_mps_models/model.10000.tar\n",
      "loaded model: H2psi0_mps_models/model.100000.tar\n",
      "loaded model: H2psi0_mps_models/model.128000.tar\n",
      "loaded model: H2psi0_mps_models/model.16000.tar\n",
      "loaded model: H2psi0_mps_models/model.2000.tar\n",
      "loaded model: H2psi0_mps_models/model.250.tar\n",
      "loaded model: H2psi0_mps_models/model.30.tar\n",
      "loaded model: H2psi0_mps_models/model.300.tar\n",
      "loaded model: H2psi0_mps_models/model.3000.tar\n",
      "loaded model: H2psi0_mps_models/model.30000.tar\n",
      "loaded model: H2psi0_mps_models/model.300000.tar\n",
      "loaded model: H2psi0_mps_models/model.4000.tar\n",
      "loaded model: H2psi0_mps_models/model.500.tar\n",
      "loaded model: H2psi0_mps_models/model.64000.tar\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "model_zoo = []\n",
    "paths = glob.glob(save_dir + '/*.tar')\n",
    "try:\n",
    "    for s in paths:\n",
    "        checkpoint = torch.load(s)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        global_step = checkpoint['global_step']\n",
    "        model_zoo.append((global_step, copy.deepcopy(model)))\n",
    "        print(\"loaded model: {}\".format(s))\n",
    "    model_zoo = sorted(model_zoo, key=lambda tup: tup[0])\n",
    "    global_step, model = model_zoo[-1]\n",
    "except:\n",
    "    print(\"no saved model to load.\") ; model_zoo = []\n",
    "    load_was_success = False\n",
    "else:\n",
    "    if len(paths) is 0: print(\"no saved model to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArQAAADeCAYAAADMzpPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112d21588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = 1\n",
    "np_H, np_e0, np_psi0 = next_batch(D_side, batch_size=b)\n",
    "real_H = Variable(torch.Tensor(np_H))\n",
    "real_psi0 = Variable(torch.Tensor(np_psi0))\n",
    "psi0_hat = forward(real_H, model).t()\n",
    "\n",
    "np_real_psi0 = real_psi0.data.numpy()\n",
    "np_psi0_hat = psi0_hat.data.numpy()\n",
    "\n",
    "sd, mn = np.std(np_real_psi0), np.mean(np_real_psi0)\n",
    "cl = [mn-2*sd,mn+2*sd]\n",
    "use_cl = True\n",
    "# if use_cl: print(cl)\n",
    "\n",
    "f2 = plt.figure(figsize=[10,4])\n",
    "plt.subplot(211)\n",
    "plt.imshow(np_real_psi0) ; plt.title('exact $\\psi_0$')\n",
    "if use_cl: plt.clim(*cl)\n",
    "plt.subplot(212)\n",
    "im = plt.imshow(np_psi0_hat) ; plt.title('low-energy estimate $\\hat \\psi_{NN,MPS}$')\n",
    "if use_cl: plt.clim(*cl)\n",
    "\n",
    "cax = f2.add_axes([0.94, 0.15, 0.02, 0.7])\n",
    "cb = f2.colorbar(im, cax=cax, orientation='vertical')\n",
    "cb.set_label('color scale')\n",
    "\n",
    "plt.show() ; f2.savefig('./figures/H2psi0-mps.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean percent error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psi0_e    :  -1.30229\n",
      "e0_e      :  -1.30229\n",
      "hat psi0_e:  -1.25093\n",
      "rand psi_e:  0.00916191 \n",
      "\n",
      "mean err nn  : 0.0514\n",
      "mean err rand: 1.3114\n",
      "\n",
      "mpe nn  : 3.9434%\n",
      "mpe rand: 100.7035%\n"
     ]
    }
   ],
   "source": [
    "k=5000\n",
    "\n",
    "real_H, real_e0, real_psi0, psi0_hat = [], [], [], []\n",
    "\n",
    "for i in range(k):\n",
    "    np_H, np_e0, np_psi0 = next_batch(D_side, batch_size=1)\n",
    "    H = Variable(torch.Tensor(np_H))\n",
    "    e0 = Variable(torch.Tensor(np_e0))\n",
    "    psi0 = Variable(torch.Tensor(np_psi0))\n",
    "    temp = forward(H, model).t()\n",
    "    real_H.append(H) ; real_e0.append(e0) ; real_psi0.append(psi0) ; psi0_hat.append(temp)\n",
    "real_H = torch.stack(real_H)\n",
    "real_e0 = torch.stack(real_e0)\n",
    "real_psi0 = torch.stack(real_psi0)\n",
    "psi0_hat = torch.stack(psi0_hat)\n",
    "\n",
    "psis = []\n",
    "for _ in range(k):\n",
    "    psis.append(rand_psi(D_side))\n",
    "rand_psis = Variable(torch.Tensor(np.vstack(psis)))\n",
    "\n",
    "psi0_e = energy_func(real_psi0, real_H, batch_size=k).data.numpy()\n",
    "e0_e = real_e0.data.numpy()\n",
    "psi0_hat_e = energy_func(psi0_hat, real_H, batch_size=k).data.numpy()\n",
    "rpsi_e = energy_func(rand_psis, real_H, batch_size=k).data.numpy()\n",
    "\n",
    "mean_err_nn = np.mean(np.abs(e0_e.ravel() - psi0_hat_e.ravel()))\n",
    "mean_err_rand = np.mean(np.abs(e0_e.ravel() - rpsi_e.ravel()))\n",
    "\n",
    "mpe_nn = mean_err_nn / np.mean(np.abs(e0_e)) * 100\n",
    "mpe_rand = mean_err_rand / np.mean(np.abs(e0_e)) * 100\n",
    "\n",
    "print(\"psi0_e    : \", np.mean(psi0_e))\n",
    "print(\"e0_e      : \", np.mean(e0_e))\n",
    "print(\"hat psi0_e: \", np.mean(psi0_hat_e))\n",
    "print(\"rand psi_e: \", np.mean(rpsi_e), '\\n')\n",
    "\n",
    "print(\"mean err nn  : {:.4f}\".format( np.mean(np.abs(e0_e.ravel() - psi0_hat_e.ravel()))) )\n",
    "print(\"mean err rand: {:.4f}\\n\".format( np.mean(np.abs(e0_e.ravel() - rpsi_e.ravel()))))\n",
    "\n",
    "print(\"mpe nn  : {:.4f}%\".format(mpe_nn))\n",
    "print(\"mpe rand: {:.4f}%\".format(mpe_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(D, batch_size, just_ground=True):\n",
    "    H_list = [] ; e0_list = [] ; psi0_list = [] ; e1_list = [] ; psi1_list = [] ; e2_list = [] ; psi2_list = []\n",
    "    for _ in range(batch_size):\n",
    "        H = ham(None, None, rand=True)\n",
    "        e0, psi0 = eigsh(H,k=3, which=\"SA\")\n",
    "        H_list.append(np.asarray(H).ravel()) ; e0_list.append(e0[0].ravel()) ; psi0_list.append(psi0[:,0].ravel())\n",
    "        if not just_ground:\n",
    "            e1_list.append(e0[1].ravel()) ; psi1_list.append(psi0[:,1].ravel())\n",
    "            e2_list.append(e0[2].ravel()) ; psi2_list.append(psi0[:,2].ravel())\n",
    "    out = (np.vstack(H_list), np.vstack(e0_list), np.vstack(psi0_list))\n",
    "    if not just_ground:\n",
    "        extras = (np.vstack(e1_list), np.vstack(psi1_list), np.vstack(e2_list), np.vstack(psi2_list))\n",
    "    out = out if just_ground else out + extras\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5000\n",
    "real_H, real_e0, real_psi0, real_e1, real_psi1, real_e2, real_psi2, psi0_hat = [], [], [], [], [], [], [], []\n",
    "\n",
    "for i in range(k):\n",
    "    np_H, np_e0, np_psi0, np_e1, np_psi1, np_e2, np_psi2 = next_batch(D_side, batch_size=1, just_ground=False)\n",
    "    H = Variable(torch.Tensor(np_H))\n",
    "    e0 = Variable(torch.Tensor(np_e0))\n",
    "    psi0 = Variable(torch.Tensor(np_psi0))\n",
    "    e1 = Variable(torch.Tensor(np_e1))\n",
    "    psi1 = Variable(torch.Tensor(np_psi1))\n",
    "    e2 = Variable(torch.Tensor(np_e2))\n",
    "    psi2 = Variable(torch.Tensor(np_psi2))\n",
    "    temp = forward(H, model).t()\n",
    "    real_H.append(H) ; real_e0.append(e0) ; real_psi0.append(psi0) ; psi0_hat.append(temp)\n",
    "    real_e1.append(e1) ; real_psi1.append(psi1)\n",
    "    real_e2.append(e2) ; real_psi2.append(psi2)\n",
    "real_H = torch.stack(real_H)\n",
    "psi0_hat = torch.stack(psi0_hat)\n",
    "real_e0 = torch.stack(real_e0)\n",
    "real_psi0 = torch.stack(real_psi0)\n",
    "real_e1 = torch.stack(real_e1)\n",
    "real_psi1 = torch.stack(real_psi1)\n",
    "real_e2 = torch.stack(real_e2)\n",
    "real_psi2 = torch.stack(real_psi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psi0_e: -1.2901 / e0_e: -1.2901\n",
      "psi1_e: -1.2396 / e1_e: -1.2396\n",
      "psi2_e: -1.0007 / e2_e: -1.0007\n",
      "hat psi0_e:  -1.23782\n",
      "rand psi_e:  -0.00768376 \n",
      "\n",
      "mean err nn  : 0.0522\n",
      "mean err rand: 1.2824\n",
      "\n",
      "mean err nn  : 4.0499%\n",
      "mean err rand: 99.4044%\n"
     ]
    }
   ],
   "source": [
    "psis = []\n",
    "for _ in range(k):\n",
    "    psis.append(rand_psi(D_side))\n",
    "rand_psis = Variable(torch.Tensor(np.vstack(psis)))\n",
    "\n",
    "psi0_e = energy_func(real_psi0, real_H, batch_size=k).data.numpy()\n",
    "psi1_e = energy_func(real_psi1, real_H, batch_size=k).data.numpy()\n",
    "psi2_e = energy_func(real_psi2, real_H, batch_size=k).data.numpy()\n",
    "e0_e = real_e0.data.numpy()\n",
    "e1_e = real_e1.data.numpy()\n",
    "e2_e = real_e2.data.numpy()\n",
    "psi0_hat_e = energy_func(psi0_hat, real_H, batch_size=k).data.numpy()\n",
    "rpsi_e = energy_func(rand_psis, real_H, batch_size=k).data.numpy()\n",
    "\n",
    "mean_err_nn = np.mean(np.abs(e0_e.ravel() - psi0_hat_e.ravel()))\n",
    "mean_err_rand = np.mean(np.abs(e0_e.ravel() - rpsi_e.ravel()))\n",
    "\n",
    "mpe_nn = mean_err_nn / np.mean(np.abs(e0_e)) * 100\n",
    "mpe_rand = mean_err_rand / np.mean(np.abs(e0_e)) * 100\n",
    "\n",
    "print(\"psi0_e: {:.4f} / e0_e: {:.4f}\".format(np.mean(psi0_e), np.mean(e0_e)))\n",
    "print(\"psi1_e: {:.4f} / e1_e: {:.4f}\".format(np.mean(psi1_e), np.mean(e1_e)))\n",
    "print(\"psi2_e: {:.4f} / e2_e: {:.4f}\".format(np.mean(psi2_e), np.mean(e2_e)))\n",
    "print(\"hat psi0_e: \", np.mean(psi0_hat_e))\n",
    "print(\"rand psi_e: \", np.mean(rpsi_e), '\\n')\n",
    "\n",
    "print(\"mean err nn  : {:.4f}\".format( np.mean(np.abs(e0_e.ravel() - psi0_hat_e.ravel()))) )\n",
    "print(\"mean err rand: {:.4f}\\n\".format( np.mean(np.abs(e0_e.ravel() - rpsi_e.ravel()))))\n",
    "\n",
    "print(\"mean err nn  : {:.4f}%\".format(mpe_nn))\n",
    "print(\"mean err rand: {:.4f}%\".format(mpe_rand))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
